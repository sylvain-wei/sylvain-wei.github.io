@online{2021,
  title = {Ego Graph {{Using Networkx}} in {{Python}}},
  date = {2021-05-17T00:54:36+00:00},
  url = {https://www.geeksforgeeks.org/ego-graph-using-networkx-in-python/},
  urldate = {2023-05-10},
  abstract = {A Computer Science portal for geeks. It contains well written, well thought and well explained computer science and programming articles, quizzes and practice/competitive programming/company interview Questions.},
  langid = {american},
  organization = {{GeeksforGeeks}},
  file = {D:\Zotero\literature_data\storage\5R4PYXD9\ego-graph-using-networkx-in-python.html}
}

@article{2022,
  title = {The Graph Connection},
  date = {2022-03-23},
  journaltitle = {Nature Machine Intelligence},
  shortjournal = {Nat Mach Intell},
  volume = {4},
  number = {3},
  pages = {187--188},
  issn = {2522-5839},
  doi = {10.1038/s42256-022-00476-6},
  url = {https://www.nature.com/articles/s42256-022-00476-6},
  urldate = {2023-05-25},
  langid = {english},
  file = {D:\Zotero\literature_data\storage\G75VQBT2\2022 - The graph connection.pdf}
}

@software{2023,
  title = {Enabling {{Large Language Models}} to {{Generate Text}} with {{Citations}}},
  date = {2023-11-07T03:54:55Z},
  origdate = {2023-05-23T13:02:26Z},
  url = {https://github.com/princeton-nlp/ALCE},
  urldate = {2023-11-08},
  abstract = {[EMNLP 2023] Enabling Large Language Models to Generate Text with Citations. Paper: https://arxiv.org/abs/2305.14627},
  organization = {{Princeton Natural Language Processing}}
}

@software{2023a,
  title = {{{AttrScore}}},
  date = {2023-10-18T19:08:56Z},
  origdate = {2023-05-08T15:09:47Z},
  url = {https://github.com/OSU-NLP-Group/AttrScore},
  urldate = {2023-10-20},
  abstract = {Code, datasets, models for the paper "Automatic Evaluation of Attribution by Large Language Models"},
  organization = {{OSU Natural Language Processing}},
  keywords = {attribution,chatgpt,evaluation-llms,gpt-4,large-language-model,large-language-models,llms,natural-language-processing,È°πÁõÆÈ¢ÑÂ§áÁü•ËØÜ}
}

@inreference{2023b,
  title = {Attention (Machine Learning)},
  booktitle = {Áª¥Âü∫ÁôæÁßë},
  date = {2023-07-26T08:57:29Z},
  url = {https://en.wikipedia.org/w/index.php?title=Attention_(machine_learning)&oldid=1167197237},
  urldate = {2023-07-26},
  abstract = {Machine learning-based attention is a mechanism, mimicking cognitive attention, and calculating (either in parallel, such as in transformers or sequentially, such as recursive neural networks) different "soft" weight for each word in the context window, for example, during each conversation with a large language model. "Soft" weights can change during each runtime, in contrast to "hard" weights, which are (pre-)trained and fine-tuned and remain frozen afterwards. Similar mechanisms were used in recursive neural networks, however, they didn't process the words in parallel, but sequentially and, at each step, considered the current word and the words within the context window to update its hidden state and make predictions. They were known as multiplicative modules, sigma pi units, and hyper-networks.  They have been used in LSTMs, and multi-sensory data processing (sound, images, video, and text) in perceivers, fast weight controllers's memory,, reasoning tasks in differentiable neural computers, and neural Turing machines},
  langid = {english},
  annotation = {Page Version ID: 1167197237}
}

@online{2023c,
  title = {Èù¢ÂêëÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÊäÄÊúØÔºöÁªºËø∞ [ËØë]},
  date = {2023-12-22},
  url = {https://baoyu.io/translations/ai-paper/2312.10997-retrieval-augmented-generation-for-large-language-models-a-survey},
  urldate = {2024-01-29},
  abstract = {Âú®ËøôÁØáÁªºËø∞‰∏≠ÔºåÊàë‰ª¨ÂÖ≥Ê≥®ÁöÑÊòØÈù¢ÂêëÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLarge Language ModelÔºâÁöÑÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÊäÄÊúØ„ÄÇËøôÈ°πÊäÄÊúØÈÄöËøáÁªìÂêàÊ£ÄÁ¥¢Êú∫Âà∂ÔºåÂ¢ûÂº∫‰∫ÜÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®Â§ÑÁêÜÂ§çÊùÇÊü•ËØ¢ÂíåÁîüÊàêÊõ¥ÂáÜÁ°Æ‰ø°ÊÅØÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇÊàë‰ª¨‰ªéÂêåÊµéÂ§ßÂ≠¶ÂíåÂ§çÊó¶Â§ßÂ≠¶ÁöÑÁõ∏ÂÖ≥Á†îÁ©∂Âõ¢ÈòüÂá∫ÂèëÔºåÁªºÂêàÂàÜÊûê‰∫ÜËØ•È¢ÜÂüüÁöÑÊúÄÊñ∞ËøõÂ±ïÂíåÊú™Êù•Ë∂ãÂäø„ÄÇ},
  langid = {english},
  file = {D:\Zotero\literature_data\storage\IBSLCRHT\2312.html}
}

@online{agrawal2023,
  title = {Do {{Language Models Know When They}}'re {{Hallucinating References}}?},
  author = {Agrawal, Ayush and Suzgun, Mirac and Mackey, Lester and Kalai, Adam Tauman},
  date = {2023-09-13},
  eprint = {2305.18248},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2305.18248},
  urldate = {2023-11-15},
  abstract = {State-of-the-art language models (LMs) are famous for "hallucinating" references. These fabricated article and book titles lead to harms, obstacles to their use, and public backlash. While other types of LM hallucinations are also important, we propose hallucinated references as the "drosophila" of research on hallucination in large language models (LLMs), as they are particularly easy to study. We show that simple search engine queries reliably identify such hallucinations, which facilitates evaluation. To begin to dissect the nature of hallucinated LM references, we attempt to classify them using black-box queries to the same LM, without consulting any external resources. Consistency checks done with "direct" queries about whether the generated reference title is real (inspired by Kadavath et al. 2022, Lin et al. 2022, Manakul et al. 2023) are compared to consistency checks with "indirect" queries which ask for ancillary details such as the authors of the work. These consistency checks are found to be partially reliable indicators of whether or not the reference is a hallucination. In particular, we find that LMs often hallucinate differing authors of hallucinated references when queried in independent sessions, while consistently identify authors of real references. This suggests that the hallucination may be more a generation issue than inherent to current training techniques or representation.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Áõ¥Êé•ÂΩíÂõ†},
  file = {D\:\\Zotero\\literature_data\\storage\\45VBHUGM\\Agrawal Á≠â - 2023 - Do Language Models Know When They're Hallucinating.pdf;D\:\\Zotero\\literature_data\\storage\\W7Y56V3I\\2305.html}
}

@article{alayrac,
  title = {ü¶© {{Flamingo}}: A {{Visual Language Model}} for {{Few-Shot Learning}}},
  author = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katie and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob and Borgeaud, Sebastian and Brock, Andrew and Nematzadeh, Aida and Sharifzadeh, Sahand and Binkowski, Mikolaj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Karen},
  abstract = {Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.},
  langid = {english},
  keywords = {ÂÖ∂‰ªñÂèÇËÄÉ},
  file = {D:\Zotero\literature_data\storage\A6SJBGIV\Alayrac Á≠â -  Flamingo a Visual Language Model for Few-Shot .pdf}
}

@online{anil2023,
  title = {{{PaLM}} 2 {{Technical Report}}},
  author = {Anil, Rohan and Dai, Andrew M. and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and Chu, Eric and Clark, Jonathan H. and Shafey, Laurent El and Huang, Yanping and Meier-Hellstern, Kathy and Mishra, Gaurav and Moreira, Erica and Omernick, Mark and Robinson, Kevin and Ruder, Sebastian and Tay, Yi and Xiao, Kefan and Xu, Yuanzhong and Zhang, Yujing and Abrego, Gustavo Hernandez and Ahn, Junwhan and Austin, Jacob and Barham, Paul and Botha, Jan and Bradbury, James and Brahma, Siddhartha and Brooks, Kevin and Catasta, Michele and Cheng, Yong and Cherry, Colin and Choquette-Choo, Christopher A. and Chowdhery, Aakanksha and Crepy, Cl√©ment and Dave, Shachi and Dehghani, Mostafa and Dev, Sunipa and Devlin, Jacob and D√≠az, Mark and Du, Nan and Dyer, Ethan and Feinberg, Vlad and Feng, Fangxiaoyu and Fienber, Vlad and Freitag, Markus and Garcia, Xavier and Gehrmann, Sebastian and Gonzalez, Lucas and Gur-Ari, Guy and Hand, Steven and Hashemi, Hadi and Hou, Le and Howland, Joshua and Hu, Andrea and Hui, Jeffrey and Hurwitz, Jeremy and Isard, Michael and Ittycheriah, Abe and Jagielski, Matthew and Jia, Wenhao and Kenealy, Kathleen and Krikun, Maxim and Kudugunta, Sneha and Lan, Chang and Lee, Katherine and Lee, Benjamin and Li, Eric and Li, Music and Li, Wei and Li, YaGuang and Li, Jian and Lim, Hyeontaek and Lin, Hanzhao and Liu, Zhongtao and Liu, Frederick and Maggioni, Marcello and Mahendru, Aroma and Maynez, Joshua and Misra, Vedant and Moussalem, Maysam and Nado, Zachary and Nham, John and Ni, Eric and Nystrom, Andrew and Parrish, Alicia and Pellat, Marie and Polacek, Martin and Polozov, Alex and Pope, Reiner and Qiao, Siyuan and Reif, Emily and Richter, Bryan and Riley, Parker and Ros, Alex Castro and Roy, Aurko and Saeta, Brennan and Samuel, Rajkumar and Shelby, Renee and Slone, Ambrose and Smilkov, Daniel and So, David R. and Sohn, Daniel and Tokumine, Simon and Valter, Dasha and Vasudevan, Vijay and Vodrahalli, Kiran and Wang, Xuezhi and Wang, Pidong and Wang, Zirui and Wang, Tao and Wieting, John and Wu, Yuhuai and Xu, Kelvin and Xu, Yunhan and Xue, Linting and Yin, Pengcheng and Yu, Jiahui and Zhang, Qiao and Zheng, Steven and Zheng, Ce and Zhou, Weikang and Zhou, Denny and Petrov, Slav and Wu, Yonghui},
  date = {2023-05-17},
  eprint = {2305.10403},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2305.10403},
  urldate = {2023-06-04},
  abstract = {We introduce PaLM 2, a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM. PaLM 2 is a Transformer-based model trained using a mixture of objectives. Through extensive evaluations on English and multilingual language, and reasoning tasks, we demonstrate that PaLM 2 has significantly improved quality on downstream tasks across different model sizes, while simultaneously exhibiting faster and more efficient inference compared to PaLM. This improved efficiency enables broader deployment while also allowing the model to respond faster, for a more natural pace of interaction. PaLM 2 demonstrates robust reasoning capabilities exemplified by large improvements over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable performance on a suite of responsible AI evaluations, and enables inference-time control over toxicity without additional overhead or impact on other capabilities. Overall, PaLM 2 achieves state-of-the-art performance across a diverse set of tasks and capabilities. When discussing the PaLM 2 family, it is important to distinguish between pre-trained models (of various sizes), fine-tuned variants of these models, and the user-facing products that use these models. In particular, user-facing products typically include additional pre- and post-processing steps. Additionally, the underlying models may evolve over time. Therefore, one should not expect the performance of user-facing products to exactly match the results reported in this report.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\E8CB7MLN\\Anil Á≠â - 2023 - PaLM 2 Technical Report.pdf;D\:\\Zotero\\literature_data\\storage\\ISN28QBP\\2305.html}
}

@inproceedings{asai2021,
  title = {One {{Question Answering Model}} for {{Many Languages}} with {{Cross-lingual Dense Passage Retrieval}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Asai, Akari and Yu, Xinyan and Kasai, Jungo and Hajishirzi, Hanna},
  date = {2021},
  volume = {34},
  pages = {7547--7560},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2021/hash/3df07fdae1ab273a967aaa1d355b8bb6-Abstract.html},
  urldate = {2023-11-29},
  abstract = {We present Cross-lingual Open-Retrieval Answer Generation (CORA), the first unified many-to-many question answering (QA) model that can answer questions across many languages, even for ones without language-specific annotated data or knowledge sources.We introduce a new dense passage retrieval algorithm that is trained to retrieve documents across languages for a question.Combined with a  multilingual autoregressive generation model, CORA answers directly in the target language without any translation or in-language retrieval modules as used in prior work. We propose an iterative training method that automatically extends annotated data available only in high-resource languages to low-resource ones. Our results show that CORA substantially outperforms the previous state of the art on multilingual open QA benchmarks across 26 languages, 9 of which are unseen during training. Our analyses show the significance of cross-lingual retrieval and generation in many languages, particularly under low-resource settings.},
  file = {D:\Zotero\literature_data\storage\BYY6LQHU\Asai Á≠â - 2021 - One Question Answering Model for Many Languages wi.pdf}
}

@article{ballester2016,
  title = {On the {{Performance}} of {{GoogLeNet}} and {{AlexNet Applied}} to {{Sketches}}},
  author = {Ballester, Pedro and Araujo, Ricardo},
  date = {2016-02-21},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {30},
  number = {1},
  issn = {2374-3468},
  doi = {10.1609/aaai.v30i1.10171},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/10171},
  urldate = {2023-04-12},
  abstract = {This work provides a study on how Convolutional Neural Networks, trained to identify objects primarily in photos, perform when applied to more abstract representations of the same objects. Our main goal is to better understand the generalization abilities of these networks and their learned inner representations. We show that both GoogLeNet and AlexNet networks are largely unable to recognize abstract sketches that are easily recognizable by humans. Moreover, we show that the measured efficacy vary considerably across different classes and we discuss possible reasons for this.},
  issue = {1},
  langid = {english},
  keywords = {Sketch Classification},
  file = {D:\Zotero\literature_data\storage\YRWPXB9Y\Ballester Âíå Araujo - 2016 - On the Performance of GoogLeNet and AlexNet Applie.pdf}
}

@inproceedings{bendersky2023,
  title = {{{SIGIR}} 2023 {{Workshop}} on {{Retrieval Enhanced Machine Learning}} ({{REML}} @ {{SIGIR}} 2023)},
  booktitle = {Proceedings of the 46th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Bendersky, Michael and Chen, Danqi and Diaz, Fernando and Zamani, Hamed},
  date = {2023-07-19},
  pages = {3468--3471},
  publisher = {{ACM}},
  location = {{Taipei Taiwan}},
  doi = {10.1145/3539618.3591925},
  url = {https://dl.acm.org/doi/10.1145/3539618.3591925},
  urldate = {2023-12-02},
  eventtitle = {{{SIGIR}} '23: {{The}} 46th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  isbn = {978-1-4503-9408-6},
  langid = {english},
  file = {D:\Zotero\literature_data\storage\79QQ47UV\Bendersky Á≠â - 2023 - SIGIR 2023 Workshop on Retrieval Enhanced Machine .pdf}
}

@article{bengio,
  title = {A {{Neural Probabilistic Language Model}}},
  author = {Bengio, Yoshua and Ducharme, R√©jean and Vincent, Pascal and Jauvin, Christian},
  abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
  langid = {english},
  file = {D:\Zotero\literature_data\storage\5X6TAHM7\Bengio Á≠â - A Neural Probabilistic Language Model.pdf}
}

@article{bielak2022,
  title = {Graph {{Barlow Twins}}: {{A}} Self-Supervised Representation Learning Framework for Graphs},
  shorttitle = {Graph {{Barlow Twins}}},
  author = {Bielak, Piotr and Kajdanowicz, Tomasz and Chawla, Nitesh V.},
  date = {2022-11},
  journaltitle = {Knowledge-Based Systems},
  shortjournal = {Knowledge-Based Systems},
  volume = {256},
  pages = {109631},
  issn = {09507051},
  doi = {10.1016/j.knosys.2022.109631},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S095070512200822X},
  urldate = {2023-07-26},
  abstract = {The self-supervised learning (SSL) paradigm is an essential exploration area, which tries to eliminate the need for expensive data labeling. Despite the great success of SSL methods in computer vision and natural language processing, most of them employ contrastive learning objectives that require negative samples, which are hard to define. This becomes even more challenging in the case of graphs and is a bottleneck for achieving robust representations. To overcome such limitations, we propose a framework for self-supervised graph representation learning ‚Äì Graph Barlow Twins, which utilizes a cross-correlation-based loss function instead of negative samples. Moreover, it does not rely on non-symmetric neural network architectures ‚Äì in contrast to state-of-the-art self-supervised graph representation learning method BGRL. We show that our method achieves as competitive results as the best self-supervised methods and fully supervised ones while requiring fewer hyperparameters and substantially shorter computation time (ca. 30 times faster than BGRL).},
  langid = {english},
  file = {D:\Zotero\literature_data\storage\KWXZBNMI\Bielak Á≠â - 2022 - Graph Barlow Twins A self-supervised representati.pdf}
}

@online{bohnet2023,
  title = {Attributed {{Question Answering}}: {{Evaluation}} and {{Modeling}} for {{Attributed Large Language Models}}},
  shorttitle = {Attributed {{Question Answering}}},
  author = {Bohnet, Bernd and Tran, Vinh Q. and Verga, Pat and Aharoni, Roee and Andor, Daniel and Soares, Livio Baldini and Ciaramita, Massimiliano and Eisenstein, Jacob and Ganchev, Kuzman and Herzig, Jonathan and Hui, Kai and Kwiatkowski, Tom and Ma, Ji and Ni, Jianmo and Saralegui, Lierni Sestorain and Schuster, Tal and Cohen, William W. and Collins, Michael and Das, Dipanjan and Metzler, Donald and Petrov, Slav and Webster, Kellie},
  date = {2023-02-10},
  eprint = {2212.08037},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2212.08037},
  urldate = {2023-11-29},
  abstract = {Large language models (LLMs) have shown impressive results while requiring little or no direct supervision. Further, there is mounting evidence that LLMs may have potential in information-seeking scenarios. We believe the ability of an LLM to attribute the text that it generates is likely to be crucial in this setting. We formulate and study Attributed QA as a key first step in the development of attributed LLMs. We propose a reproducible evaluation framework for the task and benchmark a broad set of architectures. We take human annotations as a gold standard and show that a correlated automatic metric is suitable for development. Our experimental work gives concrete answers to two key questions (How to measure attribution?, and How well do current state-of-the-art methods perform on attribution?), and give some hints as to how to address a third (How to build LLMs with attribution?).},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\Y5FGPZYM\\Bohnet Á≠â - 2023 - Attributed Question Answering Evaluation and Mode.pdf;D\:\\Zotero\\literature_data\\storage\\HDTHZACP\\2212.html}
}

@online{bohnet2023a,
  title = {Attributed {{Question Answering}}: {{Evaluation}} and {{Modeling}} for {{Attributed Large Language Models}}},
  shorttitle = {Attributed {{Question Answering}}},
  author = {Bohnet, Bernd and Tran, Vinh Q. and Verga, Pat and Aharoni, Roee and Andor, Daniel and Soares, Livio Baldini and Ciaramita, Massimiliano and Eisenstein, Jacob and Ganchev, Kuzman and Herzig, Jonathan and Hui, Kai and Kwiatkowski, Tom and Ma, Ji and Ni, Jianmo and Saralegui, Lierni Sestorain and Schuster, Tal and Cohen, William W. and Collins, Michael and Das, Dipanjan and Metzler, Donald and Petrov, Slav and Webster, Kellie},
  date = {2023-02-10},
  eprint = {2212.08037},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2212.08037},
  url = {http://arxiv.org/abs/2212.08037},
  urldate = {2023-11-14},
  abstract = {Large language models (LLMs) have shown impressive results while requiring little or no direct supervision. Further, there is mounting evidence that LLMs may have potential in information-seeking scenarios. We believe the ability of an LLM to attribute the text that it generates is likely to be crucial in this setting. We formulate and study Attributed QA as a key first step in the development of attributed LLMs. We propose a reproducible evaluation framework for the task and benchmark a broad set of architectures. We take human annotations as a gold standard and show that a correlated automatic metric is suitable for development. Our experimental work gives concrete answers to two key questions (How to measure attribution?, and How well do current state-of-the-art methods perform on attribution?), and give some hints as to how to address a third (How to build LLMs with attribution?).},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\FW84PCVL\\Bohnet Á≠â - 2023 - Attributed Question Answering Evaluation and Mode.pdf;D\:\\Zotero\\literature_data\\storage\\QX96I92T\\2212.html}
}

@online{bowman2015,
  title = {A Large Annotated Corpus for Learning Natural Language Inference},
  author = {Bowman, Samuel R. and Angeli, Gabor and Potts, Christopher and Manning, Christopher D.},
  date = {2015-08-21},
  eprint = {1508.05326},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1508.05326},
  url = {http://arxiv.org/abs/1508.05326},
  urldate = {2023-05-16},
  abstract = {Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\BL3D3VG5\\Bowman Á≠â - 2015 - A large annotated corpus for learning natural lang.pdf;D\:\\Zotero\\literature_data\\storage\\I8MARHAT\\1508.html}
}

@online{brown2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  date = {2020-07-22},
  eprint = {2005.14165},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2005.14165},
  urldate = {2023-04-17},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions ‚Äì something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3‚Äôs few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,GPT-3},
  file = {D:\Zotero\literature_data\storage\ETDCMKME\Brown Á≠â - 2020 - Language Models are Few-Shot Learners.pdf}
}

@online{bubeck2023,
  title = {Sparks of {{Artificial General Intelligence}}: {{Early}} Experiments with {{GPT-4}}},
  shorttitle = {Sparks of {{Artificial General Intelligence}}},
  author = {Bubeck, S√©bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
  date = {2023-04-13},
  eprint = {2303.12712},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.12712},
  url = {http://arxiv.org/abs/2303.12712},
  urldate = {2023-06-03},
  abstract = {Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\YWRZ464E\\Bubeck Á≠â - 2023 - Sparks of Artificial General Intelligence Early e.pdf;D\:\\Zotero\\literature_data\\storage\\KLTANBP3\\2303.html}
}

@online{bubeck2023a,
  title = {Sparks of {{Artificial General Intelligence}}: {{Early}} Experiments with {{GPT-4}}},
  shorttitle = {Sparks of {{Artificial General Intelligence}}},
  author = {Bubeck, S√©bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
  date = {2023-04-13},
  eprint = {2303.12712},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2303.12712},
  urldate = {2023-05-09},
  abstract = {Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4 [Ope23], was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT4 is part of a new cohort of LLMs (along with ChatGPT and Google‚Äôs PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4‚Äôs performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4‚Äôs capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {D:\Zotero\literature_data\storage\IEYTQ8DK\Bubeck Á≠â - 2023 - Sparks of Artificial General Intelligence Early e.pdf}
}

@article{cao2016,
  title = {Deep {{Neural Networks}} for {{Learning Graph Representations}}},
  author = {Cao, Shaosheng and Lu, Wei and Xu, Qiongkai},
  date = {2016-02-21},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {30},
  number = {1},
  issn = {2374-3468},
  doi = {10.1609/aaai.v30i1.10179},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/10179},
  urldate = {2023-06-21},
  abstract = {In this paper, we propose a novel model for learning graph representations, which generates a low-dimensional vector representation for each vertex by capturing the graph structural information. Different from other previous research efforts, we adopt a random surfing model to capture graph structural information directly, instead of using the sampling-based method for generating linear sequences proposed by Perozzi et al. (2014). The advantages of our approach will be illustrated from both theorical and empirical perspectives. We also give a new perspective for the matrix factorization method proposed by Levy and Goldberg (2014), in which the pointwise mutual information (PMI) matrix is considered as an analytical solution to the objective function of the skip-gram model with negative sampling proposed by Mikolov et al. (2013). Unlike their approach which involves the use of the SVD for finding the low-dimensitonal projections from the PMI matrix, however, the stacked denoising autoencoder is introduced in our model to extract complex features and model non-linearities. To demonstrate the effectiveness of our model, we conduct experiments on clustering and visualization tasks, employing the learned vertex representations as features. Empirical results on datasets of varying sizes show that our model outperforms other stat-of-the-art models in such tasks.},
  issue = {1},
  langid = {english},
  file = {D:\Zotero\literature_data\storage\GMU5GGZV\Cao Á≠â - 2016 - Deep Neural Networks for Learning Graph Representa.pdf}
}

@article{chang,
  title = {A {{Survey}} on {{Evaluation}} of {{Large Language Models}}},
  author = {Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Zhu, Kaijie and Chen, Hao and Yang, Linyi and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and Ye, Wei and Zhang, Yue and Chang, Yi and Yu, Philip S and Yang, Qiang and Xie, Xing},
  abstract = {Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the ‚Äòwhere‚Äô and ‚Äòhow‚Äô questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey.},
  langid = {english},
  keywords = {ÁªºËø∞},
  file = {D:\Zotero\literature_data\storage\NH4NAU3M\Chang Á≠â - A Survey on Evaluation of Large Language Models.pdf}
}

@online{chen2020,
  title = {Exploring {{Simple Siamese Representation Learning}}},
  author = {Chen, Xinlei and He, Kaiming},
  date = {2020-11-20},
  eprint = {2011.10566},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2011.10566},
  url = {http://arxiv.org/abs/2011.10566},
  urldate = {2023-07-24},
  abstract = {Siamese networks have become a common structure in various recent models for unsupervised visual representation learning. These models maximize the similarity between two augmentations of one image, subject to certain conditions for avoiding collapsing solutions. In this paper, we report surprising empirical results that simple Siamese networks can learn meaningful representations even using none of the following: (i) negative sample pairs, (ii) large batches, (iii) momentum encoders. Our experiments show that collapsing solutions do exist for the loss and structure, but a stop-gradient operation plays an essential role in preventing collapsing. We provide a hypothesis on the implication of stop-gradient, and further show proof-of-concept experiments verifying it. Our "SimSiam" method achieves competitive results on ImageNet and downstream tasks. We hope this simple baseline will motivate people to rethink the roles of Siamese architectures for unsupervised representation learning. Code will be made available.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,SimSiam},
  file = {D\:\\Zotero\\literature_data\\storage\\DLB8TV8I\\Chen Âíå He - 2020 - Exploring Simple Siamese Representation Learning.pdf;D\:\\Zotero\\literature_data\\storage\\BPM4JNKR\\2011.html}
}

@online{chen2020a,
  title = {A {{Simple Framework}} for {{Contrastive Learning}} of {{Visual Representations}}},
  author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  date = {2020-06-30},
  eprint = {2002.05709},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2002.05709},
  urldate = {2023-07-22},
  abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,SimCLR,Statistics - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\4ZBZN6MU\\Chen Á≠â - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf;D\:\\Zotero\\literature_data\\storage\\Z8GIFY8R\\2002.html}
}

@online{chen2021,
  title = {Evaluating {{Large Language Models Trained}} on {{Code}}},
  author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
  date = {2021-07-14},
  eprint = {2107.03374},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2107.03374},
  url = {http://arxiv.org/abs/2107.03374},
  urldate = {2023-06-01},
  abstract = {We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8\% of the problems, while GPT-3 solves 0\% and GPT-J solves 11.4\%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2\% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\ALWGVAWZ\\Chen Á≠â - 2021 - Evaluating Large Language Models Trained on Code.pdf;D\:\\Zotero\\literature_data\\storage\\ZK99ZX2Q\\2107.html}
}

@inproceedings{chen2022,
  title = {Rich {{Knowledge Sources Bring Complex Knowledge Conflicts}}: {{Recalibrating Models}} to {{Reflect Conflicting Evidence}}},
  shorttitle = {Rich {{Knowledge Sources Bring Complex Knowledge Conflicts}}},
  booktitle = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Chen, Hung-Ting and Zhang, Michael and Choi, Eunsol},
  editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
  date = {2022-12},
  pages = {2292--2307},
  publisher = {{Association for Computational Linguistics}},
  location = {{Abu Dhabi, United Arab Emirates}},
  doi = {10.18653/v1/2022.emnlp-main.146},
  url = {https://aclanthology.org/2022.emnlp-main.146},
  urldate = {2024-01-23},
  abstract = {Question answering models can use rich knowledge sources ‚Äî up to one hundred retrieved passages and parametric knowledge in the large-scale language model (LM). Prior work assumes information in such knowledge sources is consistent with each other, paying little attention to how models blend information stored in their LM parameters with that from retrieved evidence documents. In this paper, we simulate knowledge conflicts (i.e., where parametric knowledge suggests one answer and different passages suggest different answers) and examine model behaviors. We find retrieval performance heavily impacts which sources models rely on, and current models mostly rely on non-parametric knowledgein their best-performing settings. We discover a troubling trend that contradictions among knowledge sources affect model confidence only marginally. To address this issue, we present a new calibration study, where models are discouraged from presenting any single answer when presented with multiple conflicting answer candidates in retrieved evidences.},
  eventtitle = {{{EMNLP}} 2022},
  file = {D:\Zotero\literature_data\storage\EFAWGA9M\Chen Á≠â - 2022 - Rich Knowledge Sources Bring Complex Knowledge Con.pdf}
}

@online{chen2023,
  title = {Understanding {{Retrieval Augmentation}} for {{Long-Form Question Answering}}},
  author = {Chen, Hung-Ting and Xu, Fangyuan and Arora, Shane and Choi, Eunsol},
  date = {2023-10-18},
  eprint = {2310.12150},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2310.12150},
  urldate = {2023-11-29},
  abstract = {We present a study of retrieval-augmented language models (LMs) on long-form question answering. We analyze how retrieval augmentation impacts different LMs, by comparing answers generated from models while using the same evidence documents, and how differing quality of retrieval document set impacts the answers generated from the same LM. We study various attributes of generated answers (e.g., fluency, length, variance) with an emphasis on the attribution of generated long-form answers to in-context evidence documents. We collect human annotations of answer attribution and evaluate methods for automatically judging attribution. Our study provides new insights on how retrieval augmentation impacts long, knowledge-rich text generation of LMs. We further identify attribution patterns for long text generation and analyze the main culprits of attribution errors. Together, our analysis reveals how retrieval augmentation impacts long knowledge-rich text generation and provide directions for future work.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,LFQA,ËØÑ‰º∞RAG},
  file = {D\:\\Zotero\\literature_data\\storage\\F4GUYFUS\\Chen Á≠â - 2023 - Understanding Retrieval Augmentation for Long-Form.pdf;D\:\\Zotero\\literature_data\\storage\\C574RXM6\\2310.html}
}

@online{chen2023a,
  title = {Learning to {{Evaluate}} the {{Artness}} of {{AI-generated Images}}},
  author = {Chen, Junyu and An, Jie and Lyu, Hanjia and Luo, Jiebo},
  date = {2023-05-08},
  eprint = {2305.04923},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2305.04923},
  urldate = {2023-10-18},
  abstract = {Assessing the artness of AI-generated images continues to be a challenge within the realm of image generation. Most existing metrics cannot be used to perform instance-level and reference-free artness evaluation. This paper presents ArtScore, a metric designed to evaluate the degree to which an image resembles authentic artworks by artists (or conversely photographs), thereby offering a novel approach to artness assessment. We first blend pre-trained models for photo and artwork generation, resulting in a series of mixed models. Subsequently, we utilize these mixed models to generate images exhibiting varying degrees of artness with pseudo-annotations. Each photorealistic image has a corresponding artistic counterpart and a series of interpolated images that range from realistic to artistic. This dataset is then employed to train a neural network that learns to estimate quantized artness levels of arbitrary images. Extensive experiments reveal that the artness levels predicted by ArtScore align more closely with human artistic evaluation than existing evaluation metrics, such as Gram loss and ArtFID.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {D:\Zotero\literature_data\storage\2B24IHEH\Chen Á≠â - 2023 - Learning to Evaluate the Artness of AI-generated I.pdf}
}

@article{chen2023b,
  title = {Graph {{Enhanced Transformer}} for {{Aspect Category Detection}}},
  author = {Chen, Chen and Wang, Hou-Feng and Zhu, Qing-Qing and Liu, Jun-Fei},
  date = {2023-06},
  journaltitle = {Journal of Computer Science and Technology},
  shortjournal = {J. Comput. Sci. Technol.},
  volume = {38},
  number = {3},
  pages = {612--625},
  issn = {1000-9000, 1860-4749},
  doi = {10.1007/s11390-021-1000-1},
  url = {https://link.springer.com/10.1007/s11390-021-1000-1},
  urldate = {2023-08-24},
  abstract = {Aspect category detection is one challenging subtask of aspect based sentiment analysis, which categorizes a review sentence into a set of predefined aspect categories. Most existing methods regard the aspect category detection as a flat classification problem. However, aspect categories are inter-related, and they are usually organized with a hierarchical tree structure. To leverage the structure information, this paper proposes a hierarchical multi-label classification model to detect aspect categories and uses a graph enhanced transformer network to integrate label dependency information into prediction features. Experiments have been conducted on four widely-used benchmark datasets, showing that the proposed model outperforms all strong baselines.},
  langid = {english},
  keywords = {1},
  file = {D:\Zotero\literature_data\storage\NJRTSUSU\Chen Á≠â - 2023 - Graph Enhanced Transformer for Aspect Category Det.pdf}
}

@online{chen2023c,
  title = {From {{Node Interaction}} to {{Hop Interaction}}: {{New Effective}} and {{Scalable Graph Learning Paradigm}}},
  shorttitle = {From {{Node Interaction}} to {{Hop Interaction}}},
  author = {Chen, Jie and Li, Zilong and Zhu, Yin and Zhang, Junping and Pu, Jian},
  date = {2023-04-13},
  eprint = {2211.11761},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2211.11761},
  urldate = {2023-05-30},
  abstract = {Existing Graph Neural Networks (GNNs) follow the message-passing mechanism that conducts information interaction among nodes iteratively. While considerable progress has been made, such node interaction paradigms still have the following limitation. First, the scalability limitation precludes the broad application of GNNs in large-scale industrial settings since the node interaction among rapidly expanding neighbors incurs high computation and memory costs. Second, the over-smoothing problem restricts the discrimination ability of nodes, i.e., node representations of different classes will converge to indistinguishable after repeated node interactions. In this work, we propose a novel hop interaction paradigm to address these limitations simultaneously. The core idea is to convert the interaction target among nodes to pre-processed multi-hop features inside each node. We design a simple yet effective HopGNN framework that can easily utilize existing GNNs to achieve hop interaction. Furthermore, we propose a multi-task learning strategy with a self-supervised learning objective to enhance HopGNN. We conduct extensive experiments on 12 benchmark datasets in a wide range of domains, scales, and smoothness of graphs. Experimental results show that our methods achieve superior performance while maintaining high scalability and efficiency. The code is at https://github.com/JC-202/HopGNN.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\RHBFLEYE\\Chen Á≠â - 2023 - From Node Interaction to Hop Interaction New Effe.pdf;D\:\\Zotero\\literature_data\\storage\\P9ZNLU5G\\2211.html}
}

@online{chen2023d,
  title = {Complex {{Claim Verification}} with {{Evidence Retrieved}} in the {{Wild}}},
  author = {Chen, Jifan and Kim, Grace and Sriram, Aniruddh and Durrett, Greg and Choi, Eunsol},
  date = {2023-05-19},
  eprint = {2305.11859},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2305.11859},
  urldate = {2024-01-10},
  abstract = {Evidence retrieval is a core part of automatic fact-checking. Prior work makes simplifying assumptions in retrieval that depart from real-world use cases: either no access to evidence, access to evidence curated by a human fact-checker, or access to evidence available long after the claim has been made. In this work, we present the first fully automated pipeline to check real-world claims by retrieving raw evidence from the web. We restrict our retriever to only search documents available prior to the claim's making, modeling the realistic scenario where an emerging claim needs to be checked. Our pipeline includes five components: claim decomposition, raw document retrieval, fine-grained evidence retrieval, claim-focused summarization, and veracity judgment. We conduct experiments on complex political claims in the ClaimDecomp dataset and show that the aggregated evidence produced by our pipeline improves veracity judgments. Human evaluation finds the evidence summary produced by our system is reliable (it does not hallucinate information) and relevant to answering key questions about a claim, suggesting that it can assist fact-checkers even when it cannot surface a complete evidence set.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\L5WRTRLF\\Chen Á≠â - 2023 - Complex Claim Verification with Evidence Retrieved.pdf;D\:\\Zotero\\literature_data\\storage\\PC4YWPJS\\2305.html}
}

@online{chern2023,
  title = {{{FacTool}}: {{Factuality Detection}} in {{Generative AI}} -- {{A Tool Augmented Framework}} for {{Multi-Task}} and {{Multi-Domain Scenarios}}},
  shorttitle = {{{FacTool}}},
  author = {Chern, I.-Chun and Chern, Steffi and Chen, Shiqi and Yuan, Weizhe and Feng, Kehua and Zhou, Chunting and He, Junxian and Neubig, Graham and Liu, Pengfei},
  date = {2023-07-26},
  eprint = {2307.13528},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2307.13528},
  urldate = {2023-11-29},
  abstract = {The emergence of generative pre-trained models has facilitated the synthesis of high-quality text, but it has also posed challenges in identifying factual errors in the generated text. In particular: (1) A wider range of tasks now face an increasing risk of containing factual errors when handled by generative models. (2) Generated texts tend to be lengthy and lack a clearly defined granularity for individual facts. (3) There is a scarcity of explicit evidence available during the process of fact checking. With the above challenges in mind, in this paper, we propose FacTool, a task and domain agnostic framework for detecting factual errors of texts generated by large language models (e.g., ChatGPT). Experiments on four different tasks (knowledge-based QA, code generation, mathematical reasoning, and scientific literature review) show the efficacy of the proposed method. We release the code of FacTool associated with ChatGPT plugin interface at https://github.com/GAIR-NLP/factool .},
  pubstate = {preprint},
  version = {2},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\N6E7S8NH\\Chern Á≠â - 2023 - FacTool Factuality Detection in Generative AI -- .pdf;D\:\\Zotero\\literature_data\\storage\\HLVBYXQT\\2307.html}
}

@online{cho2022,
  title = {{{DALL-Eval}}: {{Probing}} the {{Reasoning Skills}} and {{Social Biases}} of {{Text-to-Image Generative Models}}},
  shorttitle = {{{DALL-Eval}}},
  author = {Cho, Jaemin and Zala, Abhay and Bansal, Mohit},
  date = {2022-11-14},
  eprint = {2202.04053},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2202.04053},
  urldate = {2023-06-24},
  abstract = {Recently, DALL-E, a multimodal transformer language model, and its variants (including diffusion models) have shown high-quality text-to-image generation capabilities. However, despite the interesting image generation results, there has not been a detailed analysis on how to evaluate such models. In this work, we investigate the visual reasoning capabilities and social biases of different text-to-image models, covering both multimodal transformer language models and diffusion models. First, we measure three visual reasoning skills: object recognition, object counting, and spatial relation understanding. For this, we propose PaintSkills, a compositional diagnostic dataset and evaluation toolkit that measures these skills. In our experiments, there exists a large gap between the performance of recent text-to-image models and the upper bound accuracy in object counting and spatial relation understanding skills. Second, we assess gender and skin tone biases by measuring the variance of the gender/skin tone distribution based on automated and human evaluation. We demonstrate that recent text-to-image models learn specific gender/skin tone biases from web image-text pairs. We hope that our work will help guide future progress in improving text-to-image generation models on visual reasoning skills and learning socially unbiased representations. Code and data: https://github.com/j-min/DallEval},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {D:\Zotero\literature_data\storage\8N3TMH9G\Cho Á≠â - 2022 - DALL-Eval Probing the Reasoning Skills and Social.pdf}
}

@online{chu2023,
  title = {{{TimeBench}}: {{A Comprehensive Evaluation}} of {{Temporal Reasoning Abilities}} in {{Large Language Models}}},
  shorttitle = {{{TimeBench}}},
  author = {Chu, Zheng and Chen, Jingchang and Chen, Qianglong and Yu, Weijiang and Wang, Haotian and Liu, Ming and Qin, Bing},
  date = {2023-11-29},
  eprint = {2311.17667},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2311.17667},
  urldate = {2023-11-30},
  abstract = {Understanding time is a pivotal aspect of human cognition, crucial in the broader framework of grasping the intricacies of the world. Previous studies typically focus on specific aspects of time, lacking a comprehensive temporal reasoning benchmark. To address this issue, we propose TimeBench, a comprehensive hierarchical temporal reasoning benchmark that covers a broad spectrum of temporal reasoning phenomena, which provides a thorough evaluation for investigating the temporal reasoning capabilities of large language models. We conduct extensive experiments on popular LLMs, such as GPT-4, LLaMA2, and Mistral, incorporating chain-of-thought prompting. Our experimental results indicate a significant performance gap between the state-of-the-art LLMs and humans, highlighting that there is still a considerable distance to cover in temporal reasoning. We aspire for TimeBench to serve as a comprehensive benchmark, fostering research in temporal reasoning for LLMs. Our resource is available at https://github.com/zchuz/TimeBench},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\9ACZUJ4U\\Chu Á≠â - 2023 - TimeBench A Comprehensive Evaluation of Temporal .pdf;D\:\\Zotero\\literature_data\\storage\\ARA6MIQ6\\2311.html}
}

@online{chung2022,
  title = {Scaling {{Instruction-Finetuned Language Models}}},
  author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Castro-Ros, Alex and Pellat, Marie and Robinson, Kevin and Valter, Dasha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},
  date = {2022-12-06},
  eprint = {2210.11416},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2210.11416},
  url = {http://arxiv.org/abs/2210.11416},
  urldate = {2023-06-03},
  abstract = {Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4\% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2\% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\6A6P6YUL\\Chung Á≠â - 2022 - Scaling Instruction-Finetuned Language Models.pdf;D\:\\Zotero\\literature_data\\storage\\GMC3KK7U\\2210.html}
}

@online{constantin2023,
  type = {Substack newsletter},
  title = {"{{Scaling Laws}}" for {{AI And Some Implications}}},
  author = {Constantin, Sarah},
  date = {2023-04-13},
  url = {https://sarahconstantin.substack.com/p/scaling-laws-for-ai-and-some-implications},
  urldate = {2023-05-08},
  abstract = {How much bigger (and better) can LLMs get in the 2020's?},
  organization = {{Rough Diamonds}},
  keywords = {Ê®°Âûãloss-ËßÑÊ®°È¢ÑÊµã},
  file = {D:\Zotero\literature_data\storage\RDKTA9WX\scaling-laws-for-ai-and-some-implications.html}
}

@online{dai2015,
  title = {Semi-Supervised {{Sequence Learning}}},
  author = {Dai, Andrew M. and Le, Quoc V.},
  date = {2015-11-04},
  url = {https://arxiv.org/abs/1511.01432v1},
  urldate = {2023-05-17},
  abstract = {We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a "pretraining" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups.},
  langid = {english},
  organization = {{arXiv.org}},
  file = {D:\Zotero\literature_data\storage\RNTCZLNB\Dai Âíå Le - 2015 - Semi-supervised Sequence Learning.pdf}
}

@article{deisenroth,
  title = {Mathematics for {{Machine Learning}}},
  author = {Deisenroth, Marc Peter and Faisal, A Aldo and Ong, Cheng Soon},
  langid = {english},
  file = {D:\Zotero\literature_data\storage\EV6FKBX3\Deisenroth Á≠â - Mathematics for Machine Learning.pdf}
}

@online{deletang2023,
  title = {Language {{Modeling Is Compression}}},
  author = {Del√©tang, Gr√©goire and Ruoss, Anian and Duquenne, Paul-Ambroise and Catt, Elliot and Genewein, Tim and Mattern, Christopher and Grau-Moya, Jordi and Wenliang, Li Kevin and Aitchison, Matthew and Orseau, Laurent and Hutter, Marcus and Veness, Joel},
  date = {2023-09-19},
  eprint = {2309.10668},
  eprinttype = {arxiv},
  eprintclass = {cs, math},
  url = {http://arxiv.org/abs/2309.10668},
  urldate = {2023-11-23},
  abstract = {It has long been established that predictive models can be transformed into lossless compressors and vice versa. Incidentally, in recent years, the machine learning community has focused on training increasingly large and powerful self-supervised (language) models. Since these large language models exhibit impressive predictive capabilities, they are well-positioned to be strong compressors. In this work, we advocate for viewing the prediction problem through the lens of compression and evaluate the compression capabilities of large (foundation) models. We show that large language models are powerful general-purpose predictors and that the compression viewpoint provides novel insights into scaling laws, tokenization, and in-context learning. For example, Chinchilla 70B, while trained primarily on text, compresses ImageNet patches to 43.4\% and LibriSpeech samples to 16.4\% of their raw size, beating domain-specific compressors like PNG (58.5\%) or FLAC (30.3\%), respectively. Finally, we show that the prediction-compression equivalence allows us to use any compressor (like gzip) to build a conditional generative model.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Theory,Computer Science - Machine Learning,ÂéãÁº©Âç≥Êô∫ËÉΩ},
  file = {D\:\\Zotero\\literature_data\\storage\\4EGLHNYM\\Del√©tang Á≠â - 2023 - Language Modeling Is Compression.pdf;D\:\\Zotero\\literature_data\\storage\\3V7K8VPC\\2309.html}
}

@inproceedings{deng2009,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  shorttitle = {{{ImageNet}}},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  date = {2009-06},
  pages = {248--255},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2009.5206848},
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ‚ÄúImageNet‚Äù, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500‚Äì1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  eventtitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  keywords = {Explosions,Image databases,Image retrieval,Information retrieval,Internet,Large-scale systems,Multimedia databases,Ontologies,Robustness,Spine},
  file = {D\:\\Zotero\\literature_data\\storage\\AMENZ3D2\\Deng Á≠â - 2009 - ImageNet A large-scale hierarchical image databas.pdf;D\:\\Zotero\\literature_data\\storage\\SN8GTIFR\\5206848.html}
}

@article{deng2017,
  title = {Image {{Aesthetic Assessment}}: {{An Experimental Survey}}},
  shorttitle = {Image {{Aesthetic Assessment}}},
  author = {Deng, Yubin and Loy, Chen Change and Tang, Xiaoou},
  date = {2017-07},
  journaltitle = {IEEE Signal Processing Magazine},
  shortjournal = {IEEE Signal Process. Mag.},
  volume = {34},
  number = {4},
  eprint = {1610.00838},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {80--106},
  issn = {1053-5888, 1558-0792},
  doi = {10.1109/MSP.2017.2696576},
  url = {http://arxiv.org/abs/1610.00838},
  urldate = {2023-07-25},
  abstract = {This survey aims at reviewing recent computer vision techniques used in the assessment of image aesthetic quality. Image aesthetic assessment aims at computationally distinguishing high-quality photos from low-quality ones based on photographic rules, typically in the form of binary classification or quality scoring. A variety of approaches has been proposed in the literature trying to solve this challenging problem. In this survey, we present a systematic listing of the reviewed approaches based on visual feature types (hand-crafted features and deep features) and evaluation criteria (dataset characteristics and evaluation metrics). Main contributions and novelties of the reviewed approaches are highlighted and discussed. In addition, following the emergence of deep learning techniques, we systematically evaluate recent deep learning settings that are useful for developing a robust deep model for aesthetic scoring. Experiments are conducted using simple yet solid baselines that are competitive with the current state-of-the-arts. Moreover, we discuss the possibility of manipulating the aesthetics of images through computational approaches. We hope that our survey could serve as a comprehensive reference source for future research on the study of image aesthetic assessment.},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D:\Zotero\literature_data\storage\J7ZJGGHE\Deng Á≠â - 2017 - Image Aesthetic Assessment An Experimental Survey.pdf}
}

@inproceedings{dinh2022,
  title = {{{TISE}}: {{Bag}} of~{{Metrics}} for~{{Text-to-Image Synthesis Evaluation}}},
  shorttitle = {{{TISE}}},
  booktitle = {Computer {{Vision}} ‚Äì {{ECCV}} 2022},
  author = {Dinh, Tan M. and Nguyen, Rang and Hua, Binh-Son},
  editor = {Avidan, Shai and Brostow, Gabriel and Ciss√©, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
  date = {2022},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {594--609},
  publisher = {{Springer Nature Switzerland}},
  location = {{Cham}},
  doi = {10.1007/978-3-031-20059-5_34},
  abstract = {In this paper, we conduct a study on the state-of-the-art methods for text-to-image synthesis and propose a framework to evaluate these methods. We consider syntheses where an image contains a single or multiple objects. Our study outlines several issues in the current evaluation pipeline: (i) for image quality assessment, a commonly used metric, e.g., Inception Score (IS), is often either miscalibrated for the single-object case or misused for the multi-object case; (ii) for text relevance and object accuracy assessment, there is an overfitting phenomenon in the existing R-precision (RP) and Semantic Object Accuracy (SOA) metrics, respectively; (iii) for multi-object case, many vital factors for evaluation, e.g., object fidelity, positional alignment, counting alignment, are largely dismissed; (iv) the ranking of the methods based on current metrics is highly inconsistent with real images. To overcome these issues, we propose a combined set of existing and new metrics to systematically evaluate the methods. For existing metrics, we offer an improved version of IS named IS* by using temperature scaling to calibrate the confidence of the classifier used by IS; we also propose a solution to mitigate the overfitting issues of RP and SOA. For new metrics, we develop counting alignment, positional alignment, object-centric IS, and object-centric FID metrics for evaluating the multi-object case. We show that benchmarking with our bag of metrics results in a highly consistent ranking among existing methods that is well-aligned with human evaluation. As a by-product, we create AttnGAN++, a simple but strong baseline for the benchmark by stabilizing the training of AttnGAN using spectral normalization. We also release our toolbox, so-called TISE, for advocating fair and consistent evaluation of text-to-image models.},
  isbn = {978-3-031-20059-5},
  langid = {english},
  keywords = {Language and vision,Metrics,Text-to-image synthesis},
  file = {D:\Zotero\literature_data\storage\U55VTTVU\Dinh Á≠â - 2022 - TISE Bag of¬†Metrics for¬†Text-to-Image Synthesis E.pdf}
}

@online{dosovitskiy2021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  date = {2021-06-03},
  eprint = {2010.11929},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2010.11929},
  urldate = {2023-07-21},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,ViT},
  file = {D\:\\Zotero\\literature_data\\storage\\KGINZC7S\\Dosovitskiy Á≠â - 2021 - An Image is Worth 16x16 Words Transformers for Im.pdf;D\:\\Zotero\\literature_data\\storage\\9PHRWQNW\\2010.html}
}

@online{du2023,
  title = {Quantifying and {{Attributing}} the {{Hallucination}} of {{Large Language Models}} via {{Association Analysis}}},
  author = {Du, Li and Wang, Yequan and Xing, Xingrun and Ya, Yiqun and Li, Xiang and Jiang, Xin and Fang, Xuezhi},
  date = {2023-09-10},
  eprint = {2309.05217},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2309.05217},
  urldate = {2023-11-29},
  abstract = {Although demonstrating superb performance on various NLP tasks, large language models (LLMs) still suffer from the hallucination problem, which threatens the reliability of LLMs. To measure the level of hallucination of LLMs, previous works first categorize the hallucination according to the phenomenon similarity, then quantify the proportion that model outputs contain hallucinatory contents. However, such hallucination rates could easily be distorted by confounders. Moreover, such hallucination rates could not reflect the reasons for the hallucination, as similar hallucinatory phenomena may originate from different sources. To address these issues, we propose to combine the hallucination level quantification and hallucination reason investigation through an association analysis, which builds the relationship between the hallucination rate of LLMs with a set of risk factors. In this way, we are able to observe the hallucination level under each value of each risk factor, examining the contribution and statistical significance of each risk factor, meanwhile excluding the confounding effect of other factors. Additionally, by recognizing the risk factors according to a taxonomy of model capability, we reveal a set of potential deficiencies in commonsense memorization, relational reasoning, and instruction following, which may further provide guidance for the pretraining and supervised fine-tuning process of LLMs to mitigate the hallucination.},
  pubstate = {preprint},
  version = {1},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\DLSLIYCI\\Du Á≠â - 2023 - Quantifying and Attributing the Hallucination of L.pdf;D\:\\Zotero\\literature_data\\storage\\LNELC5TC\\2309.html}
}

@online{dwivedi2021,
  title = {A {{Generalization}} of {{Transformer Networks}} to {{Graphs}}},
  author = {Dwivedi, Vijay Prakash and Bresson, Xavier},
  date = {2021-01-24},
  eprint = {2012.09699},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2012.09699},
  url = {http://arxiv.org/abs/2012.09699},
  urldate = {2023-07-01},
  abstract = {We propose a generalization of transformer neural network architecture for arbitrary graphs. The original transformer was designed for Natural Language Processing (NLP), which operates on fully connected graphs representing all connections between the words in a sequence. Such architecture does not leverage the graph connectivity inductive bias, and can perform poorly when the graph topology is important and has not been encoded into the node features. We introduce a graph transformer with four new properties compared to the standard model. First, the attention mechanism is a function of the neighborhood connectivity for each node in the graph. Second, the positional encoding is represented by the Laplacian eigenvectors, which naturally generalize the sinusoidal positional encodings often used in NLP. Third, the layer normalization is replaced by a batch normalization layer, which provides faster training and better generalization performance. Finally, the architecture is extended to edge feature representation, which can be critical to tasks s.a. chemistry (bond type) or link prediction (entity relationship in knowledge graphs). Numerical experiments on a graph benchmark demonstrate the performance of the proposed graph transformer architecture. This work closes the gap between the original transformer, which was designed for the limited case of line graphs, and graph neural networks, that can work with arbitrary graphs. As our architecture is simple and generic, we believe it can be used as a black box for future applications that wish to consider transformer and graphs.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Graph Transformer},
  file = {D\:\\Zotero\\literature_data\\storage\\UIQG7QJG\\Dwivedi Âíå Bresson - 2021 - A Generalization of Transformer Networks to Graphs.pdf;D\:\\Zotero\\literature_data\\storage\\DYEZCTN9\\2012.html}
}

@online{dziri2022,
  title = {Evaluating {{Attribution}} in {{Dialogue Systems}}: {{The BEGIN Benchmark}}},
  shorttitle = {Evaluating {{Attribution}} in {{Dialogue Systems}}},
  author = {Dziri, Nouha and Rashkin, Hannah and Linzen, Tal and Reitter, David},
  date = {2022-06-28},
  eprint = {2105.00071},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2105.00071},
  urldate = {2023-12-13},
  abstract = {Knowledge-grounded dialogue systems powered by large language models often generate responses that, while fluent, are not attributable to a relevant source of information. Progress towards models that do not exhibit this issue requires evaluation metrics that can quantify its prevalence. To this end, we introduce the Benchmark for Evaluation of Grounded INteraction (BEGIN), comprised of 12k dialogue turns generated by neural dialogue systems trained on three knowledge-grounded dialogue corpora. We collect human annotations assessing the extent to which the models' responses can be attributed to the given background information. We then use BEGIN to analyze eight evaluation metrics. We find that these metrics rely on spurious correlations, do not reliably distinguish attributable abstractive responses from unattributable ones, and perform substantially worse when the knowledge source is longer. Our findings underscore the need for more sophisticated and robust evaluation metrics for knowledge-grounded dialogue. We make BEGIN publicly available at https://github.com/google/BEGIN-dataset.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\JEBAWZE2\\Dziri Á≠â - 2022 - Evaluating Attribution in Dialogue Systems The BE.pdf;D\:\\Zotero\\literature_data\\storage\\9WY3TNRA\\2105.html}
}

@online{eloundou2023,
  title = {{{GPTs}} Are {{GPTs}}: {{An Early Look}} at the {{Labor Market Impact Potential}} of {{Large Language Models}}},
  shorttitle = {{{GPTs}} Are {{GPTs}}},
  author = {Eloundou, Tyna and Manning, Sam and Mishkin, Pamela and Rock, Daniel},
  date = {2023-03-23},
  eprint = {2303.10130},
  eprinttype = {arxiv},
  eprintclass = {cs, econ, q-fin},
  url = {http://arxiv.org/abs/2303.10130},
  urldate = {2023-05-09},
  abstract = {We investigate the potential implications of large language models (LLMs), such as Generative Pretrained Transformers (GPTs), on the U.S. labor market, focusing on the increased capabilities arising from LLM-powered software compared to LLMs on their own. Using a new rubric, we assess occupations based on their alignment with LLM capabilities, integrating both human expertise and GPT-4 classifications. Our findings reveal that around 80\% of the U.S. workforce could have at least 10\% of their work tasks affected by the introduction of LLMs, while approximately 19\% of workers may see at least 50\% of their tasks impacted. We do not make predictions about the development or adoption timeline of such LLMs. The projected effects span all wage levels, with higher-income jobs potentially facing greater exposure to LLM capabilities and LLM-powered software. Significantly, these impacts are not restricted to industries with higher recent productivity growth. Our analysis suggests that, with access to an LLM, about 15\% of all worker tasks in the US could be completed significantly faster at the same level of quality. When incorporating software and tooling built on top of LLMs, this share increases to between 47 and 56\% of all tasks. This finding implies that LLM-powered software will have a substantial effect on scaling the economic impacts of the underlying models. We conclude that LLMs such as GPTs exhibit traits of general-purpose technologies, indicating that they could have considerable economic, social, and policy implications.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Economics - General Economics},
  file = {D:\Zotero\literature_data\storage\CLHMW4FS\Eloundou Á≠â - 2023 - GPTs are GPTs An Early Look at the Labor Market I.pdf}
}

@online{fang2022,
  title = {{{EVA}}: {{Exploring}} the {{Limits}} of {{Masked Visual Representation Learning}} at {{Scale}}},
  shorttitle = {{{EVA}}},
  author = {Fang, Yuxin and Wang, Wen and Xie, Binhui and Sun, Quan and Wu, Ledell and Wang, Xinggang and Huang, Tiejun and Wang, Xinlong and Cao, Yue},
  date = {2022-12-05},
  eprint = {2211.07636},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2211.07636},
  urldate = {2023-05-20},
  abstract = {We launch EVA, a vision-centric foundation model to explore the limits of visual representation at scale using only publicly accessible data. EVA is a vanilla ViT pre-trained to reconstruct the masked out image-text aligned vision features conditioned on visible image patches. Via this pretext task, we can efficiently scale up EVA to one billion parameters, and sets new records on a broad range of representative vision downstream tasks, such as image recognition, video action recognition, object detection, instance segmentation and semantic segmentation without heavy supervised training. Moreover, we observe quantitative changes in scaling EVA result in qualitative changes in transfer learning performance that are not present in other models. For instance, EVA takes a great leap in the challenging large vocabulary instance segmentation task: our model achieves almost the same state-of-the-art performance on LVISv1.0 dataset with over a thousand categories and COCO dataset with only eighty categories. Beyond a pure vision encoder, EVA can also serve as a vision-centric, multi-modal pivot to connect images and text. We find initializing the vision tower of a giant CLIP from EVA can greatly stabilize the training and outperform the training from scratch counterpart with much fewer samples and less compute, providing a new direction for scaling up and accelerating the costly training of multi-modal foundation models. To facilitate future research, we release all the code and models at https://github.com/baaivision/EVA.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\3H9UGI6H\\Fang Á≠â - 2022 - EVA Exploring the Limits of Masked Visual Represe.pdf;D\:\\Zotero\\literature_data\\storage\\H9GAWSX7\\2211.html}
}

@article{fang2023,
  title = {{{DropMessage}}: {{Unifying Random Dropping}} for {{Graph Neural Networks}}},
  shorttitle = {{{DropMessage}}},
  author = {Fang, Taoran and Xiao, Zhiqing and Wang, Chunping and Xu, Jiarong and Yang, Xuan and Yang, Yang},
  date = {2023-06-26},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {37},
  number = {4},
  eprint = {2204.10037},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {4267--4275},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v37i4.25545},
  url = {http://arxiv.org/abs/2204.10037},
  urldate = {2023-08-10},
  abstract = {Graph Neural Networks (GNNs) are powerful tools for graph representation learning. Despite their rapid development, GNNs also face some challenges, such as over-fitting, over-smoothing, and non-robustness. Previous works indicate that these problems can be alleviated by random dropping methods, which integrate augmented data into models by randomly masking parts of the input. However, some open problems of random dropping on GNNs remain to be solved. First, it is challenging to find a universal method that are suitable for all cases considering the divergence of different datasets and models. Second, augmented data introduced to GNNs causes the incomplete coverage of parameters and unstable training process. Third, there is no theoretical analysis on the effectiveness of random dropping methods on GNNs. In this paper, we propose a novel random dropping method called DropMessage, which performs dropping operations directly on the propagated messages during the message-passing process. More importantly, we find that DropMessage provides a unified framework for most existing random dropping methods, based on which we give theoretical analysis of their effectiveness. Furthermore, we elaborate the superiority of DropMessage: it stabilizes the training process by reducing sample variance; it keeps information diversity from the perspective of information theory, enabling it become a theoretical upper bound of other methods. To evaluate our proposed method, we conduct experiments that aims for multiple tasks on five public datasets and two industrial datasets with various backbone models. The experimental results show that DropMessage has the advantages of both effectiveness and generalization, and can significantly alleviate the problems mentioned above.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,dropÊìç‰ΩúÁöÑÁêÜËß£},
  file = {D\:\\Zotero\\literature_data\\storage\\UILQS3CX\\Fang Á≠â - 2023 - DropMessage Unifying Random Dropping for Graph Ne.pdf;D\:\\Zotero\\literature_data\\storage\\V83CGG9L\\2204.html}
}

@online{federici2020,
  title = {Learning {{Robust Representations}} via {{Multi-View Information Bottleneck}}},
  author = {Federici, Marco and Dutta, Anjan and Forr√©, Patrick and Kushman, Nate and Akata, Zeynep},
  date = {2020-02-18},
  eprint = {2002.07017},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2002.07017},
  url = {http://arxiv.org/abs/2002.07017},
  urldate = {2023-05-16},
  abstract = {The information bottleneck principle provides an information-theoretic method for representation learning, by training an encoder to retain all information which is relevant for predicting the label while minimizing the amount of other, excess information in the representation. The original formulation, however, requires labeled data to identify the superfluous information. In this work, we extend this ability to the multi-view unsupervised setting, where two views of the same underlying entity are provided but the label is unknown. This enables us to identify superfluous information as that not shared by both views. A theoretical analysis leads to the definition of a new multi-view model that produces state-of-the-art results on the Sketchy dataset and label-limited versions of the MIR-Flickr dataset. We also extend our theory to the single-view setting by taking advantage of standard data augmentation techniques, empirically showing better generalization capabilities when compared to common unsupervised approaches for representation learning.},
  pubstate = {preprint},
  keywords = {bottleneck,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\KJUHMURP\\Federici Á≠â - 2020 - Learning Robust Representations via Multi-View Inf.pdf;D\:\\Zotero\\literature_data\\storage\\84MRPIX3\\2002.html}
}

@online{feng2023,
  title = {Trends in {{Integration}} of {{Knowledge}} and {{Large Language Models}}: {{A Survey}} and {{Taxonomy}} of {{Methods}}, {{Benchmarks}}, and {{Applications}}},
  shorttitle = {Trends in {{Integration}} of {{Knowledge}} and {{Large Language Models}}},
  author = {Feng, Zhangyin and Ma, Weitao and Yu, Weijiang and Huang, Lei and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and {liu}, Ting},
  date = {2023-11-10},
  eprint = {2311.05876},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2311.05876},
  urldate = {2023-11-14},
  abstract = {Large language models (LLMs) exhibit superior performance on various natural language tasks, but they are susceptible to issues stemming from outdated data and domain-specific limitations. In order to address these challenges, researchers have pursued two primary strategies, knowledge editing and retrieval augmentation, to enhance LLMs by incorporating external information from different aspects. Nevertheless, there is still a notable absence of a comprehensive survey. In this paper, we propose a review to discuss the trends in integration of knowledge and large language models, including taxonomy of methods, benchmarks, and applications. In addition, we conduct an in-depth analysis of different methods and point out potential research directions in the future. We hope this survey offers the community quick access and a comprehensive overview of this research area, with the intention of inspiring future research endeavors.},
  pubstate = {preprint},
  version = {1},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\Y6V3ZA23\\Feng Á≠â - 2023 - Trends in Integration of Knowledge and Large Langu.pdf;D\:\\Zotero\\literature_data\\storage\\BTR7PHX5\\2311.html}
}

@online{feng2023a,
  title = {Towards {{Revealing}} the {{Mystery}} behind {{Chain}} of {{Thought}}: {{A Theoretical Perspective}}},
  shorttitle = {Towards {{Revealing}} the {{Mystery}} behind {{Chain}} of {{Thought}}},
  author = {Feng, Guhao and Zhang, Bohang and Gu, Yuntian and Ye, Haotian and He, Di and Wang, Liwei},
  date = {2023-11-05},
  eprint = {2305.15408},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2305.15408},
  urldate = {2023-11-17},
  abstract = {Recent studies have discovered that Chain-of-Thought prompting (CoT) can dramatically improve the performance of Large Language Models (LLMs), particularly when dealing with complex tasks involving mathematics or reasoning. Despite the enormous empirical success, the underlying mechanisms behind CoT and how it unlocks the potential of LLMs remain elusive. In this paper, we take a first step towards theoretically answering these questions. Specifically, we examine the expressivity of LLMs with CoT in solving fundamental mathematical and decision-making problems. By using circuit complexity theory, we first give impossibility results showing that bounded-depth Transformers are unable to directly produce correct answers for basic arithmetic/equation tasks unless the model size grows super-polynomially with respect to the input length. In contrast, we then prove by construction that autoregressive Transformers of constant size suffice to solve both tasks by generating CoT derivations using a commonly used math language format. Moreover, we show LLMs with CoT can handle a general class of decision-making problems known as Dynamic Programming, thus justifying its power in tackling complex real-world tasks. Finally, an extensive set of experiments show that, while Transformers always fail to directly predict the answers, they can consistently learn to generate correct solutions step-by-step given sufficient CoT demonstrations.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Computational Complexity,Computer Science - Machine Learning,CoT,Statistics - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\KYFVQXNR\\Feng Á≠â - 2023 - Towards Revealing the Mystery behind Chain of Thou.pdf;D\:\\Zotero\\literature_data\\storage\\7SKY83MN\\2305.html}
}

@online{feng2023b,
  title = {Retrieval-{{Generation Synergy Augmented Large Language Models}}},
  author = {Feng, Zhangyin and Feng, Xiaocheng and Zhao, Dezhi and Yang, Maojin and Qin, Bing},
  date = {2023-10-08},
  eprint = {2310.05149},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2310.05149},
  urldate = {2023-11-22},
  abstract = {Large language models augmented with task-relevant documents have demonstrated impressive performance on knowledge-intensive tasks. However, regarding how to obtain effective documents, the existing methods are mainly divided into two categories. One is to retrieve from an external knowledge base, and the other is to utilize large language models to generate documents. We propose an iterative retrieval-generation collaborative framework. It is not only able to leverage both parametric and non-parametric knowledge, but also helps to find the correct reasoning path through retrieval-generation interactions, which is very important for tasks that require multi-step reasoning. We conduct experiments on four question answering datasets, including single-hop QA and multi-hop QA tasks. Empirical results show that our method significantly improves the reasoning ability of large language models and outperforms previous baselines.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\DLNU4I5S\\Feng Á≠â - 2023 - Retrieval-Generation Synergy Augmented Large Langu.pdf;D\:\\Zotero\\literature_data\\storage\\2TX3CZSN\\2310.html}
}

@online{gao2023,
  title = {Retrieval-{{Augmented Generation}} for {{Large Language Models}}: {{A Survey}}},
  shorttitle = {Retrieval-{{Augmented Generation}} for {{Large Language Models}}},
  author = {Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Guo, Qianyu and Wang, Meng and Wang, Haofen},
  date = {2023-12-18},
  url = {https://arxiv.org/abs/2312.10997v4},
  urldate = {2024-01-18},
  abstract = {Large Language Models (LLMs) demonstrate significant capabilities but face challenges such as hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the models, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval , the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces the metrics and benchmarks for assessing RAG models, along with the most up-to-date evaluation framework. In conclusion, the paper delineates prospective avenues for research, including the identification of challenges, the expansion of multi-modalities, and the progression of the RAG infrastructure and its ecosystem.},
  langid = {english},
  organization = {{arXiv.org}},
  file = {D:\Zotero\literature_data\storage\2CU5WLHV\Gao Á≠â - 2023 - Retrieval-Augmented Generation for Large Language .pdf}
}

@inproceedings{gao2023a,
  title = {{{RARR}}: {{Researching}} and {{Revising What Language Models Say}}, {{Using Language Models}}},
  shorttitle = {{{RARR}}},
  booktitle = {Proceedings of the 61st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Gao, Luyu and Dai, Zhuyun and Pasupat, Panupong and Chen, Anthony and Chaganty, Arun Tejasvi and Fan, Yicheng and Zhao, Vincent and Lao, Ni and Lee, Hongrae and Juan, Da-Cheng and Guu, Kelvin},
  editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
  date = {2023-07},
  pages = {16477--16508},
  publisher = {{Association for Computational Linguistics}},
  location = {{Toronto, Canada}},
  doi = {10.18653/v1/2023.acl-long.910},
  url = {https://aclanthology.org/2023.acl-long.910},
  urldate = {2023-11-15},
  abstract = {Language models (LMs) now excel at many tasks such as question answering, reasoning, and dialog. However, they sometimes generate unsupported or misleading content. A user cannot easily determine whether their outputs are trustworthy or not, because most LMs do not have any built-in mechanism for attribution to external evidence. To enable attribution while still preserving all the powerful advantages of recent generation models, we propose RARR (Retrofit Attribution using Research and Revision), a system that 1) automatically finds attribution for the output of any text generation model, and 2) post-edits the output to fix unsupported content while preserving the original output as much as possible. When applied to the output of several state-of-the-art LMs on a diverse set of generation tasks, we find that RARR significantly improves attribution while otherwise preserving the original input to a much greater degree than previously explored edit models. Furthermore, the implementation of RARR requires only a handful of training examples, a large language model, and standard web search.},
  eventtitle = {{{ACL}} 2023},
  keywords = {post-editing,ÂÖàÁîüÊàêÂêéÊ£ÄÁ¥¢},
  file = {D:\Zotero\literature_data\storage\486AWICC\Gao Á≠â - 2023 - RARR Researching and Revising What Language Model.pdf}
}

@online{gao2023b,
  title = {{{RARR}}: {{Researching}} and {{Revising What Language Models Say}}, {{Using Language Models}}},
  shorttitle = {{{RARR}}},
  author = {Gao, Luyu and Dai, Zhuyun and Pasupat, Panupong and Chen, Anthony and Chaganty, Arun Tejasvi and Fan, Yicheng and Zhao, Vincent Y. and Lao, Ni and Lee, Hongrae and Juan, Da-Cheng and Guu, Kelvin},
  date = {2023-05-31},
  eprint = {2210.08726},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2210.08726},
  url = {http://arxiv.org/abs/2210.08726},
  urldate = {2023-11-29},
  abstract = {Language models (LMs) now excel at many tasks such as few-shot learning, question answering, reasoning, and dialog. However, they sometimes generate unsupported or misleading content. A user cannot easily determine whether their outputs are trustworthy or not, because most LMs do not have any built-in mechanism for attribution to external evidence. To enable attribution while still preserving all the powerful advantages of recent generation models, we propose RARR (Retrofit Attribution using Research and Revision), a system that 1) automatically finds attribution for the output of any text generation model and 2) post-edits the output to fix unsupported content while preserving the original output as much as possible. When applied to the output of several state-of-the-art LMs on a diverse set of generation tasks, we find that RARR significantly improves attribution while otherwise preserving the original input to a much greater degree than previously explored edit models. Furthermore, the implementation of RARR requires only a handful of training examples, a large language model, and standard web search.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\RM99VRUR\\Gao Á≠â - 2023 - RARR Researching and Revising What Language Model.pdf;D\:\\Zotero\\literature_data\\storage\\RPULD9FT\\2210.html}
}

@online{gao2023c,
  title = {Enabling {{Large Language Models}} to {{Generate Text}} with {{Citations}}},
  author = {Gao, Tianyu and Yen, Howard and Yu, Jiatong and Chen, Danqi},
  date = {2023-10-31},
  eprint = {2305.14627},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2305.14627},
  urldate = {2023-11-08},
  abstract = {Large language models (LLMs) have emerged as a widely-used tool for information seeking, but their generated outputs are prone to hallucination. In this work, our aim is to allow LLMs to generate text with citations, improving their factual correctness and verifiability. Existing work mainly relies on commercial search engines and human evaluation, making it challenging to reproduce and compare different modeling approaches. We propose ALCE, the first benchmark for Automatic LLMs' Citation Evaluation. ALCE collects a diverse set of questions and retrieval corpora and requires building end-to-end systems to retrieve supporting evidence and generate answers with citations. We develop automatic metrics along three dimensions -- fluency, correctness, and citation quality -- and demonstrate their strong correlation with human judgements. Our experiments with state-of-the-art LLMs and novel prompting strategies show that current systems have considerable room for improvement -- For example, on the ELI5 dataset, even the best models lack complete citation support 50\% of the time. Our analyses further highlight promising future directions, including developing better retrievers, advancing long-context LLMs, and improving the ability to synthesize information from multiple sources.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\DQ4KR486\\Gao Á≠â - 2023 - Enabling Large Language Models to Generate Text wi.pdf;D\:\\Zotero\\literature_data\\storage\\NX466XU4\\2305.html}
}

@online{gasteiger2022,
  title = {Predict Then {{Propagate}}: {{Graph Neural Networks}} Meet {{Personalized PageRank}}},
  shorttitle = {Predict Then {{Propagate}}},
  author = {Gasteiger, Johannes and Bojchevski, Aleksandar and G√ºnnemann, Stephan},
  date = {2022-04-05},
  eprint = {1810.05997},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1810.05997},
  urldate = {2023-05-12},
  abstract = {Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online.},
  pubstate = {preprint},
  keywords = {APPNP,Computer Science - Machine Learning,Statistics - Machine Learning,ÂΩí‰∏ÄÂåñGCNËÆ°ÁÆóP},
  file = {D\:\\Zotero\\literature_data\\storage\\9PIJP4M2\\Gasteiger Á≠â - 2022 - Predict then Propagate Graph Neural Networks meet.pdf;D\:\\Zotero\\literature_data\\storage\\MNMY5D4E\\1810.html}
}

@online{geshkovski2023,
  title = {A Mathematical Perspective on {{Transformers}}},
  author = {Geshkovski, Borjan and Letrouit, Cyril and Polyanskiy, Yury and Rigollet, Philippe},
  date = {2023-12-22},
  eprint = {2312.10794},
  eprinttype = {arxiv},
  eprintclass = {cs, math},
  url = {http://arxiv.org/abs/2312.10794},
  urldate = {2024-01-03},
  abstract = {Transformers play a central role in the inner workings of large language models. We develop a mathematical framework for analyzing Transformers based on their interpretation as interacting particle systems, which reveals that clusters emerge in long time. Our study explores the underlying theory and offers new perspectives for mathematicians as well as computer scientists.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Mathematics - Analysis of PDEs,Mathematics - Dynamical Systems},
  file = {D\:\\Zotero\\literature_data\\storage\\P9DKLDXF\\Geshkovski Á≠â - 2023 - A mathematical perspective on Transformers.pdf;D\:\\Zotero\\literature_data\\storage\\FG43WBXP\\2312.html}
}

@article{golikov,
  title = {Non-{{Gaussian Tensor Programs}}},
  author = {Golikov, Eugene and Yang, Greg},
  langid = {english},
  file = {D:\Zotero\literature_data\storage\JL7ERYZD\Golikov Âíå Yang - Non-Gaussian Tensor Programs.pdf}
}

@article{golikova,
  title = {Non-{{Gaussian Tensor Programs}}},
  author = {Golikov, Eugene and Yang, Greg},
  langid = {english},
  file = {D:\Zotero\literature_data\storage\ZSNGCFCQ\Golikov Âíå Yang - Non-Gaussian Tensor Programs.pdf}
}

@online{goodfellow2014,
  title = {Generative {{Adversarial Networks}}},
  author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  date = {2014-06-10},
  eprint = {1406.2661},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1406.2661},
  url = {http://arxiv.org/abs/1406.2661},
  urldate = {2023-04-30},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\FQ65QVU8\\Goodfellow Á≠â - 2014 - Generative Adversarial Networks.pdf;D\:\\Zotero\\literature_data\\storage\\JM66Q9HR\\1406.html}
}

@online{goodfellow2014a,
  title = {Generative {{Adversarial Networks}}},
  author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  date = {2014-06-10},
  eprint = {1406.2661},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1406.2661},
  urldate = {2023-04-05},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {D:\Zotero\literature_data\storage\BMZML55K\Goodfellow Á≠â - 2014 - Generative Adversarial Networks.pdf}
}

@online{grill2020,
  title = {Bootstrap Your Own Latent: {{A}} New Approach to Self-Supervised {{Learning}}},
  shorttitle = {Bootstrap Your Own Latent},
  author = {Grill, Jean-Bastien and Strub, Florian and Altch√©, Florent and Tallec, Corentin and Richemond, Pierre H. and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and Piot, Bilal and Kavukcuoglu, Koray and Munos, R√©mi and Valko, Michal},
  date = {2020-09-10},
  eprint = {2006.07733},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2006.07733},
  url = {http://arxiv.org/abs/2006.07733},
  urldate = {2023-07-22},
  abstract = {We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches \$74.3\textbackslash\%\$ top-1 classification accuracy on ImageNet using a linear evaluation with a ResNet-50 architecture and \$79.6\textbackslash\%\$ with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub.},
  pubstate = {preprint},
  keywords = {BYOL,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\CTCSW5YI\\Grill Á≠â - 2020 - Bootstrap your own latent A new approach to self-.pdf;D\:\\Zotero\\literature_data\\storage\\A44JNAZ9\\2006.html}
}

@online{hamilton2018,
  title = {Inductive {{Representation Learning}} on {{Large Graphs}}},
  author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
  date = {2018-09-10},
  eprint = {1706.02216},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1706.02216},
  url = {http://arxiv.org/abs/1706.02216},
  urldate = {2023-04-27},
  abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks,GraphSage,Statistics - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\H2QJAJRZ\\Hamilton Á≠â - 2018 - Inductive Representation Learning on Large Graphs.pdf;D\:\\Zotero\\literature_data\\storage\\62F5ADLZ\\1706.html}
}

@online{harlap2018,
  title = {{{PipeDream}}: {{Fast}} and {{Efficient Pipeline Parallel DNN Training}}},
  shorttitle = {{{PipeDream}}},
  author = {Harlap, Aaron and Narayanan, Deepak and Phanishayee, Amar and Seshadri, Vivek and Devanur, Nikhil and Ganger, Greg and Gibbons, Phil},
  date = {2018-06-08},
  eprint = {1806.03377},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1806.03377},
  url = {http://arxiv.org/abs/1806.03377},
  urldate = {2023-06-03},
  abstract = {PipeDream is a Deep Neural Network(DNN) training system for GPUs that parallelizes computation by pipelining execution across multiple machines. Its pipeline parallel computing model avoids the slowdowns faced by data-parallel training when large models and/or limited network bandwidth induce high communication-to-computation ratios. PipeDream reduces communication by up to 95\% for large DNNs relative to data-parallel training, and allows perfect overlap of communication and computation. PipeDream keeps all available GPUs productive by systematically partitioning DNN layers among them to balance work and minimize communication, versions model parameters for backward pass correctness, and schedules the forward and backward passes of different inputs in round-robin fashion to optimize "time to target accuracy". Experiments with five different DNNs on two different clusters show that PipeDream is up to 5x faster in time-to-accuracy compared to data-parallel training.},
  pubstate = {preprint},
  keywords = {{Computer Science - Distributed, Parallel, and Cluster Computing}},
  file = {D\:\\Zotero\\literature_data\\storage\\3RZ7URAW\\Harlap Á≠â - 2018 - PipeDream Fast and Efficient Pipeline Parallel DN.pdf;D\:\\Zotero\\literature_data\\storage\\8KE4YLTD\\1806.html}
}

@online{hassani2020,
  title = {Contrastive {{Multi-View Representation Learning}} on {{Graphs}}},
  author = {Hassani, Kaveh and Khasahmadi, Amir Hosein},
  date = {2020-06-09},
  eprint = {2006.05582},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.05582},
  urldate = {2023-07-25},
  abstract = {We introduce a self-supervised approach for learning node and graph level representations by contrasting structural views of graphs. We show that unlike visual representation learning, increasing the number of views to more than two or contrasting multi-scale encodings do not improve performance, and the best performance is achieved by contrasting encodings from first-order neighbors and a graph diffusion. We achieve new state-of-the-art results in self-supervised learning on 8 out of 8 node and graph classification benchmarks under the linear evaluation protocol. For example, on Cora (node) and Reddit-Binary (graph) classification benchmarks, we achieve 86.8\% and 84.5\% accuracy, which are 5.5\% and 2.4\% relative improvements over previous state-of-the-art. When compared to supervised baselines, our approach outperforms them in 4 out of 8 benchmarks. Source code is released at: https://github.com/kavehhassani/mvgrl},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\3LIYXCFE\\Hassani Âíå Khasahmadi - 2020 - Contrastive Multi-View Representation Learning on .pdf;D\:\\Zotero\\literature_data\\storage\\MDTBMZR2\\2006.html}
}

@online{he2020,
  title = {Momentum {{Contrast}} for {{Unsupervised Visual Representation Learning}}},
  author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  date = {2020-03-23},
  eprint = {1911.05722},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1911.05722},
  url = {http://arxiv.org/abs/1911.05722},
  urldate = {2023-07-23},
  abstract = {We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\literature_data\\storage\\42SRRF8K\\He Á≠â - 2020 - Momentum Contrast for Unsupervised Visual Represen.pdf;D\:\\Zotero\\literature_data\\storage\\CSYVMHUS\\1911.html}
}

@inproceedings{he2022,
  title = {Masked {{Autoencoders Are Scalable Vision Learners}}},
  author = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll√°r, Piotr and Girshick, Ross},
  date = {2022},
  pages = {16000--16009},
  url = {https://openaccess.thecvf.com/content/CVPR2022/html/He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper},
  urldate = {2023-07-21},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  langid = {english},
  file = {D:\Zotero\literature_data\storage\TZIIJZ9S\He Á≠â - 2022 - Masked Autoencoders Are Scalable Vision Learners.pdf}
}

@online{hendrycks2020,
  title = {Gaussian {{Error Linear Units}} ({{GELUs}})},
  author = {Hendrycks, Dan and Gimpel, Kevin},
  date = {2020-07-08},
  eprint = {1606.08415},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1606.08415},
  url = {http://arxiv.org/abs/1606.08415},
  urldate = {2023-06-03},
  abstract = {We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU activation function is \$x\textbackslash Phi(x)\$, where \$\textbackslash Phi(x)\$ the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLUs (\$x\textbackslash mathbf\{1\}\_\{x{$>$}0\}\$). We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\U7QGQ72M\\Hendrycks Âíå Gimpel - 2020 - Gaussian Error Linear Units (GELUs).pdf;D\:\\Zotero\\literature_data\\storage\\KP8YISLI\\1606.html}
}

@online{henighan2020,
  title = {Scaling {{Laws}} for {{Autoregressive Generative Modeling}}},
  author = {Henighan, Tom and Kaplan, Jared and Katz, Mor and Chen, Mark and Hesse, Christopher and Jackson, Jacob and Jun, Heewoo and Brown, Tom B. and Dhariwal, Prafulla and Gray, Scott and Hallacy, Chris and Mann, Benjamin and Radford, Alec and Ramesh, Aditya and Ryder, Nick and Ziegler, Daniel M. and Schulman, John and Amodei, Dario and McCandlish, Sam},
  date = {2020-11-05},
  eprint = {2010.14701},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2010.14701},
  url = {http://arxiv.org/abs/2010.14701},
  urldate = {2023-06-04},
  abstract = {We identify empirical scaling laws for the cross-entropy loss in four domains: generative image modeling, video modeling, multimodal image\$\textbackslash leftrightarrow\$text models, and mathematical problem solving. In all cases autoregressive Transformers smoothly improve in performance as model size and compute budgets increase, following a power-law plus constant scaling law. The optimal model size also depends on the compute budget through a power-law, with exponents that are nearly universal across all data domains. The cross-entropy loss has an information theoretic interpretation as \$S(\$True\$) + D\_\{\textbackslash mathrm\{KL\}\}(\$True\$||\$Model\$)\$, and the empirical scaling laws suggest a prediction for both the true data distribution's entropy and the KL divergence between the true and model distributions. With this interpretation, billion-parameter Transformers are nearly perfect models of the YFCC100M image distribution downsampled to an \$8\textbackslash times 8\$ resolution, and we can forecast the model size needed to achieve any given reducible loss (ie \$D\_\{\textbackslash mathrm\{KL\}\}\$) in nats/image for other resolutions. We find a number of additional scaling laws in specific domains: (a) we identify a scaling relation for the mutual information between captions and images in multimodal models, and show how to answer the question "Is a picture worth a thousand words?"; (b) in the case of mathematical problem solving, we identify scaling laws for model performance when extrapolating beyond the training distribution; (c) we finetune generative image models for ImageNet classification and find smooth scaling of the classification loss and error rate, even as the generative loss levels off. Taken together, these results strengthen the case that scaling laws have important implications for neural network performance, including on downstream tasks.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\GECVXRIS\\Henighan Á≠â - 2020 - Scaling Laws for Autoregressive Generative Modelin.pdf;D\:\\Zotero\\literature_data\\storage\\3PIFKLCL\\2010.html}
}

@online{hestness2017,
  title = {Deep {{Learning Scaling}} Is {{Predictable}}, {{Empirically}}},
  author = {Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Md Mostofa Ali and Yang, Yang and Zhou, Yanqi},
  date = {2017-12-01},
  eprint = {1712.00409},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1712.00409},
  urldate = {2023-05-08},
  abstract = {Deep learning (DL) creates impactful advances following a virtuous recipe: model architecture search, creating large training data sets, and scaling computation. It is widely believed that growing training sets and models should improve accuracy and result in better products. As DL application domains grow, we would like a deeper understanding of the relationships between training set size, computational scale, and model accuracy improvements to advance the state-of-the-art. This paper presents a large scale empirical characterization of generalization error and model size growth as training sets grow. We introduce a methodology for this measurement and test four machine learning domains: machine translation, language modeling, image processing, and speech recognition. Our empirical results show power-law generalization error scaling across a breadth of factors, resulting in power-law exponents---the "steepness" of the learning curve---yet to be explained by theoretical work. Further, model improvements only shift the error but do not appear to affect the power-law exponent. We also show that model size scales sublinearly with data size. These scaling relationships have significant implications on deep learning research, practice, and systems. They can assist model debugging, setting accuracy targets, and decisions about data set growth. They can also guide computing system design and underscore the importance of continued computational scaling.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Ê®°Âûãloss-ËßÑÊ®°È¢ÑÊµã},
  file = {D\:\\Zotero\\literature_data\\storage\\SNQRWBFH\\Hestness Á≠â - 2017 - Deep Learning Scaling is Predictable, Empirically.pdf;D\:\\Zotero\\literature_data\\storage\\WRWF7T83\\1712.html}
}

@article{hinz2022,
  title = {Semantic {{Object Accuracy}} for {{Generative Text-to-Image Synthesis}}},
  author = {Hinz, Tobias and Heinrich, Stefan and Wermter, Stefan},
  date = {2022-03-01},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  volume = {44},
  number = {3},
  eprint = {1910.13321},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {1552--1565},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2020.3021209},
  url = {http://arxiv.org/abs/1910.13321},
  urldate = {2023-05-20},
  abstract = {Generative adversarial networks conditioned on textual image descriptions are capable of generating realistic-looking images. However, current methods still struggle to generate images based on complex image captions from a heterogeneous domain. Furthermore, quantitatively evaluating these text-to-image models is challenging, as most evaluation metrics only judge image quality but not the conformity between the image and its caption. To address these challenges we introduce a new model that explicitly models individual objects within an image and a new evaluation metric called Semantic Object Accuracy (SOA) that specifically evaluates images given an image caption. The SOA uses a pre-trained object detector to evaluate if a generated image contains objects that are mentioned in the image caption, e.g. whether an image generated from ‚Äúa car driving down the street‚Äù contains a car. We perform a user study comparing several text-to-image models and show that our SOA metric ranks the models the same way as humans, whereas other metrics such as the Inception Score do not. Our evaluation also shows that models which explicitly model objects outperform models which only model global image characteristics.},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {D:\Zotero\literature_data\storage\ILXVZNR3\Hinz Á≠â - 2022 - Semantic Object Accuracy for Generative Text-to-Im.pdf}
}

@software{hinz2023,
  title = {Semantic {{Object Accuracy}} for {{Generative Text-to-Image Synthesis}}},
  author = {Hinz, Tobias},
  date = {2023-04-24T03:40:12Z},
  origdate = {2019-10-21T09:41:17Z},
  url = {https://github.com/tohinz/semantic-object-accuracy-for-generative-text-to-image-synthesis},
  urldate = {2023-05-21},
  abstract = {Code for "Semantic Object Accuracy for Generative Text-to-Image Synthesis" (TPAMI 2020)},
  keywords = {evaluation-metrics,gan,gan-evaluation,image-generation,image-synthesis,ms-coco}
}

@online{hoffmann2022,
  title = {Training {{Compute-Optimal Large Language Models}}},
  author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and family=Driessche, given=George, prefix=van den, useprefix=false and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
  date = {2022-03-29},
  eprint = {2203.15556},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2203.15556},
  urldate = {2023-06-04},
  abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4\$\textbackslash times\$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over Gopher.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\JHQ3CRZ8\\Hoffmann Á≠â - 2022 - Training Compute-Optimal Large Language Models.pdf;D\:\\Zotero\\literature_data\\storage\\FYSFKCGN\\2203.html}
}

@online{honovich2022,
  title = {{{TRUE}}: {{Re-evaluating Factual Consistency Evaluation}}},
  shorttitle = {{{TRUE}}},
  author = {Honovich, Or and Aharoni, Roee and Herzig, Jonathan and Taitelbaum, Hagai and Kukliansy, Doron and Cohen, Vered and Scialom, Thomas and Szpektor, Idan and Hassidim, Avinatan and Matias, Yossi},
  date = {2022-05-03},
  eprint = {2204.04991},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2204.04991},
  urldate = {2023-11-29},
  abstract = {Grounded text generation systems often generate text that contains factual inconsistencies, hindering their real-world applicability. Automatic factual consistency evaluation may help alleviate this limitation by accelerating evaluation cycles, filtering inconsistent outputs and augmenting training data. While attracting increasing attention, such evaluation metrics are usually developed and evaluated in silo for a single task or dataset, slowing their adoption. Moreover, previous meta-evaluation protocols focused on system-level correlations with human annotations, which leave the example-level accuracy of such metrics unclear. In this work, we introduce TRUE: a comprehensive survey and assessment of factual consistency metrics on a standardized collection of existing texts from diverse tasks, manually annotated for factual consistency. Our standardization enables an example-level meta-evaluation protocol that is more actionable and interpretable than previously reported correlations, yielding clearer quality measures. Across diverse state-of-the-art metrics and 11 datasets we find that large-scale NLI and question generation-and-answering-based approaches achieve strong and complementary results. We recommend those methods as a starting point for model and metric developers, and hope TRUE will foster progress towards even better evaluation methods.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\F4HE989E\\Honovich Á≠â - 2022 - TRUE Re-evaluating Factual Consistency Evaluation.pdf;D\:\\Zotero\\literature_data\\storage\\6IFLMYJF\\2204.html}
}

@online{hou2022,
  title = {{{GraphMAE}}: {{Self-Supervised Masked Graph Autoencoders}}},
  shorttitle = {{{GraphMAE}}},
  author = {Hou, Zhenyu and Liu, Xiao and Cen, Yukuo and Dong, Yuxiao and Yang, Hongxia and Wang, Chunjie and Tang, Jie},
  date = {2022-07-13},
  eprint = {2205.10803},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2205.10803},
  url = {http://arxiv.org/abs/2205.10803},
  urldate = {2023-05-25},
  abstract = {Self-supervised learning (SSL) has been extensively explored in recent years. Particularly, generative SSL has seen emerging success in natural language processing and other AI fields, such as the wide adoption of BERT and GPT. Despite this, contrastive learning-which heavily relies on structural data augmentation and complicated training strategies-has been the dominant approach in graph SSL, while the progress of generative SSL on graphs, especially graph autoencoders (GAEs), has thus far not reached the potential as promised in other fields. In this paper, we identify and examine the issues that negatively impact the development of GAEs, including their reconstruction objective, training robustness, and error metric. We present a masked graph autoencoder GraphMAE that mitigates these issues for generative self-supervised graph pretraining. Instead of reconstructing graph structures, we propose to focus on feature reconstruction with both a masking strategy and scaled cosine error that benefit the robust training of GraphMAE. We conduct extensive experiments on 21 public datasets for three different graph learning tasks. The results manifest that GraphMAE-a simple graph autoencoder with careful designs-can consistently generate outperformance over both contrastive and generative state-of-the-art baselines. This study provides an understanding of graph autoencoders and demonstrates the potential of generative self-supervised pre-training on graphs.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\749AAX4F\\Hou Á≠â - 2022 - GraphMAE Self-Supervised Masked Graph Autoencoder.pdf;D\:\\Zotero\\literature_data\\storage\\G5F5GM5W\\2205.html}
}

@online{howard2018,
  title = {Universal {{Language Model Fine-tuning}} for {{Text Classification}}},
  author = {Howard, Jeremy and Ruder, Sebastian},
  date = {2018-01-18},
  url = {https://arxiv.org/abs/1801.06146v5},
  urldate = {2023-05-17},
  abstract = {Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24\% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.},
  langid = {english},
  organization = {{arXiv.org}},
  file = {D:\Zotero\literature_data\storage\D334M5TI\Howard Âíå Ruder - 2018 - Universal Language Model Fine-tuning for Text Clas.pdf}
}

@inproceedings{hu2023,
  title = {{{UnifEE}}: {{Unified Evidence Extraction}} for {{Fact Verification}}},
  shorttitle = {{{UnifEE}}},
  booktitle = {Proceedings of the 17th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Hu, Nan and Wu, Zirui and Lai, Yuxuan and Zhang, Chen and Feng, Yansong},
  editor = {Vlachos, Andreas and Augenstein, Isabelle},
  date = {2023-05},
  pages = {1150--1160},
  publisher = {{Association for Computational Linguistics}},
  location = {{Dubrovnik, Croatia}},
  doi = {10.18653/v1/2023.eacl-main.82},
  url = {https://aclanthology.org/2023.eacl-main.82},
  urldate = {2023-12-09},
  abstract = {FEVEROUS is a fact extraction and verification task that requires systems to extract evidence of both sentences and table cells from a Wikipedia dump, then predict the veracity of the given claim accordingly. Existing works extract evidence in the two formats separately, ignoring potential connections between them. In this paper, we propose a Unified Evidence Extraction model (UnifEE), which uses a mixed evidence graph to extract the evidence in both formats. With the carefully-designed unified evidence graph, UnifEE allows evidence interactions among all candidates in both formats at similar granularity. Experiments show that, with information aggregated from related evidence candidates in the fusion graph, UnifEE can make better decisions about which evidence should be kept, especially for claims requiring multi-hop reasoning or a combination of tables and texts. Thus it outperforms all previous evidence extraction methods and brings significant improvement in the subsequent claim verification step.},
  eventtitle = {{{EACL}} 2023},
  file = {D:\Zotero\literature_data\storage\42ZFSVWG\Hu Á≠â - 2023 - UnifEE Unified Evidence Extraction for Fact Verif.pdf}
}

@online{hu2024,
  title = {Benchmarking {{Large Language Models}} in {{Complex Question Answering Attribution}} Using {{Knowledge Graphs}}},
  author = {Hu, Nan and Chen, Jiaoyan and Wu, Yike and Qi, Guilin and Bi, Sheng and Wu, Tongtong and Pan, Jeff Z.},
  date = {2024-01-25},
  eprint = {2401.14640},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2401.14640},
  urldate = {2024-02-01},
  abstract = {The attribution of question answering is to provide citations for supporting generated statements, and has attracted wide research attention. The current methods for automatically evaluating the attribution, which are often based on Large Language Models (LLMs), are still inadequate, particularly in recognizing subtle differences between attributions, and complex relationships between citations and statements. To compare these attribution evaluation methods and develop new ones, we introduce a set of fine-grained categories (i.e., supportive, insufficient, contradictory and irrelevant) for measuring the attribution, and develop a Complex Attributed Question Answering (CAQA) benchmark by leveraging knowledge graphs (KGs) for automatically generating attributions of different categories to question-answer pairs. Our analysis reveals that existing evaluators perform poorly under fine-grained attribution settings and exhibit weaknesses in complex citation-statement reasoning. Our CAQA benchmark, validated with human annotations, emerges as a promising tool for selecting and developing LLM attribution evaluators.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language}
}

@online{huang2019,
  title = {{{GPipe}}: {{Efficient Training}} of {{Giant Neural Networks}} Using {{Pipeline Parallelism}}},
  shorttitle = {{{GPipe}}},
  author = {Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Mia Xu and Chen, Dehao and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V. and Wu, Yonghui and Chen, Zhifeng},
  date = {2019-07-25},
  eprint = {1811.06965},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1811.06965},
  url = {http://arxiv.org/abs/1811.06965},
  urldate = {2023-06-03},
  abstract = {Scaling up deep neural network capacity has been known as an effective approach to improving model quality for several different machine learning tasks. In many cases, increasing model capacity beyond the memory limit of a single accelerator has required developing special algorithms or infrastructure. These solutions are often architecture-specific and do not transfer to other tasks. To address the need for efficient and task-independent model parallelism, we introduce GPipe, a pipeline parallelism library that allows scaling any network that can be expressed as a sequence of layers. By pipelining different sub-sequences of layers on separate accelerators, GPipe provides the flexibility of scaling a variety of different networks to gigantic sizes efficiently. Moreover, GPipe utilizes a novel batch-splitting pipelining algorithm, resulting in almost linear speedup when a model is partitioned across multiple accelerators. We demonstrate the advantages of GPipe by training large-scale neural networks on two different tasks with distinct network architectures: (i) Image Classification: We train a 557-million-parameter AmoebaNet model and attain a top-1 accuracy of 84.4\% on ImageNet-2012, (ii) Multilingual Neural Machine Translation: We train a single 6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100 languages and achieve better quality than all bilingual models.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\literature_data\\storage\\YU9I6N9G\\Huang Á≠â - 2019 - GPipe Efficient Training of Giant Neural Networks.pdf;D\:\\Zotero\\literature_data\\storage\\FMDXBVSS\\1811.html}
}

@inproceedings{huang2020,
  title = {Syntax-{{Aware Graph Attention Network}} for {{Aspect-Level Sentiment Classification}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Computational Linguistics}}},
  author = {Huang, Lianzhe and Sun, Xin and Li, Sujian and Zhang, Linhao and Wang, Houfeng},
  date = {2020-12},
  pages = {799--810},
  publisher = {{International Committee on Computational Linguistics}},
  location = {{Barcelona, Spain (Online)}},
  doi = {10.18653/v1/2020.coling-main.69},
  url = {https://aclanthology.org/2020.coling-main.69},
  urldate = {2023-08-24},
  abstract = {Aspect-level sentiment classification aims to distinguish the sentiment polarities over aspect terms in a sentence. Existing approaches mostly focus on modeling the relationship between the given aspect words and their contexts with attention, and ignore the use of more elaborate knowledge implicit in the context. In this paper, we exploit syntactic awareness to the model by the graph attention network on the dependency tree structure and external pre-training knowledge by BERT language model, which helps to model the interaction between the context and aspect words better. And the subwords of BERT are integrated into the dependency tree graphs, which can obtain more accurate representations of words by graph attention. Experiments demonstrate the effectiveness of our model.},
  eventtitle = {{{COLING}} 2020},
  keywords = {3},
  file = {D:\Zotero\literature_data\storage\UG59JNWK\Huang Á≠â - 2020 - Syntax-Aware Graph Attention Network for Aspect-Le.pdf}
}

@online{huang2023,
  title = {Zero-Shot {{Faithful Factual Error Correction}}},
  author = {Huang, Kung-Hsiang and Chan, Hou Pong and Ji, Heng},
  date = {2023-05-27},
  eprint = {2305.07982},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2305.07982},
  urldate = {2023-12-01},
  abstract = {Faithfully correcting factual errors is critical for maintaining the integrity of textual knowledge bases and preventing hallucinations in sequence-to-sequence models. Drawing on humans' ability to identify and correct factual errors, we present a zero-shot framework that formulates questions about input claims, looks for correct answers in the given evidence, and assesses the faithfulness of each correction based on its consistency with the evidence. Our zero-shot framework outperforms fully-supervised approaches, as demonstrated by experiments on the FEVER and SciFact datasets, where our outputs are shown to be more faithful. More importantly, the decomposability nature of our framework inherently provides interpretability. Additionally, to reveal the most suitable metrics for evaluating factual error corrections, we analyze the correlation between commonly used metrics with human judgments in terms of three different dimensions regarding intelligibility and faithfulness.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\43ZRFSEP\\Huang Á≠â - 2023 - Zero-shot Faithful Factual Error Correction.pdf;D\:\\Zotero\\literature_data\\storage\\XA66NW79\\2305.html}
}

@online{huang2023a,
  title = {Privacy {{Implications}} of {{Retrieval-Based Language Models}}},
  author = {Huang, Yangsibo and Gupta, Samyak and Zhong, Zexuan and Li, Kai and Chen, Danqi},
  date = {2023-05-24},
  eprint = {2305.14888},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.14888},
  url = {http://arxiv.org/abs/2305.14888},
  urldate = {2023-12-02},
  abstract = {Retrieval-based language models (LMs) have demonstrated improved interpretability, factuality, and adaptability compared to their parametric counterparts, by incorporating retrieved text from external datastores. While it is well known that parametric models are prone to leaking private data, it remains unclear how the addition of a retrieval datastore impacts model privacy. In this work, we present the first study of privacy risks in retrieval-based LMs, particularly \$k\$NN-LMs. Our goal is to explore the optimal design and training procedure in domains where privacy is of concern, aiming to strike a balance between utility and privacy. Crucially, we find that \$k\$NN-LMs are more susceptible to leaking private information from their private datastore than parametric models. We further explore mitigations of privacy risks. When privacy information is targeted and readily detected in the text, we find that a simple sanitization step would completely eliminate the risks, while decoupling query and key encoders achieves an even better utility-privacy trade-off. Otherwise, we consider strategies of mixing public and private data in both datastore and encoder training. While these methods offer modest improvements, they leave considerable room for future work. Together, our findings provide insights for practitioners to better understand and mitigate privacy risks in retrieval-based LMs. Our code is available at: https://github.com/Princeton-SysML/kNNLM\_privacy .},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\U7LKMVUH\\Huang Á≠â - 2023 - Privacy Implications of Retrieval-Based Language M.pdf;D\:\\Zotero\\literature_data\\storage\\R6IYQRWW\\2305.html}
}

@online{huang2023b,
  title = {Citation: {{A Key}} to {{Building Responsible}} and {{Accountable Large Language Models}}},
  shorttitle = {Citation},
  author = {Huang, Jie and Chang, Kevin Chen-Chuan},
  date = {2023-11-15},
  eprint = {2307.02185},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2307.02185},
  urldate = {2023-12-30},
  abstract = {Large Language Models (LLMs) bring transformative benefits alongside unique challenges, including intellectual property (IP) and ethical concerns. This position paper explores a novel angle to mitigate these risks, drawing parallels between LLMs and established web systems. We identify "citation" - the acknowledgement or reference to a source or evidence - as a crucial yet missing component in LLMs. Incorporating citation could enhance content transparency and verifiability, thereby confronting the IP and ethical issues in the deployment of LLMs. We further propose that a comprehensive citation mechanism for LLMs should account for both non-parametric and parametric content. Despite the complexity of implementing such a citation mechanism, along with the potential pitfalls, we advocate for its development. Building on this foundation, we outline several research problems in this area, aiming to guide future explorations towards building more responsible and accountable LLMs.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Cryptography and Security},
  file = {D\:\\Zotero\\literature_data\\storage\\I6N3GYAZ\\Huang Âíå Chang - 2023 - Citation A Key to Building Responsible and Accoun.pdf;D\:\\Zotero\\literature_data\\storage\\K2RIMBWQ\\2307.html}
}

@online{huang2023c,
  title = {Citation: {{A Key}} to {{Building Responsible}} and {{Accountable Large Language Models}}},
  shorttitle = {Citation},
  author = {Huang, Jie and Chang, Kevin Chen-Chuan},
  date = {2023-07-05},
  eprint = {2307.02185},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2307.02185},
  url = {http://arxiv.org/abs/2307.02185},
  urldate = {2023-11-15},
  abstract = {Large Language Models (LLMs) bring transformative benefits alongside unique challenges, including intellectual property (IP) and ethical concerns. This position paper explores a novel angle to mitigate these risks, drawing parallels between LLMs and established web systems. We identify "citation" as a crucial yet missing component in LLMs, which could enhance content transparency and verifiability while addressing IP and ethical dilemmas. We further propose that a comprehensive citation mechanism for LLMs should account for both non-parametric and parametric content. Despite the complexity of implementing such a citation mechanism, along with the inherent potential pitfalls, we advocate for its development. Building on this foundation, we outline several research problems in this area, aiming to guide future explorations towards building more responsible and accountable LLMs.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Cryptography and Security,ÁõÆÂâçÂ≠òÂú®ÁöÑÊåëÊàò},
  file = {D\:\\Zotero\\literature_data\\storage\\QX7EB6JJ\\Huang Âíå Chang - 2023 - Citation A Key to Building Responsible and Accoun.pdf;D\:\\Zotero\\literature_data\\storage\\D96BE97U\\2307.html}
}

@online{huang2023d,
  title = {Catastrophic {{Jailbreak}} of {{Open-source LLMs}} via {{Exploiting Generation}}},
  author = {Huang, Yangsibo and Gupta, Samyak and Xia, Mengzhou and Li, Kai and Chen, Danqi},
  date = {2023-10-10},
  eprint = {2310.06987},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2310.06987},
  url = {http://arxiv.org/abs/2310.06987},
  urldate = {2023-12-02},
  abstract = {The rapid progress in open-source large language models (LLMs) is significantly advancing AI development. Extensive efforts have been made before model release to align their behavior with human values, with the primary goal of ensuring their helpfulness and harmlessness. However, even carefully aligned models can be manipulated maliciously, leading to unintended behaviors, known as "jailbreaks". These jailbreaks are typically triggered by specific text inputs, often referred to as adversarial prompts. In this work, we propose the generation exploitation attack, an extremely simple approach that disrupts model alignment by only manipulating variations of decoding methods. By exploiting different generation strategies, including varying decoding hyper-parameters and sampling methods, we increase the misalignment rate from 0\% to more than 95\% across 11 language models including LLaMA2, Vicuna, Falcon, and MPT families, outperforming state-of-the-art attacks with \$30\textbackslash times\$ lower computational cost. Finally, we propose an effective alignment method that explores diverse generation strategies, which can reasonably reduce the misalignment rate under our attack. Altogether, our study underscores a major failure in current safety evaluation and alignment procedures for open-source LLMs, strongly advocating for more comprehensive red teaming and better alignment before releasing such models. Our code is available at https://github.com/Princeton-SysML/Jailbreak\_LLM.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Cryptography and Security},
  file = {D\:\\Zotero\\literature_data\\storage\\AC8AK8IC\\Huang Á≠â - 2023 - Catastrophic Jailbreak of Open-source LLMs via Exp.pdf;D\:\\Zotero\\literature_data\\storage\\NKK7MMI9\\2310.html}
}

@online{huang2023e,
  title = {A {{Survey}} on {{Hallucination}} in {{Large Language Models}}: {{Principles}}, {{Taxonomy}}, {{Challenges}}, and {{Open Questions}}},
  shorttitle = {A {{Survey}} on {{Hallucination}} in {{Large Language Models}}},
  author = {Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and Liu, Ting},
  date = {2023-11-09},
  eprint = {2311.05232},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2311.05232},
  url = {http://arxiv.org/abs/2311.05232},
  urldate = {2023-11-18},
  abstract = {The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), leading to remarkable advancements in text understanding and generation. Nevertheless, alongside these strides, LLMs exhibit a critical tendency to produce hallucinations, resulting in content that is inconsistent with real-world facts or user inputs. This phenomenon poses substantial challenges to their practical deployment and raises concerns over the reliability of LLMs in real-world scenarios, which attracts increasing attention to detect and mitigate these hallucinations. In this survey, we aim to provide a thorough and in-depth overview of recent advances in the field of LLM hallucinations. We begin with an innovative taxonomy of LLM hallucinations, then delve into the factors contributing to hallucinations. Subsequently, we present a comprehensive overview of hallucination detection methods and benchmarks. Additionally, representative approaches designed to mitigate hallucinations are introduced accordingly. Finally, we analyze the challenges that highlight the current limitations and formulate open questions, aiming to delineate pathways for future research on hallucinations in LLMs.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,ÂπªËßâ},
  file = {D\:\\Zotero\\literature_data\\storage\\KCSUMR6C\\Huang Á≠â - 2023 - A Survey on Hallucination in Large Language Models.pdf;D\:\\Zotero\\literature_data\\storage\\XIANWMDC\\2311.html}
}

@inproceedings{huo2023,
  title = {Retrieving {{Supporting Evidence}} for {{Generative Question Answering}}},
  booktitle = {Proceedings of the {{Annual International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}} in the {{Asia Pacific Region}}},
  author = {Huo, Siqing and Arabzadeh, Negar and Clarke, Charles L. A.},
  date = {2023-11-26},
  eprint = {2309.11392},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {11--20},
  doi = {10.1145/3624918.3625336},
  url = {http://arxiv.org/abs/2309.11392},
  urldate = {2024-01-16},
  abstract = {Current large language models (LLMs) can exhibit near-human levels of performance on many natural language-based tasks, including open-domain question answering. Unfortunately, at this time, they also convincingly hallucinate incorrect answers, so that responses to questions must be verified against external sources before they can be accepted at face value. In this paper, we report two simple experiments to automatically validate generated answers against a corpus. We base our experiments on questions and passages from the MS MARCO (V1) test collection, and a retrieval pipeline consisting of sparse retrieval, dense retrieval and neural rerankers. In the first experiment, we validate the generated answer in its entirety. After presenting a question to an LLM and receiving a generated answer, we query the corpus with the combination of the question + generated answer. We then present the LLM with the combination of the question + generated answer + retrieved answer, prompting it to indicate if the generated answer can be supported by the retrieved answer. In the second experiment, we consider the generated answer at a more granular level, prompting the LLM to extract a list of factual statements from the answer and verifying each statement separately. We query the corpus with each factual statement and then present the LLM with the statement and the corresponding retrieved evidence. The LLM is prompted to indicate if the statement can be supported and make necessary edits using the retrieved material. With an accuracy of over 80\%, we find that an LLM is capable of verifying its generated answer when a corpus of supporting material is provided. However, manual assessment of a random sample of questions reveals that incorrect generated answers are missed by this verification process. While this verification process can reduce hallucinations, it can not entirely eliminate them.},
  keywords = {Computer Science - Information Retrieval},
  file = {D\:\\Zotero\\literature_data\\storage\\92TSER86\\Huo Á≠â - 2023 - Retrieving Supporting Evidence for Generative Ques.pdf;D\:\\Zotero\\literature_data\\storage\\LNHI88YM\\2309.html}
}

@online{ji2023,
  title = {{{AI Alignment}}: {{A Comprehensive Survey}}},
  shorttitle = {{{AI Alignment}}},
  author = {Ji, Jiaming and Qiu, Tianyi and Chen, Boyuan and Zhang, Borong and Lou, Hantao and Wang, Kaile and Duan, Yawen and He, Zhonghao and Zhou, Jiayi and Zhang, Zhaowei and Zeng, Fanzhi and Ng, Kwan Yee and Dai, Juntao and Pan, Xuehai and O'Gara, Aidan and Lei, Yingshan and Xu, Hua and Tse, Brian and Fu, Jie and McAleer, Stephen and Yang, Yaodong and Wang, Yizhou and Zhu, Song-Chun and Guo, Yike and Gao, Wen},
  date = {2023-11-01},
  eprint = {2310.19852},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2310.19852},
  urldate = {2023-11-04},
  abstract = {AI alignment aims to make AI systems behave in line with human intentions and values. As AI systems grow more capable, the potential large-scale risks associated with misaligned AI systems become salient. Hundreds of AI experts and public figures have expressed concerns about AI risks, arguing that "mitigating the risk of extinction from AI should be a global priority, alongside other societal-scale risks such as pandemics and nuclear war". To provide a comprehensive and up-to-date overview of the alignment field, in this survey paper, we delve into the core concepts, methodology, and practice of alignment. We identify the RICE principles as the key objectives of AI alignment: Robustness, Interpretability, Controllability, and Ethicality. Guided by these four principles, we outline the landscape of current alignment research and decompose them into two key components: forward alignment and backward alignment. The former aims to make AI systems aligned via alignment training, while the latter aims to gain evidence about the systems' alignment and govern them appropriately to avoid exacerbating misalignment risks. Forward alignment and backward alignment form a recurrent process where the alignment of AI systems from the forward process is verified in the backward process, meanwhile providing updated objectives for forward alignment in the next round. On forward alignment, we discuss learning from feedback and learning under distribution shift. On backward alignment, we discuss assurance techniques and governance practices that apply to every stage of AI systems' lifecycle. We also release and continually update the website (www.alignmentsurvey.com) which features tutorials, collections of papers, blog posts, and other resources.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence},
  file = {D\:\\Zotero\\literature_data\\storage\\TL9GCVZ3\\Ji Á≠â - 2023 - AI Alignment A Comprehensive Survey.pdf;D\:\\Zotero\\literature_data\\storage\\3G4UMYGY\\2310.html}
}

@online{jin2020,
  title = {Self-Supervised {{Learning}} on {{Graphs}}: {{Deep Insights}} and {{New Direction}}},
  shorttitle = {Self-Supervised {{Learning}} on {{Graphs}}},
  author = {Jin, Wei and Derr, Tyler and Liu, Haochen and Wang, Yiqi and Wang, Suhang and Liu, Zitao and Tang, Jiliang},
  date = {2020-06-17},
  eprint = {2006.10141},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.10141},
  urldate = {2023-07-25},
  abstract = {The success of deep learning notoriously requires larger amounts of costly annotated data. This has led to the development of self-supervised learning (SSL) that aims to alleviate this limitation by creating domain specific pretext tasks on unlabeled data. Simultaneously, there are increasing interests in generalizing deep learning to the graph domain in the form of graph neural networks (GNNs). GNNs can naturally utilize unlabeled nodes through the simple neighborhood aggregation that is unable to thoroughly make use of unlabeled nodes. Thus, we seek to harness SSL for GNNs to fully exploit the unlabeled data. Different from data instances in the image and text domains, nodes in graphs present unique structure information and they are inherently linked indicating not independent and identically distributed (or i.i.d.). Such complexity is a double-edged sword for SSL on graphs. On the one hand, it determines that it is challenging to adopt solutions from the image and text domains to graphs and dedicated efforts are desired. On the other hand, it provides rich information that enables us to build SSL from a variety of perspectives. Thus, in this paper, we first deepen our understandings on when, why, and which strategies of SSL work with GNNs by empirically studying numerous basic SSL pretext tasks on graphs. Inspired by deep insights from the empirical studies, we propose a new direction SelfTask to build advanced pretext tasks that are able to achieve state-of-the-art performance on various real-world datasets. The specific experimental settings to reproduce our results can be found in \textbackslash url\{https://github.com/ChandlerBang/SelfTask-GNN\}.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\LPRZU3R5\\Jin Á≠â - 2020 - Self-supervised Learning on Graphs Deep Insights .pdf;D\:\\Zotero\\literature_data\\storage\\MQBTJFTF\\2006.html}
}

@online{kadavath2022,
  title = {Language {{Models}} ({{Mostly}}) {{Know What They Know}}},
  author = {Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Hatfield-Dodds, Zac and DasSarma, Nova and Tran-Johnson, Eli and Johnston, Scott and El-Showk, Sheer and Jones, Andy and Elhage, Nelson and Hume, Tristan and Chen, Anna and Bai, Yuntao and Bowman, Sam and Fort, Stanislav and Ganguli, Deep and Hernandez, Danny and Jacobson, Josh and Kernion, Jackson and Kravec, Shauna and Lovitt, Liane and Ndousse, Kamal and Olsson, Catherine and Ringer, Sam and Amodei, Dario and Brown, Tom and Clark, Jack and Joseph, Nicholas and Mann, Ben and McCandlish, Sam and Olah, Chris and Kaplan, Jared},
  date = {2022-11-21},
  eprint = {2207.05221},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2207.05221},
  urldate = {2023-11-22},
  abstract = {We study whether language models can evaluate the validity of their own claims and predict which questions they will be able to answer correctly. We first show that larger models are well-calibrated on diverse multiple choice and true/false questions when they are provided in the right format. Thus we can approach self-evaluation on open-ended sampling tasks by asking models to first propose answers, and then to evaluate the probability "P(True)" that their answers are correct. We find encouraging performance, calibration, and scaling for P(True) on a diverse array of tasks. Performance at self-evaluation further improves when we allow models to consider many of their own samples before predicting the validity of one specific possibility. Next, we investigate whether models can be trained to predict "P(IK)", the probability that "I know" the answer to a question, without reference to any particular proposed answer. Models perform well at predicting P(IK) and partially generalize across tasks, though they struggle with calibration of P(IK) on new tasks. The predicted P(IK) probabilities also increase appropriately in the presence of relevant source materials in the context, and in the presence of hints towards the solution of mathematical word problems. We hope these observations lay the groundwork for training more honest models, and for investigating how honesty generalizes to cases where models are trained on objectives other than the imitation of human writing.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\89EFLS45\\Kadavath Á≠â - 2022 - Language Models (Mostly) Know What They Know.pdf;D\:\\Zotero\\literature_data\\storage\\ER8H2KYI\\2207.html}
}

@online{kamoi2023,
  title = {{{WiCE}}: {{Real-World Entailment}} for {{Claims}} in {{Wikipedia}}},
  shorttitle = {{{WiCE}}},
  author = {Kamoi, Ryo and Goyal, Tanya and Rodriguez, Juan Diego and Durrett, Greg},
  date = {2023-10-22},
  eprint = {2303.01432},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2303.01432},
  urldate = {2023-11-29},
  abstract = {Textual entailment models are increasingly applied in settings like fact-checking, presupposition verification in question answering, or summary evaluation. However, these represent a significant domain shift from existing entailment datasets, and models underperform as a result. We propose WiCE, a new fine-grained textual entailment dataset built on natural claim and evidence pairs extracted from Wikipedia. In addition to standard claim-level entailment, WiCE provides entailment judgments over sub-sentence units of the claim, and a minimal subset of evidence sentences that support each subclaim. To support this, we propose an automatic claim decomposition strategy using GPT-3.5 which we show is also effective at improving entailment models' performance on multiple datasets at test time. Finally, we show that real claims in our dataset involve challenging verification and retrieval problems that existing models fail to address.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\GEYS39DM\\Kamoi Á≠â - 2023 - WiCE Real-World Entailment for Claims in Wikipedi.pdf;D\:\\Zotero\\literature_data\\storage\\A2PJ5Q4R\\2303.html}
}

@online{kaplan2020,
  title = {Scaling {{Laws}} for {{Neural Language Models}}},
  author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  date = {2020-01-22},
  eprint = {2001.08361},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2001.08361},
  url = {http://arxiv.org/abs/2001.08361},
  urldate = {2023-12-25},
  abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Scaling Law,Statistics - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\STYUHQPX\\Kaplan Á≠â - 2020 - Scaling Laws for Neural Language Models.pdf;D\:\\Zotero\\literature_data\\storage\\HZVGXCJH\\2001.html}
}

@online{kaplan2020a,
  title = {Scaling {{Laws}} for {{Neural Language Models}}},
  author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  date = {2020-01-22},
  eprint = {2001.08361},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2001.08361},
  url = {http://arxiv.org/abs/2001.08361},
  urldate = {2023-06-04},
  abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\CFH4C47B\\Kaplan Á≠â - 2020 - Scaling Laws for Neural Language Models.pdf;D\:\\Zotero\\literature_data\\storage\\5AIYFV2S\\2001.html}
}

@online{kipf2016,
  title = {Variational {{Graph Auto-Encoders}}},
  author = {Kipf, Thomas N. and Welling, Max},
  date = {2016-11-21},
  eprint = {1611.07308},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1611.07308},
  url = {http://arxiv.org/abs/1611.07308},
  urldate = {2023-05-26},
  abstract = {We introduce the variational graph auto-encoder (VGAE), a framework for unsupervised learning on graph-structured data based on the variational auto-encoder (VAE). This model makes use of latent variables and is capable of learning interpretable latent representations for undirected graphs. We demonstrate this model using a graph convolutional network (GCN) encoder and a simple inner product decoder. Our model achieves competitive results on a link prediction task in citation networks. In contrast to most existing models for unsupervised learning on graph-structured data and link prediction, our model can naturally incorporate node features, which significantly improves predictive performance on a number of benchmark datasets.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,GAE,Statistics - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\7ITYXMU5\\Kipf Âíå Welling - 2016 - Variational Graph Auto-Encoders.pdf;D\:\\Zotero\\literature_data\\storage\\CMM6ZDFY\\1611.html}
}

@online{kipf2017,
  title = {Semi-{{Supervised Classification}} with {{Graph Convolutional Networks}}},
  author = {Kipf, Thomas N. and Welling, Max},
  date = {2017-02-22},
  eprint = {1609.02907},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1609.02907},
  url = {http://arxiv.org/abs/1609.02907},
  urldate = {2023-04-27},
  abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\R77YPIG8\\Kipf Âíå Welling - 2017 - Semi-Supervised Classification with Graph Convolut.pdf;D\:\\Zotero\\literature_data\\storage\\NKIBJXLB\\1609.html}
}

@online{kirstain2023,
  title = {Pick-a-{{Pic}}: {{An Open Dataset}} of {{User Preferences}} for {{Text-to-Image Generation}}},
  shorttitle = {Pick-a-{{Pic}}},
  author = {Kirstain, Yuval and Polyak, Adam and Singer, Uriel and Matiana, Shahbuland and Penna, Joe and Levy, Omer},
  date = {2023-05-02},
  eprint = {2305.01569},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2305.01569},
  urldate = {2023-07-13},
  abstract = {The ability to collect a large dataset of human preferences from text-to-image users is usually limited to companies, making such datasets inaccessible to the public. To address this issue, we create a web app that enables text-to-image users to generate images and specify their preferences. Using this web app we build Pick-a-Pic, a large, open dataset of text-to-image prompts and real users' preferences over generated images. We leverage this dataset to train a CLIP-based scoring function, PickScore, which exhibits superhuman performance on the task of predicting human preferences. Then, we test PickScore's ability to perform model evaluation and observe that it correlates better with human rankings than other automatic evaluation metrics. Therefore, we recommend using PickScore for evaluating future text-to-image generation models, and using Pick-a-Pic prompts as a more relevant dataset than MS-COCO. Finally, we demonstrate how PickScore can enhance existing text-to-image models via ranking.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Êï∞ÊçÆÈõÜ},
  file = {D\:\\Zotero\\literature_data\\storage\\JTAUR7VI\\Kirstain Á≠â - 2023 - Pick-a-Pic An Open Dataset of User Preferences fo.pdf;D\:\\Zotero\\literature_data\\storage\\U4K8BYSZ\\2305.html}
}

@online{kwon2023,
  title = {{{DataInf}}: {{Efficiently Estimating Data Influence}} in {{LoRA-tuned LLMs}} and {{Diffusion Models}}},
  shorttitle = {{{DataInf}}},
  author = {Kwon, Yongchan and Wu, Eric and Wu, Kevin and Zou, James},
  date = {2023-10-02},
  eprint = {2310.00902},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2310.00902},
  urldate = {2023-12-30},
  abstract = {Quantifying the impact of training data points is crucial for understanding the outputs of machine learning models and for improving the transparency of the AI pipeline. The influence function is a principled and popular data attribution method, but its computational cost often makes it challenging to use. This issue becomes more pronounced in the setting of large language models and text-to-image models. In this work, we propose DataInf, an efficient influence approximation method that is practical for large-scale generative AI models. Leveraging an easy-to-compute closed-form expression, DataInf outperforms existing influence computation algorithms in terms of computational and memory efficiency. Our theoretical analysis shows that DataInf is particularly well-suited for parameter-efficient fine-tuning techniques such as LoRA. Through systematic empirical evaluations, we show that DataInf accurately approximates influence scores and is orders of magnitude faster than existing methods. In applications to RoBERTa-large, Llama-2-13B-chat, and stable-diffusion-v1.5 models, DataInf effectively identifies the most influential fine-tuning examples better than other approximate influence scores. Moreover, it can help to identify which data points are mislabeled.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Ë¥°ÁåÆÂºèÂΩíÂõ†},
  file = {D\:\\Zotero\\literature_data\\storage\\7BM53KPE\\Kwon Á≠â - 2023 - DataInf Efficiently Estimating Data Influence in .pdf;D\:\\Zotero\\literature_data\\storage\\GBQBU84L\\2310.html}
}

@inproceedings{lee2019,
  title = {Image {{Aesthetic Assessment Based}} on {{Pairwise Comparison}} ¬≠ {{A Unified Approach}} to {{Score Regression}}, {{Binary Classification}}, and {{Personalization}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Lee, Jun-Tae and Kim, Chang-Su},
  date = {2019-10},
  pages = {1191--1200},
  publisher = {{IEEE}},
  location = {{Seoul, Korea (South)}},
  doi = {10.1109/ICCV.2019.00128},
  url = {https://ieeexplore.ieee.org/document/9009059/},
  urldate = {2023-06-24},
  abstract = {We propose a unified approach to three tasks of aesthetic score regression, binary aesthetic classification, and personalized aesthetics. First, we develop a comparator to estimate the ratio of aesthetic scores for two images. Then, we construct a pairwise comparison matrix for multiple reference images and an input image, and predict the aesthetic score of the input via the eigenvalue decomposition of the matrix. By varying the reference images, the proposed algorithm can be used for binary aesthetic classification and personalized aesthetics, as well as generic score regression. Experimental results demonstrate that the proposed unified algorithm provides the state-of-the-art performances in all three tasks of image aesthetics.},
  eventtitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-72814-803-8},
  langid = {english},
  file = {D:\Zotero\literature_data\storage\KNUMBHY6\Lee Âíå Kim - 2019 - Image Aesthetic Assessment Based on Pairwise Compa.pdf}
}

@online{lee2021,
  title = {Augmentation-{{Free Self-Supervised Learning}} on {{Graphs}}},
  author = {Lee, Namkyeong and Lee, Junseok and Park, Chanyoung},
  date = {2021-12-06},
  eprint = {2112.02472},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2112.02472},
  urldate = {2023-08-02},
  abstract = {Inspired by the recent success of self-supervised methods applied on images, self-supervised learning on graph structured data has seen rapid growth especially centered on augmentation-based contrastive methods. However, we argue that without carefully designed augmentation techniques, augmentations on graphs may behave arbitrarily in that the underlying semantics of graphs can drastically change. As a consequence, the performance of existing augmentation-based methods is highly dependent on the choice of augmentation scheme, i.e., hyperparameters associated with augmentations. In this paper, we propose a novel augmentation-free self-supervised learning framework for graphs, named AFGRL. Specifically, we generate an alternative view of a graph by discovering nodes that share the local structural information and the global semantics with the graph. Extensive experiments towards various node-level tasks, i.e., node classification, clustering, and similarity search on various real-world datasets demonstrate the superiority of AFGRL. The source code for AFGRL is available at https://github.com/Namkyeong/AFGRL.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Êó†Â¢ûÂº∫ÁöÑÂõæËá™ÁõëÁù£Â≠¶‰π†},
  file = {D\:\\Zotero\\literature_data\\storage\\2NH5F6GC\\Lee Á≠â - 2021 - Augmentation-Free Self-Supervised Learning on Grap.pdf;D\:\\Zotero\\literature_data\\storage\\UATLHLHA\\2112.html}
}

@article{lee2022,
  title = {Augmentation-{{Free Self-Supervised Learning}} on {{Graphs}}},
  author = {Lee, Namkyeong and Lee, Junseok and Park, Chanyoung},
  date = {2022-06-28},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {36},
  number = {7},
  pages = {7372--7380},
  issn = {2374-3468},
  doi = {10.1609/aaai.v36i7.20700},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/20700},
  urldate = {2023-07-06},
  abstract = {Inspired by the recent success of self-supervised methods applied on images, self-supervised learning on graph structured data has seen rapid growth especially centered on augmentation-based contrastive methods. However, we argue that without carefully designed augmentation techniques, augmentations on graphs may behave arbitrarily in that the underlying semantics of graphs can drastically change. As a consequence, the performance of existing augmentation-based methods is highly dependent on the choice of augmentation scheme, i.e., augmentation hyperparameters and combinations of augmentation. In this paper, we propose a novel augmentation-free self-supervised learning framework for graphs, named AFGRL. Specifically, we generate an alternative view of a graph by discovering nodes that share the local structural information and the global semantics with the graph. Extensive experiments towards various node-level tasks, i.e., node classification, clustering, and similarity search on various real-world datasets demonstrate the superiority of AFGRL. The source code for AFGRL is available at https://github.com/Namkyeong/AFGRL.},
  issue = {7},
  langid = {english},
  keywords = {Data Mining \& Knowledge Management (DMKM)},
  file = {D:\Zotero\literature_data\storage\NMPPB5CD\Lee Á≠â - 2022 - Augmentation-Free Self-Supervised Learning on Grap.pdf}
}

@online{lei2021,
  title = {Multi-{{Modal Aesthetic Assessment}} for {{MObile Gaming Image}}},
  author = {Lei, Zhenyu and Xie, Yejing and Ling, Suiyi and Pastor, Andreas and Wang, Junle and Callet, Patrick Le},
  date = {2021-01-27},
  eprint = {2101.11700},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2101.11700},
  urldate = {2023-08-02},
  abstract = {With the proliferation of various gaming technology, services, game styles, and platforms, multi-dimensional aesthetic assessment of the gaming contents is becoming more and more important for the gaming industry. Depending on the diverse needs of diversified game players, game designers, graphical developers, etc. in particular conditions, multi-modal aesthetic assessment is required to consider different aesthetic dimensions/perspectives. Since there are different underlying relationships between different aesthetic dimensions, e.g., between the ‚ÄòColorfulness‚Äô and ‚ÄòColor Harmony‚Äô, it could be advantageous to leverage effective information attached in multiple relevant dimensions. To this end, we solve this problem via multi-task learning. Our inclination is to seek and learn the correlations between different aesthetic relevant dimensions to further boost the generalization performance in predicting all the aesthetic dimensions. Therefore, the ‚Äòbottleneck‚Äô of obtaining good predictions with limited labeled data for one individual dimension could be unplugged by harnessing complementary sources of other dimensions, i.e., augment the training data indirectly by sharing training information across dimensions. According to experimental results, the proposed model outperforms state-of-the-art aesthetic metrics significantly in predicting four gaming aesthetic dimensions.},
  langid = {english},
  pubstate = {preprint},
  keywords = {68U10,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,J.0,Ê∏∏ÊàèÂõæÁâáË¥®ÈáèËØÑ‰ª∑},
  file = {D:\Zotero\literature_data\storage\5DV3G3I8\Lei Á≠â - 2021 - Multi-Modal Aesthetic Assessment for MObile Gaming.pdf}
}

@online{li2023,
  title = {Towards {{Verifiable Generation}}: {{A Benchmark}} for {{Knowledge-aware Language Model Attribution}}},
  shorttitle = {Towards {{Verifiable Generation}}},
  author = {Li, Xinze and Cao2, Yixin and Pan, Liangming and Ma, Yubo and Sun, Aixin},
  date = {2023-10-09},
  eprint = {2310.05634},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2310.05634},
  urldate = {2023-11-29},
  abstract = {Although achieving great success, Large Language Models (LLMs) usually suffer from unreliable hallucinations. In this paper, we define a new task of Knowledge-aware Language Model Attribution (KaLMA) that improves upon three core concerns on conventional attributed LMs. First, we extend attribution source from unstructured texts to Knowledge Graph (KG), whose rich structures benefit both the attribution performance and working scenarios. Second, we propose a new ``Conscious Incompetence" setting considering the incomplete knowledge repository, where the model identifies the need for supporting knowledge beyond the provided KG. Third, we propose a comprehensive automatic evaluation metric encompassing text quality, citation quality, and text citation alignment. To implement the above innovations, we build a dataset in biography domain BioKaLMA via a well-designed evolutionary question generation strategy, to control the question complexity and necessary knowledge to the answer. For evaluation, we develop a baseline solution and demonstrate the room for improvement in LLMs' citation generation, emphasizing the importance of incorporating the "Conscious Incompetence" setting, and the critical role of retrieval accuracy.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\EPGHKME9\\Li Á≠â - 2023 - Towards Verifiable Generation A Benchmark for Kno.pdf;D\:\\Zotero\\literature_data\\storage\\HN75YGMQ\\2310.html}
}

@inproceedings{li2023a,
  title = {{{HaluEval}}: {{A Large-Scale Hallucination Evaluation Benchmark}} for {{Large Language Models}}},
  shorttitle = {{{HaluEval}}},
  booktitle = {Proceedings of the 2023 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Li, Junyi and Cheng, Xiaoxue and Zhao, Xin and Nie, Jian-Yun and Wen, Ji-Rong},
  editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  date = {2023-12},
  pages = {6449--6464},
  publisher = {{Association for Computational Linguistics}},
  location = {{Singapore}},
  url = {https://aclanthology.org/2023.emnlp-main.397},
  urldate = {2023-12-13},
  abstract = {Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, i.e., content that conflicts with the source or cannot be verified by the factual knowledge. To understand what types of content and to which extent LLMs are apt to hallucinate, we introduce the Hallucination Evaluation for Large Language Models (HaluEval) benchmark, a large collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in recognizing hallucination. To generate these samples, we propose a ChatGPT-based two-step framework, i.e., sampling-then-filtering. Besides, we also hire some human labelers to annotate the hallucinations in ChatGPT responses. The empirical results suggest that ChatGPT is likely to generate hallucinated content in specific topics by fabricating unverifiable information (i.e., about 19.5\% user queries). Moreover, existing LLMs face great challenges in recognizing the hallucinations in texts. While, our experiments also prove that the hallucination recognition can be improved by providing external knowledge or adding reasoning steps.},
  eventtitle = {{{EMNLP}} 2023},
  keywords = {HaluEval},
  file = {D:\Zotero\literature_data\storage\C8QCTHHV\Li Á≠â - 2023 - HaluEval A Large-Scale Hallucination Evaluation B.pdf}
}

@online{li2023b,
  title = {{{BLIP-2}}: {{Bootstrapping Language-Image Pre-training}} with {{Frozen Image Encoders}} and {{Large Language Models}}},
  shorttitle = {{{BLIP-2}}},
  author = {Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  date = {2023-06-15},
  eprint = {2301.12597},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2301.12597},
  urldate = {2023-11-18},
  abstract = {The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7\% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.},
  pubstate = {preprint},
  version = {2},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\Zotero\\literature_data\\storage\\3PXCS2MB\\Li Á≠â - 2023 - BLIP-2 Bootstrapping Language-Image Pre-training .pdf;D\:\\Zotero\\literature_data\\storage\\AUPVXRHR\\2301.html}
}

@online{li2023c,
  title = {{{AGIQA-3K}}: {{An Open Database}} for {{AI-Generated Image Quality Assessment}}},
  shorttitle = {{{AGIQA-3K}}},
  author = {Li, Chunyi and Zhang, Zicheng and Wu, Haoning and Sun, Wei and Min, Xiongkuo and Liu, Xiaohong and Zhai, Guangtao and Lin, Weisi},
  date = {2023-06-12},
  eprint = {2306.04717},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2306.04717},
  urldate = {2023-07-13},
  abstract = {With the rapid advancements of the text-to-image generative model, AI-generated images (AGIs) have been widely applied to entertainment, education, social media, etc. However, considering the large quality variance among different AGIs, there is an urgent need for quality models that are consistent with human subjective ratings. To address this issue, we extensively consider various popular AGI models, generated AGI through different prompts and model parameters, and collected subjective scores at the perceptual quality and text-to-image alignment, thus building the most comprehensive AGI subjective quality database AGIQA-3K so far. Furthermore, we conduct a benchmark experiment on this database to evaluate the consistency between the current Image Quality Assessment (IQA) model and human perception, while proposing StairReward that significantly improves the assessment performance of subjective text-to-image alignment. We believe that the fine-grained subjective scores in AGIQA-3K will inspire subsequent AGI quality models to fit human subjective perception mechanisms at both perception and alignment levels and to optimize the generation result of future AGI models. The database is released on https://github.com/lcysyzxdxc/AGIQA-3k-Database.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing,Êï∞ÊçÆÈõÜ},
  file = {D\:\\Zotero\\literature_data\\storage\\5WNEDL2Y\\Li Á≠â - 2023 - AGIQA-3K An Open Database for AI-Generated Image .pdf;D\:\\Zotero\\literature_data\\storage\\ULGUGUW6\\2306.html}
}

@online{li2023d,
  title = {A {{Survey}} of {{Large Language Models Attribution}}},
  author = {Li, Dongfang and Sun, Zetian and Hu, Xinshuo and Liu, Zhenyu and Chen, Ziyang and Hu, Baotian and Wu, Aiguo and Zhang, Min},
  date = {2023-11-07},
  eprint = {2311.03731},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2311.03731},
  urldate = {2023-11-10},
  abstract = {Open-domain generative systems have gained significant attention in the field of conversational AI (e.g., generative search engines). This paper presents a comprehensive review of the attribution mechanisms employed by these systems, particularly large language models. Though attribution or citation improve the factuality and verifiability, issues like ambiguous knowledge reservoirs, inherent biases, and the drawbacks of excessive attribution can hinder the effectiveness of these systems. The aim of this survey is to provide valuable insights for researchers, aiding in the refinement of attribution methodologies to enhance the reliability and veracity of responses generated by open-domain generative systems. We believe that this field is still in its early stages; hence, we maintain a repository to keep track of ongoing studies at https://github.com/HITsz-TMG/awesome-llm-attributions.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\P9HLE8IH\\Li Á≠â - 2023 - A Survey of Large Language Models Attribution.pdf;D\:\\Zotero\\literature_data\\storage\\5VBHSGKL\\2311.html}
}

@online{liang2022,
  title = {Holistic {{Evaluation}} of {{Language Models}}},
  author = {Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and Newman, Benjamin and Yuan, Binhang and Yan, Bobby and Zhang, Ce and Cosgrove, Christian and Manning, Christopher D. and R√©, Christopher and Acosta-Navas, Diana and Hudson, Drew A. and Zelikman, Eric and Durmus, Esin and Ladhak, Faisal and Rong, Frieda and Ren, Hongyu and Yao, Huaxiu and Wang, Jue and Santhanam, Keshav and Orr, Laurel and Zheng, Lucia and Yuksekgonul, Mert and Suzgun, Mirac and Kim, Nathan and Guha, Neel and Chatterji, Niladri and Khattab, Omar and Henderson, Peter and Huang, Qian and Chi, Ryan and Xie, Sang Michael and Santurkar, Shibani and Ganguli, Surya and Hashimoto, Tatsunori and Icard, Thomas and Zhang, Tianyi and Chaudhary, Vishrav and Wang, William and Li, Xuechen and Mai, Yifan and Zhang, Yuhui and Koreeda, Yuta},
  date = {2022-11-16},
  eprint = {2211.09110},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2211.09110},
  urldate = {2023-08-26},
  abstract = {Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when possible (87.5\% of the time). This ensures metrics beyond accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze specific aspects (e.g. reasoning, disinformation). Third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, 21 of which were not previously used in mainstream LM evaluation. Prior to HELM, models on average were evaluated on just 17.9\% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0\%: now all 30 models have been densely benchmarked on the same core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings. For full transparency, we release all raw model prompts and completions publicly for further analysis, as well as a general modular toolkit. We intend for HELM to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {D:\Zotero\literature_data\storage\ZLZ6XSB6\Liang Á≠â - 2022 - Holistic Evaluation of Language Models.pdf}
}

@inproceedings{littwin2023,
  title = {Adaptive {{Optimization}} in the \$\textbackslash infty\$-{{Width Limit}}},
  author = {Littwin, Etai and Yang, Greg},
  date = {2023-02-01},
  url = {https://openreview.net/forum?id=zgVDqw9ZUES},
  urldate = {2023-06-04},
  abstract = {Recent works have developed detailed understanding of large neural networks' behaviors via their infinite-width limits, e.g., the neural tangent kernel (NTK) and the feature learning (\$\textbackslash mu\$) limits. These theories were developed for stochastic gradient descent. Yet, in practice, all large NN are trained using Adam or other adaptive gradient optimizers (AGO), which are not covered by such previous works. Here, we close this gap via the Tensor Programs framework. Specifically, for deep MLPs, we derive the NTK and \$\textbackslash mu\$ parametrizations as well as their infinite-width limits. We find 1) The NTK limit of AGO, in contrast to that of SGD, now depends nonlinearly on the loss derivative but nevertheless still fails to learn features; 2) this is fixed by the \$\textbackslash mu\$ limit of AGO (as in the case of SGD). To obtain these results, we extend the Tensor Programs language with a new instruction that allows one to express the gradient processing done by AGOs.},
  eventtitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {D:\Zotero\literature_data\storage\NKN93SCT\Littwin Âíå Yang - 2023 - Adaptive Optimization in the $infty$-Width Limit.pdf}
}

@inproceedings{littwin2023a,
  title = {Adaptive {{Optimization}} in the \$\textbackslash infty\$-{{Width Limit}}},
  author = {Littwin, Etai and Yang, Greg},
  date = {2023-02-01},
  url = {https://openreview.net/forum?id=zgVDqw9ZUES},
  urldate = {2023-05-08},
  abstract = {Recent works have developed detailed understanding of large neural networks' behaviors via their infinite-width limits, e.g., the neural tangent kernel (NTK) and the feature learning (\$\textbackslash mu\$) limits. These theories were developed for stochastic gradient descent. Yet, in practice, all large NN are trained using Adam or other adaptive gradient optimizers (AGO), which are not covered by such previous works. Here, we close this gap via the Tensor Programs framework. Specifically, for deep MLPs, we derive the NTK and \$\textbackslash mu\$ parametrizations as well as their infinite-width limits. We find 1) The NTK limit of AGO, in contrast to that of SGD, now depends nonlinearly on the loss derivative but nevertheless still fails to learn features; 2) this is fixed by the \$\textbackslash mu\$ limit of AGO (as in the case of SGD). To obtain these results, we extend the Tensor Programs language with a new instruction that allows one to express the gradient processing done by AGOs.},
  eventtitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  langid = {english},
  keywords = {MUP},
  file = {D:\Zotero\literature_data\storage\F8SEIEF2\Littwin Âíå Yang - 2023 - Adaptive Optimization in the $infty$-Width Limit.pdf}
}

@inproceedings{liu2020,
  title = {Composition-{{Aware Image Aesthetics Assessment}}},
  booktitle = {2020 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Liu, Dong and Puri, Rohit and Kamath, Nagendra and Bhattacharya, Subhabrata},
  date = {2020-03},
  pages = {3558--3567},
  publisher = {{IEEE}},
  location = {{Snowmass Village, CO, USA}},
  doi = {10.1109/WACV45572.2020.9093412},
  url = {https://ieeexplore.ieee.org/document/9093412/},
  urldate = {2023-06-24},
  abstract = {Automatic image aesthetics assessment is important for a wide variety of applications such as on-line photo suggestion, photo album management and image retrieval. Previous methods have focused on mapping the holistic image content to a high or low aesthetics rating. However, the composition information of an image characterizes the harmony of its visual elements according to the principles of art, and provides richer information for learning aesthetics. In this work, we propose to model the image composition information as the mutual dependency of its local regions, and design a novel architecture to leverage such information to boost the performance of aesthetics assessment. To achieve this, we densely partition an image into local regions and compute aesthetics-preserving features over the regions to characterize the aesthetics properties of image content. With the feature representation of local regions, we build a region composition graph in which each node denotes one region and any two nodes are connected by an edge weighted by the similarity of the region features. We perform reasoning on this graph via graph convolution, in which the activation of each node is determined by its highly correlated neighbors. Our method naturally uncovers the mutual dependency of local regions in the network training procedure, and achieves the state-of-the-art performance on the benchmark visual aesthetics datasets.},
  eventtitle = {2020 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  isbn = {978-1-72816-553-0},
  langid = {english},
  file = {D:\Zotero\literature_data\storage\HGG2SMDQ\Liu Á≠â - 2020 - Composition-Aware Image Aesthetics Assessment.pdf}
}

@article{liu2023,
  title = {Pre-Train, {{Prompt}}, and {{Predict}}: {{A Systematic Survey}} of {{Prompting Methods}} in {{Natural Language Processing}}},
  shorttitle = {Pre-Train, {{Prompt}}, and {{Predict}}},
  author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  date = {2023-09-30},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {55},
  number = {9},
  pages = {1--35},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3560815},
  url = {https://dl.acm.org/doi/10.1145/3560815},
  urldate = {2023-10-19},
  abstract = {This article surveys and organizes research works in a new paradigm in natural language processing, which we dub ‚Äúprompt-based learning.‚Äù Unlike traditional supervised learning, which trains a model to take in an input                                x                              and predict an output                                y                              as               P               (                                y|x                              ), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input                                x                              is modified using a               template               into a textual string               prompt                                x‚Ä≤                              that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string                                xÃÇ                              , from which the final output                                y                              can be derived. This framework is powerful and attractive for a number of reasons: It allows the language model to be               pre-trained               on massive amounts of raw text, and by defining a new prompting function the model is able to perform               few-shot               or even               zero-shot               learning, adapting to new scenarios with few or no labeled data. In this article, we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.,~the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources, e.g., a website               NLPedia‚ÄìPretrain               including constantly updated survey and paperlist.},
  langid = {english},
  keywords = {È¢ÑËÆ≠ÁªÉ},
  file = {D:\Zotero\literature_data\storage\37FEXV53\Liu Á≠â - 2023 - Pre-train, Prompt, and Predict A Systematic Surve.pdf}
}

@article{liu2023a,
  title = {Graph {{Self-Supervised Learning}}: {{A Survey}}},
  shorttitle = {Graph {{Self-Supervised Learning}}},
  author = {Liu, Yixin and Jin, Ming and Pan, Shirui and Zhou, Chuan and Zheng, Yu and Xia, Feng and Yu, Philip S.},
  date = {2023-06},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {35},
  number = {6},
  pages = {5879--5900},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2022.3172903},
  abstract = {Deep learning on graphs has attracted significant interests recently. However, most of the works have focused on (semi-) supervised learning, resulting in shortcomings including heavy label reliance, poor generalization, and weak robustness. To address these issues, self-supervised learning (SSL), which extracts informative knowledge through well-designed pretext tasks without relying on manual labels, has become a promising and trending learning paradigm for graph data. Different from SSL on other domains like computer vision and natural language processing, SSL on graphs has an exclusive background, design ideas, and taxonomies. Under the umbrella of graph self-supervised learning, we present a timely and comprehensive review of the existing approaches which employ SSL techniques for graph data. We construct a unified framework that mathematically formalizes the paradigm of graph SSL. According to the objectives of pretext tasks, we divide these approaches into four categories: generation-based, auxiliary property-based, contrast-based, and hybrid approaches. We further describe the applications of graph SSL across various research fields and summarize the commonly used datasets, evaluation benchmark, performance comparison and open-source codes of graph SSL. Finally, we discuss the remaining challenges and potential future directions in this research field.},
  eventtitle = {{{IEEE Transactions}} on {{Knowledge}} and {{Data Engineering}}},
  keywords = {Data models,deep learning,Deep learning,graph analytics,graph neural networks,graph representation learning,Manuals,Natural language processing,Self-supervised learning,Supervised learning,Task analysis,Taxonomy},
  file = {D\:\\Zotero\\literature_data\\storage\\9KJAJTLT\\Liu Á≠â - 2023 - Graph Self-Supervised Learning A Survey.pdf;D\:\\Zotero\\literature_data\\storage\\GW9NPHND\\9770382.html}
}

@online{liu2023b,
  title = {Evaluating {{Verifiability}} in {{Generative Search Engines}}},
  author = {Liu, Nelson F. and Zhang, Tianyi and Liang, Percy},
  date = {2023-10-23},
  eprint = {2304.09848},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.09848},
  url = {http://arxiv.org/abs/2304.09848},
  urldate = {2023-11-29},
  abstract = {Generative search engines directly generate responses to user queries, along with in-line citations. A prerequisite trait of a trustworthy generative search engine is verifiability, i.e., systems should cite comprehensively (high citation recall; all statements are fully supported by citations) and accurately (high citation precision; every cite supports its associated statement). We conduct human evaluation to audit four popular generative search engines -- Bing Chat, NeevaAI, perplexity.ai, and YouChat -- across a diverse set of queries from a variety of sources (e.g., historical Google user queries, dynamically-collected open-ended questions on Reddit, etc.). We find that responses from existing generative search engines are fluent and appear informative, but frequently contain unsupported statements and inaccurate citations: on average, a mere 51.5\% of generated sentences are fully supported by citations and only 74.5\% of citations support their associated sentence. We believe that these results are concerningly low for systems that may serve as a primary tool for information-seeking users, especially given their facade of trustworthiness. We hope that our results further motivate the development of trustworthy generative search engines and help researchers and users better understand the shortcomings of existing commercial systems.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {D\:\\Zotero\\literature_data\\storage\\YQJYHY24\\Liu Á≠â - 2023 - Evaluating Verifiability in Generative Search Engi.pdf;D\:\\Zotero\\literature_data\\storage\\ZPEM48EX\\2304.html}
}

@online{liu2023c,
  title = {Evaluating {{Verifiability}} in {{Generative Search Engines}}},
  author = {Liu, Nelson F. and Zhang, Tianyi and Liang, Percy},
  date = {2023-10-23},
  eprint = {2304.09848},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2304.09848},
  urldate = {2023-11-01},
  abstract = {Generative search engines directly generate responses to user queries, along with in-line citations. A prerequisite trait of a trustworthy generative search engine is verifiability, i.e., systems should cite comprehensively (high citation recall; all statements are fully supported by citations) and accurately (high citation precision; every cite supports its associated statement). We conduct human evaluation to audit four popular generative search engines -- Bing Chat, NeevaAI, perplexity.ai, and YouChat -- across a diverse set of queries from a variety of sources (e.g., historical Google user queries, dynamically-collected open-ended questions on Reddit, etc.). We find that responses from existing generative search engines are fluent and appear informative, but frequently contain unsupported statements and inaccurate citations: on average, a mere 51.5\% of generated sentences are fully supported by citations and only 74.5\% of citations support their associated sentence. We believe that these results are concerningly low for systems that may serve as a primary tool for information-seeking users, especially given their facade of trustworthiness. We hope that our results further motivate the development of trustworthy generative search engines and help researchers and users better understand the shortcomings of existing commercial systems.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {D\:\\Zotero\\literature_data\\storage\\Q9YRJ8KE\\Liu Á≠â - 2023 - Evaluating Verifiability in Generative Search Engi.pdf;D\:\\Zotero\\literature_data\\storage\\5QU7GJEK\\2304.html}
}

@online{ma2023,
  title = {Entropy {{Neural Estimation}} for {{Graph Contrastive Learning}}},
  author = {Ma, Yixuan and Zhang, Xiaolin and Zhang, Peng and Zhan, Kun},
  date = {2023-07-25},
  eprint = {2307.13944},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2307.13944},
  urldate = {2023-07-29},
  abstract = {Contrastive learning on graphs aims at extracting distinguishable high-level representations of nodes. In this paper, we theoretically illustrate that the entropy of a dataset can be approximated by maximizing the lower bound of the mutual information across different views of a graph, \textbackslash ie, entropy is estimated by a neural network. Based on this finding, we propose a simple yet effective subset sampling strategy to contrast pairwise representations between views of a dataset. In particular, we randomly sample nodes and edges from a given graph to build the input subset for a view. Two views are fed into a parameter-shared Siamese network to extract the high-dimensional embeddings and estimate the information entropy of the entire graph. For the learning process, we propose to optimize the network using two objectives, simultaneously. Concretely, the input of the contrastive loss function consists of positive and negative pairs. Our selection strategy of pairs is different from previous works and we present a novel strategy to enhance the representation ability of the graph encoder by selecting nodes based on cross-view similarities. We enrich the diversity of the positive and negative pairs by selecting highly similar samples and totally different data with the guidance of cross-view similarity scores, respectively. We also introduce a cross-view consistency constraint on the representations generated from the different views. This objective guarantees the learned representations are consistent across views from the perspective of the entire graph. We conduct extensive experiments on seven graph benchmarks, and the proposed approach achieves competitive performance compared to the current state-of-the-art methods. The source code will be publicly released once this paper is accepted.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\UDHNEE5D\\Ma Á≠â - 2023 - Entropy Neural Estimation for Graph Contrastive Le.pdf;D\:\\Zotero\\literature_data\\storage\\G5X7T7WV\\2307.html}
}

@online{ma2023a,
  title = {Class-{{Imbalanced Learning}} on {{Graphs}}: {{A Survey}}},
  shorttitle = {Class-{{Imbalanced Learning}} on {{Graphs}}},
  author = {Ma, Yihong and Tian, Yijun and Moniz, Nuno and Chawla, Nitesh V.},
  date = {2023-04-09},
  eprint = {2304.04300},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.04300},
  url = {http://arxiv.org/abs/2304.04300},
  urldate = {2023-04-13},
  abstract = {The rapid advancement in data-driven research has increased the demand for effective graph data analysis. However, real-world data often exhibits class imbalance, leading to poor performance of machine learning models. To overcome this challenge, class-imbalanced learning on graphs (CILG) has emerged as a promising solution that combines the strengths of graph representation learning and class-imbalanced learning. In recent years, significant progress has been made in CILG. Anticipating that such a trend will continue, this survey aims to offer a comprehensive understanding of the current state-of-the-art in CILG and provide insights for future research directions. Concerning the former, we introduce the first taxonomy of existing work and its connection to existing imbalanced learning literature. Concerning the latter, we critically analyze recent work in CILG and discuss urgent lines of inquiry within the topic. Moreover, we provide a continuously maintained reading list of papers and code at https://github.com/yihongma/CILG-Papers.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\SXNJU7RL\\Ma Á≠â - 2023 - Class-Imbalanced Learning on Graphs A Survey.pdf;D\:\\Zotero\\literature_data\\storage\\MSN72MMA\\2304.html}
}

@inproceedings{mahendran2015,
  title = {Understanding Deep Image Representations by Inverting Them},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Mahendran, Aravindh and Vedaldi, Andrea},
  date = {2015-06},
  pages = {5188--5196},
  publisher = {{IEEE}},
  location = {{Boston, MA, USA}},
  doi = {10.1109/CVPR.2015.7299155},
  url = {http://ieeexplore.ieee.org/document/7299155/},
  urldate = {2023-10-31},
  abstract = {Image representations, from SIFT and Bag of Visual Words to Convolutional Neural Networks (CNNs), are a crucial component of almost any image understanding system. Nevertheless, our understanding of them remains limited. In this paper we conduct a direct analysis of the visual information contained in representations by asking the following question: given an encoding of an image, to which extent is it possible to reconstruct the image itself? To answer this question we contribute a general framework to invert representations. We show that this method can invert representations such as HOG more accurately than recent alternatives while being applicable to CNNs too. We then use this technique to study the inverse of recent state-of-theart CNN image representations for the first time. Among our findings, we show that several layers in CNNs retain photographically accurate information about the image, with different degrees of geometric and photometric invariance.},
  eventtitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-4673-6964-0},
  langid = {english},
  keywords = {‰Ωú‰∏öÈòÖËØªÊùêÊñô},
  file = {D\:\\Zotero\\literature_data\\storage\\8IUPWHUV\\ËßÜËßâË°®Á§∫Â≠¶‰π†-Â§ß‰Ωú‰∏ö.pdf;D\:\\Zotero\\literature_data\\storage\\ZCK9ZGMB\\Mahendran Âíå Vedaldi - 2015 - Understanding deep image representations by invert.pdf}
}

@online{malaviya2023,
  title = {{{ExpertQA}}: {{Expert-Curated Questions}} and {{Attributed Answers}}},
  shorttitle = {{{ExpertQA}}},
  author = {Malaviya, Chaitanya and Lee, Subin and Chen, Sihao and Sieber, Elizabeth and Yatskar, Mark and Roth, Dan},
  date = {2023-09-14},
  eprint = {2309.07852},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2309.07852},
  urldate = {2023-11-15},
  abstract = {As language models are adapted by a more sophisticated and diverse set of users, the importance of guaranteeing that they provide factually correct information supported by verifiable sources is critical across fields of study \& professions. This is especially the case for high-stakes fields, such as medicine and law, where the risk of propagating false information is high and can lead to undesirable societal consequences. Previous work studying factuality and attribution has not focused on analyzing these characteristics of language model outputs in domain-specific scenarios. In this work, we present an evaluation study analyzing various axes of factuality and attribution provided in responses from a few systems, by bringing domain experts in the loop. Specifically, we first collect expert-curated questions from 484 participants across 32 fields of study, and then ask the same experts to evaluate generated responses to their own questions. We also ask experts to revise answers produced by language models, which leads to ExpertQA, a high-quality long-form QA dataset with 2177 questions spanning 32 fields, along with verified answers and attributions for claims in the answers.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {D:\Zotero\literature_data\storage\NKLJZZ7L\Malaviya Á≠â - 2023 - ExpertQA Expert-Curated Questions and Attributed .pdf}
}

@online{manakul2023,
  title = {{{SelfCheckGPT}}: {{Zero-Resource Black-Box Hallucination Detection}} for {{Generative Large Language Models}}},
  shorttitle = {{{SelfCheckGPT}}},
  author = {Manakul, Potsawee and Liusie, Adian and Gales, Mark J. F.},
  date = {2023-10-11},
  eprint = {2303.08896},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.08896},
  url = {http://arxiv.org/abs/2303.08896},
  urldate = {2023-11-22},
  abstract = {Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose "SelfCheckGPT", a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if an LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GPT-3 to generate passages about individuals from the WikiBio dataset, and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that our approach has considerably higher AUC-PR scores in sentence-level hallucination detection and higher correlation scores in passage-level factuality assessment compared to grey-box methods.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\CDE6PDVE\\Manakul Á≠â - 2023 - SelfCheckGPT Zero-Resource Black-Box Hallucinatio.pdf;D\:\\Zotero\\literature_data\\storage\\JSFY8UZC\\2303.html}
}

@incollection{marra2018,
  title = {An {{Unsupervised Character-Aware Neural Approach}} to {{Word}} and {{Context Representation Learning}}},
  booktitle = {Artificial {{Neural Networks}} and {{Machine Learning}} ‚Äì {{ICANN}} 2018},
  author = {Marra, Giuseppe and Zugarini, Andrea and Melacci, Stefano and Maggini, Marco},
  editor = {K≈Ørkov√°, Vƒõra and Manolopoulos, Yannis and Hammer, Barbara and Iliadis, Lazaros and Maglogiannis, Ilias},
  date = {2018},
  volume = {11141},
  pages = {126--136},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-01424-7_13},
  url = {http://link.springer.com/10.1007/978-3-030-01424-7_13},
  urldate = {2023-05-17},
  abstract = {In the last few years, neural networks have been intensively used to develop meaningful distributed representations of words and contexts around them. When these representations, also known as ‚Äúembeddings‚Äù, are learned from unsupervised large corpora, they can be transferred to different tasks with positive effects in terms of performances, especially when only a few supervisions are available. In this work, we further extend this concept, and we present an unsupervised neural architecture that jointly learns word and context embeddings, processing words as sequences of characters. This allows our model to spot the regularities that are due to the word morphology, and to avoid the need of a fixed-sized input vocabulary of words. We show that we can learn compact encoders that, despite the relatively small number of parameters, reach high-level performances in downstream tasks, comparing them with related state-of-the-art approaches or with fully supervised methods.},
  isbn = {978-3-030-01423-0 978-3-030-01424-7},
  langid = {english},
  file = {D:\Zotero\literature_data\storage\622S7DCL\Marra Á≠â - 2018 - An Unsupervised Character-Aware Neural Approach to.pdf}
}

@online{mccann2017,
  title = {Learned in {{Translation}}: {{Contextualized Word Vectors}}},
  shorttitle = {Learned in {{Translation}}},
  author = {McCann, Bryan and Bradbury, James and Xiong, Caiming and Socher, Richard},
  date = {2017-08-01},
  url = {https://arxiv.org/abs/1708.00107v2},
  urldate = {2023-05-17},
  abstract = {Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.},
  langid = {english},
  organization = {{arXiv.org}},
  file = {D:\Zotero\literature_data\storage\4A3USU2X\McCann Á≠â - 2017 - Learned in Translation Contextualized Word Vector.pdf}
}

@online{micikevicius2018,
  title = {Mixed {{Precision Training}}},
  author = {Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and Wu, Hao},
  date = {2018-02-15},
  eprint = {1710.03740},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1710.03740},
  url = {http://arxiv.org/abs/1710.03740},
  urldate = {2023-06-03},
  abstract = {Deep neural networks have enabled progress in a wide variety of applications. Growing the size of the neural network typically results in improved accuracy. As model sizes grow, the memory and compute requirements for training these models also increases. We introduce a technique to train deep neural networks using half precision floating point numbers. In our technique, weights, activations and gradients are stored in IEEE half-precision format. Half-precision floating numbers have limited numerical range compared to single-precision numbers. We propose two techniques to handle this loss of information. Firstly, we recommend maintaining a single-precision copy of the weights that accumulates the gradients after each optimizer step. This single-precision copy is rounded to half-precision format during training. Secondly, we propose scaling the loss appropriately to handle the loss of information with half-precision gradients. We demonstrate that this approach works for a wide variety of models including convolution neural networks, recurrent neural networks and generative adversarial networks. This technique works for large scale models with more than 100 million parameters trained on large datasets. Using this approach, we can reduce the memory consumption of deep learning models by nearly 2x. In future processors, we can also expect a significant computation speedup using half-precision hardware units.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\Z79GYVCX\\Micikevicius Á≠â - 2018 - Mixed Precision Training.pdf;D\:\\Zotero\\literature_data\\storage\\367U9H5D\\1710.html}
}

@online{mikolov2013,
  title = {Distributed {{Representations}} of {{Words}} and {{Phrases}} and Their {{Compositionality}}},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  date = {2013-10-16},
  eprint = {1310.4546},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1310.4546},
  url = {http://arxiv.org/abs/1310.4546},
  urldate = {2023-05-16},
  abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\N2ID93WN\\Mikolov Á≠â - 2013 - Distributed Representations of Words and Phrases a.pdf;D\:\\Zotero\\literature_data\\storage\\XMX54RHB\\1310.html}
}

@online{min2023,
  title = {{{FActScore}}: {{Fine-grained Atomic Evaluation}} of {{Factual Precision}} in {{Long Form Text Generation}}},
  shorttitle = {{{FActScore}}},
  author = {Min, Sewon and Krishna, Kalpesh and Lyu, Xinxi and Lewis, Mike and Yih, Wen-tau and Koh, Pang Wei and Iyyer, Mohit and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
  date = {2023-10-11},
  eprint = {2305.14251},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2305.14251},
  urldate = {2023-11-29},
  abstract = {Evaluating the factuality of long-form text generated by large language models (LMs) is non-trivial because (1) generations often contain a mixture of supported and unsupported pieces of information, making binary judgments of quality inadequate, and (2) human evaluation is time-consuming and costly. In this paper, we introduce FACTSCORE, a new evaluation that breaks a generation into a series of atomic facts and computes the percentage of atomic facts supported by a reliable knowledge source. We conduct an extensive human evaluation to obtain FACTSCOREs of people biographies generated by several state-of-the-art commercial LMs -- InstructGPT, ChatGPT, and the retrieval-augmented PerplexityAI -- and report new analysis demonstrating the need for such a fine-grained score (e.g., ChatGPT only achieves 58\%). Since human evaluation is costly, we also introduce an automated model that estimates FACTSCORE using retrieval and a strong language model, with less than a 2\% error rate. Finally, we use this automated metric to evaluate 6,500 generations from a new set of 13 recent LMs that would have cost \$26K if evaluated by humans, with various findings: GPT-4 and ChatGPT are more factual than public models, and Vicuna and Alpaca are some of the best public models. FACTSCORE is available for public use via `pip install factscore`.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\NDY38WUC\\Min Á≠â - 2023 - FActScore Fine-grained Atomic Evaluation of Factu.pdf;D\:\\Zotero\\literature_data\\storage\\WEEPSEVK\\2305.html}
}

@inproceedings{mnih2008,
  title = {A {{Scalable Hierarchical Distributed Language Model}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Mnih, Andriy and Hinton, Geoffrey E},
  date = {2008},
  volume = {21},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper_files/paper/2008/hash/1e056d2b0ebd5c878c550da6ac5d3724-Abstract.html},
  urldate = {2023-05-16},
  abstract = {Neural probabilistic language models (NPLMs) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models. The main drawback of NPLMs is their extremely long training and testing times. Morin and Bengio have proposed a hierarchical language model built around a binary tree of words that was two orders of magnitude faster than the non-hierarchical language model it was based on. However, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. We introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. We then show that the resulting models can outperform non-hierarchical models and achieve state-of-the-art performance.},
  file = {D:\Zotero\literature_data\storage\7IYQK9VA\Mnih Âíå Hinton - 2008 - A Scalable Hierarchical Distributed Language Model.pdf}
}

@online{muennighoff2023,
  title = {Crosslingual {{Generalization}} through {{Multitask Finetuning}}},
  author = {Muennighoff, Niklas and Wang, Thomas and Sutawika, Lintang and Roberts, Adam and Biderman, Stella and Scao, Teven Le and Bari, M. Saiful and Shen, Sheng and Yong, Zheng-Xin and Schoelkopf, Hailey and Tang, Xiangru and Radev, Dragomir and Aji, Alham Fikri and Almubarak, Khalid and Albanie, Samuel and Alyafeai, Zaid and Webson, Albert and Raff, Edward and Raffel, Colin},
  date = {2023-05-29},
  eprint = {2211.01786},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2211.01786},
  url = {http://arxiv.org/abs/2211.01786},
  urldate = {2023-06-03},
  abstract = {Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https://github.com/bigscience-workshop/xmtf.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\PDAA4WGD\\Muennighoff Á≠â - 2023 - Crosslingual Generalization through Multitask Fine.pdf;D\:\\Zotero\\literature_data\\storage\\9NCINFM7\\2211.html}
}

@online{muller2023,
  title = {Evaluating and {{Modeling Attribution}} for {{Cross-Lingual Question Answering}}},
  author = {Muller, Benjamin and Wieting, John and Clark, Jonathan H. and Kwiatkowski, Tom and Ruder, Sebastian and Soares, Livio Baldini and Aharoni, Roee and Herzig, Jonathan and Wang, Xinyi},
  date = {2023-11-15},
  eprint = {2305.14332},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.14332},
  url = {http://arxiv.org/abs/2305.14332},
  urldate = {2023-11-29},
  abstract = {Trustworthy answer content is abundant in many high-resource languages and is instantly accessible through question answering systems, yet this content can be hard to access for those that do not speak these languages. The leap forward in cross-lingual modeling quality offered by generative language models offers much promise, yet their raw generations often fall short in factuality. To improve trustworthiness in these systems, a promising direction is to attribute the answer to a retrieved source, possibly in a content-rich language different from the query. Our work is the first to study attribution for cross-lingual question answering. First, we collect data in 5 languages to assess the attribution level of a state-of-the-art cross-lingual QA system. To our surprise, we find that a substantial portion of the answers is not attributable to any retrieved passages (up to 50\% of answers exactly matching a gold reference) despite the system being able to attend directly to the retrieved text. Second, to address this poor attribution level, we experiment with a wide range of attribution detection techniques. We find that Natural Language Inference models and PaLM 2 fine-tuned on a very small amount of attribution data can accurately detect attribution. Based on these models, we improve the attribution level of a cross-lingual question-answering system. Overall, we show that current academic generative cross-lingual QA systems have substantial shortcomings in attribution and we build tooling to mitigate these issues.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\LQ86YI5S\\Muller Á≠â - 2023 - Evaluating and Modeling Attribution for Cross-Ling.pdf;D\:\\Zotero\\literature_data\\storage\\Q3NM9725\\2305.html}
}

@online{nakano2022,
  title = {{{WebGPT}}: {{Browser-assisted}} Question-Answering with Human Feedback},
  shorttitle = {{{WebGPT}}},
  author = {Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and Jiang, Xu and Cobbe, Karl and Eloundou, Tyna and Krueger, Gretchen and Button, Kevin and Knight, Matthew and Chess, Benjamin and Schulman, John},
  date = {2022-06-01},
  eprint = {2112.09332},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2112.09332},
  urldate = {2023-11-29},
  abstract = {We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56\% of the time to those of our human demonstrators, and 69\% of the time to the highest-voted answer from Reddit.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\I79MBVC8\\Nakano Á≠â - 2022 - WebGPT Browser-assisted question-answering with h.pdf;D\:\\Zotero\\literature_data\\storage\\STII7YBG\\Nakano Á≠â - 2022 - WebGPT Browser-assisted question-answering with h.pdf;D\:\\Zotero\\literature_data\\storage\\33ED2UAR\\2112.html}
}

@online{nijkamp2023,
  title = {{{CodeGen}}: {{An Open Large Language Model}} for {{Code}} with {{Multi-Turn Program Synthesis}}},
  shorttitle = {{{CodeGen}}},
  author = {Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},
  date = {2023-02-27},
  eprint = {2203.13474},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2203.13474},
  urldate = {2023-06-03},
  abstract = {Program synthesis strives to generate a computer program as a solution to a given problem specification, expressed with input-output examples or natural language descriptions. The prevalence of large language models advances the state-of-the-art for program synthesis, though limited training resources and data impede open access to such models. To democratize this, we train and release a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open source the training library JAXFORMER. We show the utility of the trained model by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval. We further investigate the multi-step paradigm for program synthesis, where a single program is factorized into multiple prompts specifying subproblems. To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts. Our analysis on MTPB shows that the same intent provided to CODEGEN in multiturn fashion significantly improves program synthesis over that provided as a single turn. We make the training library JAXFORMER and model checkpoints available as open source contribution: https://github.com/salesforce/CodeGen.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Programming Languages},
  file = {D:\Zotero\literature_data\storage\Z2JJ595R\Nijkamp Á≠â - 2023 - CodeGen An Open Large Language Model for Code wit.pdf}
}

@online{nijkamp2023a,
  title = {{{CodeGen}}: {{An Open Large Language Model}} for {{Code}} with {{Multi-Turn Program Synthesis}}},
  shorttitle = {{{CodeGen}}},
  author = {Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},
  date = {2023-02-27},
  eprint = {2203.13474},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2203.13474},
  urldate = {2023-06-03},
  abstract = {Program synthesis strives to generate a computer program as a solution to a given problem specification, expressed with input-output examples or natural language descriptions. The prevalence of large language models advances the state-of-the-art for program synthesis, though limited training resources and data impede open access to such models. To democratize this, we train and release a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open source the training library JAXFORMER. We show the utility of the trained model by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval. We further investigate the multi-step paradigm for program synthesis, where a single program is factorized into multiple prompts specifying subproblems. To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts. Our analysis on MTPB shows that the same intent provided to CODEGEN in multiturn fashion significantly improves program synthesis over that provided as a single turn. We make the training library JAXFORMER and model checkpoints available as open source contribution: https://github.com/salesforce/CodeGen.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Programming Languages},
  file = {D:\Zotero\literature_data\storage\KGLF9NL4\Nijkamp Á≠â - 2023 - CodeGen An Open Large Language Model for Code wit.pdf}
}

@online{nylund2023,
  title = {Time Is {{Encoded}} in the {{Weights}} of {{Finetuned Language Models}}},
  author = {Nylund, Kai and Gururangan, Suchin and Smith, Noah A.},
  date = {2023-12-30},
  eprint = {2312.13401},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2312.13401},
  urldate = {2024-01-06},
  abstract = {We present time vectors, a simple tool to customize language models to new time periods. Time vectors are created by finetuning a language model on data from a single time (e.g., a year or month), and then subtracting the weights of the original pretrained model. This vector specifies a direction in weight space that, as our experiments show, improves performance on text from that time period. Time vectors specialized to adjacent time periods appear to be positioned closer together in a manifold. Using this structure, we interpolate between time vectors to induce new models that perform better on intervening and future time periods, without any additional training. We demonstrate the consistency of our findings across different tasks, domains, model sizes, and time scales. Our results suggest that time is encoded in the weight space of finetuned models.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\AUKRY54J\\Nylund Á≠â - 2023 - Time is Encoded in the Weights of Finetuned Langua.pdf;D\:\\Zotero\\literature_data\\storage\\3RZUZJLE\\2312.html}
}

@online{openai2023,
  title = {{{GPT-4 Technical Report}}},
  author = {OpenAI},
  date = {2023-03-27},
  eprint = {2303.08774},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2303.08774},
  urldate = {2023-04-17},
  abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformerbased model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4‚Äôs performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,GPT-4},
  file = {D:\Zotero\literature_data\storage\NJUR2E5L\OpenAI - 2023 - GPT-4 Technical Report.pdf}
}

@article{ouyang,
  title = {Training Language Models to Follow Instructions with Human Feedback},
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  abstract = {Making language models bigger does not inherently make them better at following a user‚Äôs intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback (RLHF). We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
  langid = {english},
  file = {D:\Zotero\literature_data\storage\VL6F8QJ2\Ouyang Á≠â - Training language models to follow instructions wi.pdf}
}

@article{ouyanga,
  title = {Training Language Models to Follow Instructions with Human Feedback},
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  abstract = {Making language models bigger does not inherently make them better at following a user‚Äôs intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through a language model API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
  langid = {english},
  keywords = {GPT-3.5},
  file = {D:\Zotero\literature_data\storage\WNPXJ3HM\Ouyang Á≠â - Training language models to follow instructions wi.pdf}
}

@article{park,
  title = {{{TRAK}}: {{Attributing Model Behavior}} at {{Scale}}},
  author = {Park, Sung Min and Georgiev, Kristian and Ilyas, Andrew and Leclerc, Guillaume and MƒÖdry, Aleksander},
  abstract = {The goal of data attribution is to trace model predictions back to training data. Despite a long line of work towards this goal, existing approaches to data attribution tend to force users to choose between computational tractability and efficacy. That is, computationally tractable methods can struggle with accurately attributing model predictions in non-convex settings (e.g., in the context of deep neural networks), while methods that are effective in such regimes require training thousands of models, which makes them impractical for large models or datasets. In this work, we introduce TRAK (Tracing with the Randomly-projected After Kernel), a data attribution method that is both effective and computationally tractable for large-scale, differentiable models. In particular, by leveraging only a handful of trained models, TRAK can match the performance of attribution methods that require training thousands of models. We demonstrate the utility of TRAK across various modalities and scales: image classifiers trained on ImageNet, vision-language models (CLIP), and language models (BERT and mT5). We provide code for using TRAK (and reproducing our work) at https://github.com/MadryLab/trak.},
  langid = {english},
  keywords = {Ë¥°ÁåÆÊÄßÂΩíÂõ†},
  file = {D:\Zotero\literature_data\storage\CEXXPGX4\Park Á≠â - TRAK Attributing Model Behavior at Scale.pdf}
}

@article{parka,
  title = {Benchmark for {{Compositional Text-to-Image Synthesis}}},
  author = {Park, Dong Huk and Azadi, Samaneh and Liu, Xihui and Darrell, Trevor and Rohrbach, Anna},
  abstract = {Rapid progress in text-to-image generation has been often measured by Frec¬¥het Inception Distance (FID) to capture how realistic the generated images are, or by R-Precision to assess if they are well conditioned on the given textual descriptions. However, a systematic study on how well the text-to-image synthesis models generalize to novel word compositions is missing. In this work, we focus on assessing how true the generated images are to the input texts in this particularly challenging scenario of novel compositions. We present the first systematic study of text-to-image generation on zero-shot compositional splits targeting two scenarios, unseen object-color (e.g. ‚Äúblue petal‚Äù) and object-shape (e.g. ‚Äúlong beak‚Äù) phrases. We create new benchmarks building on the existing CUB and Oxford Flowers datasets. We also propose a new metric, based on a powerful vision-and-language CLIP model, which we leverage to compute R-Precision. This is in contrast to the common approach where the same retrieval model is used during training and evaluation, potentially leading to biased behavior. We experiment with several recent text-to-image generation methods. Our automatic and human evaluation confirm that there is indeed a gap in performance when encountering previously unseen phrases. We show that the image correctness rather than purely perceptual quality is especially impacted. Finally, our CLIP-R-Precision metric demonstrates better correlation with human judgments than the commonly used metric. Dataset and evaluation code at: https://github.com/Seth-Park/comp-t2i-dataset.},
  langid = {english},
  file = {D:\Zotero\literature_data\storage\TMBPRHTG\Park Á≠â - Benchmark for Compositional Text-to-Image Synthesi.pdf}
}

@online{peters2018,
  title = {Dissecting {{Contextual Word Embeddings}}: {{Architecture}} and {{Representation}}},
  shorttitle = {Dissecting {{Contextual Word Embeddings}}},
  author = {Peters, Matthew E. and Neumann, Mark and Zettlemoyer, Luke and Yih, Wen-tau},
  date = {2018-08-27},
  url = {https://arxiv.org/abs/1808.08949v2},
  urldate = {2023-05-17},
  abstract = {Contextual word representations derived from pre-trained bidirectional language models (biLMs) have recently been shown to provide significant improvements to the state of the art for a wide range of NLP tasks. However, many questions remain as to how and why these models are so effective. In this paper, we present a detailed empirical study of how the choice of neural architecture (e.g. LSTM, CNN, or self attention) influences both end task accuracy and qualitative properties of the representations that are learned. We show there is a tradeoff between speed and accuracy, but all architectures learn high quality contextual representations that outperform word embeddings for four challenging NLP tasks. Additionally, all architectures learn representations that vary with network depth, from exclusively morphological based at the word embedding layer through local syntax based in the lower contextual layers to longer range semantics such coreference at the upper layers. Together, these results suggest that unsupervised biLMs, independent of architecture, are learning much more about the structure of language than previously appreciated.},
  langid = {english},
  organization = {{arXiv.org}},
  file = {D:\Zotero\literature_data\storage\W2VQ2A3K\Peters Á≠â - 2018 - Dissecting Contextual Word Embeddings Architectur.pdf}
}

@inproceedings{peters2018a,
  title = {Deep {{Contextualized Word Representations}}},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of           the {{Association}} for {{Computational Linguistics}}: {{Human Language}}           {{Technologies}}, {{Volume}} 1 ({{Long Papers}})},
  author = {Peters, Matthew and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  date = {2018},
  pages = {2227--2237},
  publisher = {{Association for Computational Linguistics}},
  location = {{New Orleans, Louisiana}},
  doi = {10.18653/v1/N18-1202},
  url = {http://aclweb.org/anthology/N18-1202},
  urldate = {2023-03-07},
  abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
  eventtitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of           the {{Association}} for {{Computational Linguistics}}: {{Human Language}}           {{Technologies}}, {{Volume}} 1 ({{Long Papers}})},
  langid = {english},
  file = {D:\Zotero\literature_data\storage\FRUZSAE4\Peters Á≠â - 2018 - Deep Contextualized Word Representations.pdf}
}

@online{peters2018b,
  title = {Deep Contextualized Word Representations},
  author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  date = {2018-03-22},
  eprint = {1802.05365},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1802.05365},
  url = {http://arxiv.org/abs/1802.05365},
  urldate = {2023-05-16},
  abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {D:\Zotero\literature_data\storage\XWAS8ZA7\1802.html}
}

@online{petroni2022,
  title = {Improving {{Wikipedia Verifiability}} with {{AI}}},
  author = {Petroni, Fabio and Broscheit, Samuel and Piktus, Aleksandra and Lewis, Patrick and Izacard, Gautier and Hosseini, Lucas and Dwivedi-Yu, Jane and Lomeli, Maria and Schick, Timo and Mazar√©, Pierre-Emmanuel and Joulin, Armand and Grave, Edouard and Riedel, Sebastian},
  date = {2022-07-08},
  eprint = {2207.06220},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2207.06220},
  urldate = {2023-11-29},
  abstract = {Verifiability is a core content policy of Wikipedia: claims that are likely to be challenged need to be backed by citations. There are millions of articles available online and thousands of new articles are released each month. For this reason, finding relevant sources is a difficult task: many claims do not have any references that support them. Furthermore, even existing citations might not support a given claim or become obsolete once the original source is updated or deleted. Hence, maintaining and improving the quality of Wikipedia references is an important challenge and there is a pressing need for better tools to assist humans in this effort. Here, we show that the process of improving references can be tackled with the help of artificial intelligence (AI). We develop a neural network based system, called Side, to identify Wikipedia citations that are unlikely to support their claims, and subsequently recommend better ones from the web. We train this model on existing Wikipedia references, therefore learning from the contributions and combined wisdom of thousands of Wikipedia editors. Using crowd-sourcing, we observe that for the top 10\% most likely citations to be tagged as unverifiable by our system, humans prefer our system's suggested alternatives compared to the originally cited reference 70\% of the time. To validate the applicability of our system, we built a demo to engage with the English-speaking Wikipedia community and find that Side's first citation recommendation collects over 60\% more preferences than existing Wikipedia citations for the same top 10\% most likely unverifiable claims according to Side. Our results indicate that an AI-based system could be used, in tandem with humans, to improve the verifiability of Wikipedia. More generally, we hope that our work can be used to assist fact checking efforts and increase the general trustworthiness of information online.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Retrieval},
  file = {D\:\\Zotero\\literature_data\\storage\\96QDXDCI\\Petroni Á≠â - 2022 - Improving Wikipedia Verifiability with AI.pdf;D\:\\Zotero\\literature_data\\storage\\RZNC867V\\2207.html}
}

@online{petsiuk2022,
  title = {Human {{Evaluation}} of {{Text-to-Image Models}} on a {{Multi-Task Benchmark}}},
  author = {Petsiuk, Vitali and Siemenn, Alexander E. and Surbehera, Saisamrit and Chin, Zad and Tyser, Keith and Hunter, Gregory and Raghavan, Arvind and Hicke, Yann and Plummer, Bryan A. and Kerret, Ori and Buonassisi, Tonio and Saenko, Kate and Solar-Lezama, Armando and Drori, Iddo},
  date = {2022-11-22},
  eprint = {2211.12112},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2211.12112},
  urldate = {2023-06-24},
  abstract = {We provide a new multi-task benchmark for evaluating text-to-image models. We perform a human evaluation comparing the most common open-source (Stable Diffusion) and commercial (DALL-E 2) models. Twenty computer science AI graduate students evaluated the two models, on three tasks, at three difficulty levels, across ten prompts each, providing 3,600 ratings. Text-to-image generation has seen rapid progress to the point that many recent models have demonstrated their ability to create realistic high-resolution images for various prompts. However, current text-to-image methods and the broader body of research in vision-language understanding still struggle with intricate text prompts that contain many objects with multiple attributes and relationships. We introduce a new text-to-image benchmark that contains a suite of thirty-two tasks over multiple applications that capture a model‚Äôs ability to handle different features of a text prompt. For example, asking a model to generate a varying number of the same object to measure its ability to count or providing a text prompt with several objects that each have a different attribute to identify its ability to match objects and attributes correctly. Rather than subjectively evaluating text-to-image results on a set of prompts, our new multi-task benchmark consists of challenge tasks at three difficulty levels (easy, medium, and hard) and human ratings for each generated image.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D:\Zotero\literature_data\storage\4ZGFNCP4\Petsiuk Á≠â - 2022 - Human Evaluation of Text-to-Image Models on a Mult.pdf}
}

@online{qin2023,
  title = {{{WebCPM}}: {{Interactive Web Search}} for {{Chinese Long-form Question Answering}}},
  shorttitle = {{{WebCPM}}},
  author = {Qin, Yujia and Cai, Zihan and Jin, Dian and Yan, Lan and Liang, Shihao and Zhu, Kunlun and Lin, Yankai and Han, Xu and Ding, Ning and Wang, Huadong and Xie, Ruobing and Qi, Fanchao and Liu, Zhiyuan and Sun, Maosong and Zhou, Jie},
  date = {2023-05-23},
  eprint = {2305.06849},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2305.06849},
  urldate = {2023-11-29},
  abstract = {Long-form question answering (LFQA) aims at answering complex, open-ended questions with detailed, paragraph-length responses. The de facto paradigm of LFQA necessitates two procedures: information retrieval, which searches for relevant supporting facts, and information synthesis, which integrates these facts into a coherent answer. In this paper, we introduce WebCPM, the first Chinese LFQA dataset. One unique feature of WebCPM is that its information retrieval is based on interactive web search, which engages with a search engine in real time. Following WebGPT, we develop a web search interface. We recruit annotators to search for relevant information using our interface and then answer questions. Meanwhile, the web search behaviors of our annotators would be recorded. In total, we collect 5,500 high-quality question-answer pairs, together with 14,315 supporting facts and 121,330 web search actions. We fine-tune pre-trained language models to imitate human behaviors for web search and to generate answers based on the collected facts. Our LFQA pipeline, built on these fine-tuned models, generates answers that are no worse than human-written ones in 32.5\% and 47.5\% of the cases on our dataset and DuReader, respectively.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {D\:\\Zotero\\literature_data\\storage\\B5BYDMRG\\Qin Á≠â - 2023 - WebCPM Interactive Web Search for Chinese Long-fo.pdf;D\:\\Zotero\\literature_data\\storage\\DHEKPDE5\\2305.html}
}

@inproceedings{qiu2020,
  title = {{{GCC}}: {{Graph Contrastive Coding}} for {{Graph Neural Network Pre-Training}}},
  shorttitle = {{{GCC}}},
  booktitle = {Proceedings of the 26th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Qiu, Jiezhong and Chen, Qibin and Dong, Yuxiao and Zhang, Jing and Yang, Hongxia and Ding, Ming and Wang, Kuansan and Tang, Jie},
  date = {2020-08-23},
  eprint = {2006.09963},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  pages = {1150--1160},
  doi = {10.1145/3394486.3403168},
  url = {http://arxiv.org/abs/2006.09963},
  urldate = {2023-07-25},
  abstract = {Graph representation learning has emerged as a powerful technique for addressing real-world problems. Various downstream graph learning tasks have benefited from its recent developments, such as node classification, similarity search, and graph classification. However, prior arts on graph representation learning focus on domain specific problems and train a dedicated model for each graph dataset, which is usually non-transferable to out-of-domain data. Inspired by the recent advances in pre-training from natural language processing and computer vision, we design Graph Contrastive Coding (GCC) -- a self-supervised graph neural network pre-training framework -- to capture the universal network topological properties across multiple networks. We design GCC's pre-training task as subgraph instance discrimination in and across networks and leverage contrastive learning to empower graph neural networks to learn the intrinsic and transferable structural representations. We conduct extensive experiments on three graph learning tasks and ten graph datasets. The results show that GCC pre-trained on a collection of diverse datasets can achieve competitive or better performance to its task-specific and trained-from-scratch counterparts. This suggests that the pre-training and fine-tuning paradigm presents great potential for graph representation learning.},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\ETB4M46P\\Qiu Á≠â - 2020 - GCC Graph Contrastive Coding for Graph Neural Net.pdf;D\:\\Zotero\\literature_data\\storage\\U9YAIWX3\\2006.html}
}

@inproceedings{qiu2020a,
  title = {{{GCC}}: {{Graph Contrastive Coding}} for {{Graph Neural Network Pre-Training}}},
  shorttitle = {{{GCC}}},
  booktitle = {Proceedings of the 26th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Qiu, Jiezhong and Chen, Qibin and Dong, Yuxiao and Zhang, Jing and Yang, Hongxia and Ding, Ming and Wang, Kuansan and Tang, Jie},
  date = {2020-08-20},
  series = {{{KDD}} '20},
  pages = {1150--1160},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3394486.3403168},
  url = {https://dl.acm.org/doi/10.1145/3394486.3403168},
  urldate = {2023-05-06},
  abstract = {Graph representation learning has emerged as a powerful technique for addressing real-world problems. Various downstream graph learning tasks have benefited from its recent developments, such as node classification, similarity search, and graph classification. However, prior arts on graph representation learning focus on domain specific problems and train a dedicated model for each graph dataset, which is usually non-transferable to out-of-domain data. Inspired by the recent advances in pre-training from natural language processing and computer vision, we design Graph Contrastive Coding (GCC) --- a self-supervised graph neural network pre-training framework --- to capture the universal network topological properties across multiple networks. We design GCC's pre-training task as subgraph instance discrimination in and across networks and leverage contrastive learning to empower graph neural networks to learn the intrinsic and transferable structural representations. We conduct extensive experiments on three graph learning tasks and ten graph datasets. The results show that GCC pre-trained on a collection of diverse datasets can achieve competitive or better performance to its task-specific and trained-from-scratch counterparts. This suggests that the pre-training and fine-tuning paradigm presents great potential for graph representation learning.},
  isbn = {978-1-4503-7998-4},
  keywords = {ego-network,graph neural network,graph representation learning,pre-training},
  file = {D:\Zotero\literature_data\storage\26JH9AHH\Qiu Á≠â - 2020 - GCC Graph Contrastive Coding for Graph Neural Net.pdf}
}

@article{radford,
  title = {Language {{Models}} Are {{Unsupervised Multitask Learners}}},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  langid = {english},
  keywords = {GPT-2},
  file = {D:\Zotero\literature_data\storage\H2H6U939\Radford Á≠â - Language Models are Unsupervised Multitask Learner.pdf}
}

@online{radford2021,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  date = {2021-02-26},
  eprint = {2103.00020},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2103.00020},
  urldate = {2023-04-22},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D:\Zotero\literature_data\storage\2XF399D9\Radford Á≠â - 2021 - Learning Transferable Visual Models From Natural L.pdf}
}

@article{radforda,
  title = {Improving {{Language Understanding}} by {{Generative Pre-Training}}},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
  langid = {english},
  file = {D:\Zotero\literature_data\storage\H2QTK9CY\Radford Á≠â - Improving Language Understanding by Generative Pre.pdf}
}

@article{radfordb,
  title = {Improving {{Language Understanding}} by {{Generative Pre-Training}}},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
  langid = {english},
  keywords = {GPT},
  file = {D:\Zotero\literature_data\storage\GJ8RCGN5\Radford Á≠â - Improving Language Understanding by Generative Pre.pdf}
}

@article{radfordc,
  title = {Improving {{Language Understanding}} by {{Generative Pre-Training}}},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
  langid = {english},
  keywords = {related work},
  file = {D:\Zotero\literature_data\storage\LK2U69AV\Radford Á≠â - Improving Language Understanding by Generative Pre.pdf}
}

@online{raffel2020,
  title = {Exploring the {{Limits}} of {{Transfer Learning}} with a {{Unified Text-to-Text Transformer}}},
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  date = {2020-07-28},
  eprint = {1910.10683},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1910.10683},
  urldate = {2023-06-03},
  abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning,t5},
  file = {D\:\\Zotero\\literature_data\\storage\\YG6GGPNB\\Raffel Á≠â - 2020 - Exploring the Limits of Transfer Learning with a U.pdf;D\:\\Zotero\\literature_data\\storage\\ARJ374FE\\1910.html}
}

@online{rajbhandari2020,
  title = {{{ZeRO}}: {{Memory Optimizations Toward Training Trillion Parameter Models}}},
  shorttitle = {{{ZeRO}}},
  author = {Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  date = {2020-05-13},
  eprint = {1910.02054},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1910.02054},
  url = {http://arxiv.org/abs/1910.02054},
  urldate = {2023-06-03},
  abstract = {Large deep learning models offer significant accuracy gains, but training billions to trillions of parameters is challenging. Existing solutions such as data and model parallelisms exhibit fundamental limitations to fit these models into limited device memory, while obtaining computation, communication and development efficiency. We develop a novel solution, Zero Redundancy Optimizer (ZeRO), to optimize memory, vastly improving training speed while increasing the model size that can be efficiently trained. ZeRO eliminates memory redundancies in data- and model-parallel training while retaining low communication volume and high computational granularity, allowing us to scale the model size proportional to the number of devices with sustained high efficiency. Our analysis on memory requirements and communication volume demonstrates: ZeRO has the potential to scale beyond 1 Trillion parameters using today's hardware. We implement and evaluate ZeRO: it trains large models of over 100B parameter with super-linear speedup on 400 GPUs, achieving throughput of 15 Petaflops. This represents an 8x increase in model size and 10x increase in achievable performance over state-of-the-art. In terms of usability, ZeRO can train large models of up to 13B parameters (e.g., larger than Megatron GPT 8.3B and T5 11B) without requiring model parallelism which is harder for scientists to apply. Last but not the least, researchers have used the system breakthroughs of ZeRO to create the world's largest language model (Turing-NLG, 17B parameters) with record breaking accuracy.},
  pubstate = {preprint},
  keywords = {{Computer Science - Distributed, Parallel, and Cluster Computing},Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\NRHWIHEZ\\Rajbhandari Á≠â - 2020 - ZeRO Memory Optimizations Toward Training Trillio.pdf;D\:\\Zotero\\literature_data\\storage\\VFKDVNUC\\1910.html}
}

@online{rajpurkar2016,
  title = {{{SQuAD}}: 100,000+ {{Questions}} for {{Machine Comprehension}} of {{Text}}},
  shorttitle = {{{SQuAD}}},
  author = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  date = {2016-10-10},
  eprint = {1606.05250},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1606.05250},
  url = {http://arxiv.org/abs/1606.05250},
  urldate = {2023-05-16},
  abstract = {We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0\%, a significant improvement over a simple baseline (20\%). However, human performance (86.8\%) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at https://stanford-qa.com},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\Z8MDGN99\\Rajpurkar Á≠â - 2016 - SQuAD 100,000+ Questions for Machine Comprehensio.pdf;D\:\\Zotero\\literature_data\\storage\\V4MC93G8\\1606.html}
}

@online{rashkin2022,
  title = {Measuring {{Attribution}} in {{Natural Language Generation Models}}},
  author = {Rashkin, Hannah and Nikolaev, Vitaly and Lamm, Matthew and Aroyo, Lora and Collins, Michael and Das, Dipanjan and Petrov, Slav and Tomar, Gaurav Singh and Turc, Iulia and Reitter, David},
  date = {2022-08-02},
  eprint = {2112.12870},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2112.12870},
  urldate = {2023-11-29},
  abstract = {With recent improvements in natural language generation (NLG) models for various applications, it has become imperative to have the means to identify and evaluate whether NLG output is only sharing verifiable information about the external world. In this work, we present a new evaluation framework entitled Attributable to Identified Sources (AIS) for assessing the output of natural language generation models, when such output pertains to the external world. We first define AIS and introduce a two-stage annotation pipeline for allowing annotators to appropriately evaluate model output according to AIS guidelines. We empirically validate this approach on generation datasets spanning three tasks (two conversational QA datasets, a summarization dataset, and a table-to-text dataset) via human evaluation studies that suggest that AIS could serve as a common framework for measuring whether model-generated statements are supported by underlying sources. We release guidelines for the human evaluation studies.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\HY8GMX94\\Rashkin Á≠â - 2022 - Measuring Attribution in Natural Language Generati.pdf;D\:\\Zotero\\literature_data\\storage\\XTZYKJHT\\2112.html}
}

@online{rashkin2022a,
  title = {Measuring {{Attribution}} in {{Natural Language Generation Models}}},
  author = {Rashkin, Hannah and Nikolaev, Vitaly and Lamm, Matthew and Aroyo, Lora and Collins, Michael and Das, Dipanjan and Petrov, Slav and Tomar, Gaurav Singh and Turc, Iulia and Reitter, David},
  date = {2022-08-02},
  eprint = {2112.12870},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2112.12870},
  urldate = {2023-11-14},
  abstract = {With recent improvements in natural language generation (NLG) models for various applications, it has become imperative to have the means to identify and evaluate whether NLG output is only sharing verifiable information about the external world. In this work, we present a new evaluation framework entitled Attributable to Identified Sources (AIS) for assessing the output of natural language generation models, when such output pertains to the external world. We first define AIS and introduce a two-stage annotation pipeline for allowing annotators to appropriately evaluate model output according to AIS guidelines. We empirically validate this approach on generation datasets spanning three tasks (two conversational QA datasets, a summarization dataset, and a table-to-text dataset) via human evaluation studies that suggest that AIS could serve as a common framework for measuring whether model-generated statements are supported by underlying sources. We release guidelines for the human evaluation studies.},
  pubstate = {preprint},
  keywords = {AIS,Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\MYJMHY38\\Rashkin Á≠â - 2022 - Measuring Attribution in Natural Language Generati.pdf;D\:\\Zotero\\literature_data\\storage\\9RUEMKVU\\2112.html}
}

@article{rashkin2023,
  title = {Measuring {{Attribution}} in {{Natural Language Generation Models}}},
  author = {Rashkin, Hannah and Nikolaev, Vitaly and Lamm, Matthew and Aroyo, Lora and Collins, Michael and Das, Dipanjan and Petrov, Slav and Tomar, Gaurav Singh and Turc, Iulia and Reitter, David},
  date = {2023-08-28},
  journaltitle = {Computational Linguistics},
  shortjournal = {Computational Linguistics},
  pages = {1--64},
  issn = {0891-2017},
  doi = {10.1162/coli_a_00486},
  url = {https://doi.org/10.1162/coli_a_00486},
  urldate = {2023-11-01},
  abstract = {Large neural models have brought a new challenge to natural language generation (NLG): It has become imperative to ensure the safety and reliability of the output of models that generate freely. To this end, we present an evaluation framework, Attributable to Identified Sources (AIS), stipulating that NLG output pertaining to the external world is to be verified against an independent, provided source. We define AIS and a two-stage annotation pipeline for allowing annotators to evaluate model output according to annotation guidelines. We successfully validate this approach on generation datasets spanning three tasks (two conversational QA datasets, a summarization dataset, and a table-to-text dataset). We provide full annotation guidelines in the appendices and publicly release the annotated data at https://github.com/google-research-datasets/AIS.},
  file = {D:\Zotero\literature_data\storage\DUWQNUR3\Rashkin Á≠â - 2023 - Measuring Attribution in Natural Language Generati.pdf}
}

@online{sang2003,
  title = {Introduction to the {{CoNLL-2003 Shared Task}}: {{Language-Independent Named Entity Recognition}}},
  shorttitle = {Introduction to the {{CoNLL-2003 Shared Task}}},
  author = {Sang, Erik F. Tjong Kim and De Meulder, Fien},
  date = {2003-06-12},
  eprint = {cs/0306050},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.cs/0306050},
  url = {http://arxiv.org/abs/cs/0306050},
  urldate = {2023-05-16},
  abstract = {We describe the CoNLL-2003 shared task: language-independent named entity recognition. We give background information on the data sets (English and German) and the evaluation method, present a general overview of the systems that have taken part in the task and discuss their performance.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,I.2.7},
  file = {D\:\\Zotero\\literature_data\\storage\\IADDR3YE\\Sang Âíå De Meulder - 2003 - Introduction to the CoNLL-2003 Shared Task Langua.pdf;D\:\\Zotero\\literature_data\\storage\\N2IXUF5C\\0306050.html}
}

@online{sanh2022,
  title = {Multitask {{Prompted Training Enables Zero-Shot Task Generalization}}},
  author = {Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H. and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Scao, Teven Le and Raja, Arun and Dey, Manan and Bari, M. Saiful and Xu, Canwen and Thakker, Urmish and Sharma, Shanya Sharma and Szczechla, Eliza and Kim, Taewoon and Chhablani, Gunjan and Nayak, Nihal and Datta, Debajyoti and Chang, Jonathan and Jiang, Mike Tian-Jian and Wang, Han and Manica, Matteo and Shen, Sheng and Yong, Zheng Xin and Pandey, Harshit and Bawden, Rachel and Wang, Thomas and Neeraj, Trishala and Rozen, Jos and Sharma, Abheesht and Santilli, Andrea and Fevry, Thibault and Fries, Jason Alan and Teehan, Ryan and Bers, Tali and Biderman, Stella and Gao, Leo and Wolf, Thomas and Rush, Alexander M.},
  date = {2022-03-17},
  eprint = {2110.08207},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2110.08207},
  url = {http://arxiv.org/abs/2110.08207},
  urldate = {2023-06-03},
  abstract = {Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al., 2020). It has been hypothesized that this is a consequence of implicit multitask learning in language models' pretraining (Radford et al., 2019). Can zero-shot generalization instead be directly induced by explicit multitask learning? To test this question at scale, we develop a system for easily mapping any natural language tasks into a human-readable prompted form. We convert a large set of supervised datasets, each with multiple prompts with diverse wording. These prompted datasets allow for benchmarking the ability of a model to perform completely held-out tasks. We fine-tune a pretrained encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance on several standard datasets, often outperforming models up to 16x its size. Further, our approach attains strong performance on a subset of tasks from the BIG-bench benchmark, outperforming models up to 6x its size. All trained models are available at https://github.com/bigscience-workshop/t-zero and all prompts are available at https://github.com/bigscience-workshop/promptsource.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,T0},
  file = {D\:\\Zotero\\literature_data\\storage\\8T5R27GD\\Sanh Á≠â - 2022 - Multitask Prompted Training Enables Zero-Shot Task.pdf;D\:\\Zotero\\literature_data\\storage\\62X9ZI83\\2110.html}
}

@article{schuhmann,
  title = {{{LAION-5B}}: {{An}} Open Large-Scale Dataset for Training next Generation Image-Text Models},
  author = {Schuhmann, Christoph and Beaumont, Romain and Vencu, Richard and Gordon, Cade and Wightman, Ross and Cherti, Mehdi and Coombes, Theo and Katta, Aarush and Mullis, Clayton and Wortsman, Mitchell and Schramowski, Patrick and Kundurthy, Srivatsa and Crowson, Katherine and Schmidt, Ludwig and Kaczmarczyk, Robert and Jitsev, Jenia},
  langid = {english},
  file = {D:\Zotero\literature_data\storage\S8LI8QHH\Schuhmann Á≠â - LAION-5B An open large-scale dataset for training.pdf}
}

@inproceedings{she2021,
  title = {Hierarchical {{Layout-Aware Graph Convolutional Network}} for {{Unified Aesthetics Assessment}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {She, Dongyu and Lai, Yu-Kun and Yi, Gaoxiong and Xu, Kun},
  date = {2021-06},
  pages = {8471--8480},
  publisher = {{IEEE}},
  location = {{Nashville, TN, USA}},
  doi = {10.1109/CVPR46437.2021.00837},
  url = {https://ieeexplore.ieee.org/document/9577542/},
  urldate = {2023-06-24},
  abstract = {Learning computational models of image aesthetics can have a substantial impact on visual art and graphic design. Although automatic image aesthetics assessment is a challenging topic by its subjective nature, psychological studies have confirmed a strong correlation between image layouts and perceived image quality. While previous state-of-the-art methods attempt to learn holistic information using deep Convolutional Neural Networks (CNNs), our approach is motivated by the fact that Graph Convolutional Network (GCN) architecture is conceivably more suited for modeling complex relations among image regions than vanilla convolutional layers. Specifically, we present a Hierarchical Layout-Aware Graph Convolutional Network (HLA-GCN) to capture layout information. It is a dedicated double-subnet neural network consisting of two LAGCN modules. The first LA-GCN module constructs an aesthetics-related graph in the coordinate space and performs reasoning over spatial nodes. The second LA-GCN module performs graph reasoning after aggregating significant regions in a latent space. The model output is a hierarchical representation with layout-aware features from both spatial and aggregated nodes for unified aesthetics assessment. Extensive evaluations show that our proposed model outperforms the state-of-the-art on the AVA and AADB datasets across three different tasks. The code is available at http://github.com/days1011/HLAGCN .},
  eventtitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-66544-509-2},
  langid = {english},
  file = {D:\Zotero\literature_data\storage\DTUB6BHW\She Á≠â - 2021 - Hierarchical Layout-Aware Graph Convolutional Netw.pdf}
}

@article{sheikh2006,
  title = {A {{Statistical Evaluation}} of {{Recent Full Reference Image Quality Assessment Algorithms}}},
  author = {Sheikh, H.R. and Sabir, M.F. and Bovik, A.C.},
  date = {2006-11},
  journaltitle = {IEEE Transactions on Image Processing},
  volume = {15},
  number = {11},
  pages = {3440--3451},
  issn = {1941-0042},
  doi = {10.1109/TIP.2006.881959},
  abstract = {Measurement of visual quality is of fundamental importance for numerous image and video processing applications, where the goal of quality assessment (QA) algorithms is to automatically assess the quality of images or videos in agreement with human quality judgments. Over the years, many researchers have taken different approaches to the problem and have contributed significant research in this area and claim to have made progress in their respective domains. It is important to evaluate the performance of these algorithms in a comparative setting and analyze the strengths and weaknesses of these methods. In this paper, we present results of an extensive subjective quality assessment study in which a total of 779 distorted images were evaluated by about two dozen human subjects. The "ground truth" image quality data obtained from about 25 000 individual human quality judgments is used to evaluate the performance of several prominent full-reference image quality assessment algorithms. To the best of our knowledge, apart from video quality studies conducted by the Video Quality Experts Group, the study presented in this paper is the largest subjective image quality study in the literature in terms of number of images, distortion types, and number of human judgments per image. Moreover, we have made the data from the study freely available to the research community . This would allow other researchers to easily report comparative results in the future},
  eventtitle = {{{IEEE Transactions}} on {{Image Processing}}},
  keywords = {Algorithm design and analysis,Humans,Image processing,Image quality,Image quality assessment performance,image quality study,Laboratories,Performance analysis,PSNR,Quality assessment,subjective quality assessment,Testing,Video compression},
  file = {D\:\\Zotero\\literature_data\\storage\\DLKC9KLA\\Sheikh Á≠â - 2006 - A Statistical Evaluation of Recent Full Reference .pdf;D\:\\Zotero\\literature_data\\storage\\S5W4SBF5\\1709988.html}
}

@online{shen2023,
  title = {Large {{Language Model Alignment}}: {{A Survey}}},
  shorttitle = {Large {{Language Model Alignment}}},
  author = {Shen, Tianhao and Jin, Renren and Huang, Yufei and Liu, Chuang and Dong, Weilong and Guo, Zishan and Wu, Xinwei and Liu, Yan and Xiong, Deyi},
  date = {2023-09-26},
  eprint = {2309.15025},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2309.15025},
  urldate = {2023-10-19},
  abstract = {Recent years have witnessed remarkable progress made in large language models (LLMs). Such advancements, while garnering significant attention, have concurrently elicited various concerns. The potential of these models is undeniably vast; however, they may yield texts that are imprecise, misleading, or even detrimental. Consequently, it becomes paramount to employ alignment techniques to ensure these models to exhibit behaviors consistent with human values. This survey endeavors to furnish an extensive exploration of alignment methodologies designed for LLMs, in conjunction with the extant capability research in this domain. Adopting the lens of AI alignment, we categorize the prevailing methods and emergent proposals for the alignment of LLMs into outer and inner alignment. We also probe into salient issues including the models' interpretability, and potential vulnerabilities to adversarial attacks. To assess LLM alignment, we present a wide variety of benchmarks and evaluation methodologies. After discussing the state of alignment research for LLMs, we finally cast a vision toward the future, contemplating the promising avenues of research that lie ahead. Our aspiration for this survey extends beyond merely spurring research interests in this realm. We also envision bridging the gap between the AI alignment research community and the researchers engrossed in the capability exploration of LLMs for both capable and safe LLMs.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,ÂØπÈΩê},
  file = {D\:\\Zotero\\literature_data\\storage\\H66TBP6J\\Shen Á≠â - 2023 - Large Language Model Alignment A Survey.pdf;D\:\\Zotero\\literature_data\\storage\\N9NP2XEN\\2309.html}
}

@online{shoeybi2020,
  title = {Megatron-{{LM}}: {{Training Multi-Billion Parameter Language Models Using Model Parallelism}}},
  shorttitle = {Megatron-{{LM}}},
  author = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  date = {2020-03-13},
  eprint = {1909.08053},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1909.08053},
  url = {http://arxiv.org/abs/1909.08053},
  urldate = {2023-06-03},
  abstract = {Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76\% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30\% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5\% compared to SOTA accuracy of 63.2\%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9\% compared to SOTA accuracy of 89.4\%).},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\F3ZWTSWK\\Shoeybi Á≠â - 2020 - Megatron-LM Training Multi-Billion Parameter Lang.pdf;D\:\\Zotero\\literature_data\\storage\\B6WQE7YL\\1909.html}
}

@article{sigaki2018,
  title = {History of Art Paintings through the Lens of Entropy and Complexity},
  author = {Sigaki, Higor Y. D. and Perc, Matjaz and Ribeiro, Haroldo V.},
  date = {2018-09-11},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {115},
  number = {37},
  eprint = {1809.05760},
  eprinttype = {arxiv},
  eprintclass = {physics, stat},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1800083115},
  url = {http://arxiv.org/abs/1809.05760},
  urldate = {2023-05-23},
  abstract = {Art is the ultimate expression of human creativity that is deeply influenced by the philosophy and culture of the corresponding historical epoch. The quantitative analysis of art is therefore essential for better understanding human cultural evolution. Here we present a large-scale quantitative analysis of almost 140 thousand paintings, spanning nearly a millennium of art history. Based on the local spatial patterns in the images of these paintings, we estimate the permutation entropy and the statistical complexity of each painting. These measures map the degree of visual order of artworks into a scale of order-disorder and simplicity-complexity that locally reflects qualitative categories proposed by art historians. The dynamical behavior of these measures reveals a clear temporal evolution of art, marked by transitions that agree with the main historical periods of art. Our research shows that different artistic styles have a distinct average degree of entropy and complexity, thus allowing a hierarchical organization and clustering of styles according to these metrics. We have further verified that the identified groups correspond well with the textual content used to qualitatively describe the styles, and that the employed complexity-entropy measures can be used for an effective classification of artworks.},
  langid = {english},
  keywords = {Physics - Physics and Society,Statistics - Applications},
  file = {D:\Zotero\literature_data\storage\KXTGGAXC\Sigaki Á≠â - 2018 - History of art paintings through the lens of entro.pdf}
}

@inproceedings{su2020,
  title = {Blindly {{Assess Image Quality}} in the {{Wild Guided}} by a {{Self-Adaptive Hyper Network}}},
  author = {Su, Shaolin and Yan, Qingsen and Zhu, Yu and Zhang, Cheng and Ge, Xin and Sun, Jinqiu and Zhang, Yanning},
  date = {2020},
  pages = {3667--3676},
  url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Su_Blindly_Assess_Image_Quality_in_the_Wild_Guided_by_a_CVPR_2020_paper.html},
  urldate = {2023-07-25},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {D:\Zotero\literature_data\storage\RDJ4XBNH\Su Á≠â - 2020 - Blindly Assess Image Quality in the Wild Guided by.pdf}
}

@online{sun2020,
  title = {{{InfoGraph}}: {{Unsupervised}} and {{Semi-supervised Graph-Level Representation Learning}} via {{Mutual Information Maximization}}},
  shorttitle = {{{InfoGraph}}},
  author = {Sun, Fan-Yun and Hoffmann, Jordan and Verma, Vikas and Tang, Jian},
  date = {2020-01-17},
  eprint = {1908.01000},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1908.01000},
  url = {http://arxiv.org/abs/1908.01000},
  urldate = {2023-05-21},
  abstract = {This paper studies learning the representations of whole graphs in both unsupervised and semi-supervised scenarios. Graph-level representations are critical in a variety of real-world applications such as predicting the properties of molecules and community analysis in social networks. Traditional graph kernel based methods are simple, yet effective for obtaining fixed-length representations for graphs but they suffer from poor generalization due to hand-crafted designs. There are also some recent methods based on language models (e.g. graph2vec) but they tend to only consider certain substructures (e.g. subtrees) as graph representatives. Inspired by recent progress of unsupervised representation learning, in this paper we proposed a novel method called InfoGraph for learning graph-level representations. We maximize the mutual information between the graph-level representation and the representations of substructures of different scales (e.g., nodes, edges, triangles). By doing so, the graph-level representations encode aspects of the data that are shared across different scales of substructures. Furthermore, we further propose InfoGraph*, an extension of InfoGraph for semi-supervised scenarios. InfoGraph* maximizes the mutual information between unsupervised graph representations learned by InfoGraph and the representations learned by existing supervised methods. As a result, the supervised encoder learns from unlabeled data while preserving the latent semantic space favored by the current supervised task. Experimental results on the tasks of graph classification and molecular property prediction show that InfoGraph is superior to state-of-the-art baselines and InfoGraph* can achieve performance competitive with state-of-the-art semi-supervised models.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\ESKZMAKM\\Sun Á≠â - 2020 - InfoGraph Unsupervised and Semi-supervised Graph-.pdf;D\:\\Zotero\\literature_data\\storage\\TMXTRRLL\\1908.html}
}

@online{sun2022,
  title = {Contrastive {{Learning Reduces Hallucination}} in {{Conversations}}},
  author = {Sun, Weiwei and Shi, Zhengliang and Gao, Shen and Ren, Pengjie and family=Rijke, given=Maarten, prefix=de, useprefix=true and Ren, Zhaochun},
  date = {2022-12-20},
  eprint = {2212.10400},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2212.10400},
  url = {http://arxiv.org/abs/2212.10400},
  urldate = {2023-11-29},
  abstract = {Pre-trained language models (LMs) store knowledge in their parameters and can generate informative responses when used in conversational systems. However, LMs suffer from the problem of "hallucination:" they may generate plausible-looking statements that are irrelevant or factually incorrect. To address this problem, we propose a contrastive learning scheme, named MixCL. A novel mixed contrastive objective is proposed to explicitly optimize the implicit knowledge elicitation process of LMs, and thus reduce their hallucination in conversations. We also examine negative sampling strategies of retrieved hard negatives and model-generated negatives. We conduct experiments on Wizard-of-Wikipedia, a public, open-domain knowledge-grounded dialogue benchmark, and assess the effectiveness of MixCL. MixCL effectively reduces the hallucination of LMs in conversations and achieves the highest performance among LM-based dialogue agents in terms of relevancy and factuality. We show that MixCL achieves comparable performance to state-of-the-art KB-based approaches while enjoying notable advantages in terms of efficiency and scalability.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\7W7KPF7P\\Sun Á≠â - 2022 - Contrastive Learning Reduces Hallucination in Conv.pdf;D\:\\Zotero\\literature_data\\storage\\INQPY9PP\\2212.html}
}

@online{tan2024,
  title = {Blinded by {{Generated Contexts}}: {{How Language Models Merge Generated}} and {{Retrieved Contexts}} for {{Open-Domain QA}}?},
  shorttitle = {Blinded by {{Generated Contexts}}},
  author = {Tan, Hexiang and Sun, Fei and Yang, Wanli and Wang, Yuanzhuo and Cao, Qi and Cheng, Xueqi},
  date = {2024-01-22},
  eprint = {2401.11911},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2401.11911},
  urldate = {2024-01-23},
  abstract = {While auxiliary information has become a key to enhance Large Language Models (LLMs), relatively little is known about how well LLMs merge these contexts, specifically generated and retrieved. To study this, we formulate a task specifically designed to identify whether the answers, derived from the integration of generated and retrieved contexts, are attributed to either generated or retrieved contexts. To support this task, we develop a methodology to construct datasets with conflicting contexts, where each question is paired with both generated and retrieved contexts, yet only one of them contains the correct answer. Our experiments reveal a significant bias in LLMs towards generated contexts, as evidenced across state-of-the-art open (Llama2-7b/13b) and closed (GPT 3.5/4) systems. We further identify two key factors contributing to this bias: i) Contexts generated by LLMs typically show greater similarity to the questions, increasing their likelihood of selection; ii) The segmentation process used in retrieved contexts disrupts their completeness, thereby hindering their full utilization in LLMs. Our analysis enhances the understanding of how LLMs merge diverse contexts, offering valuable insights for advancing current augmentation methods for LLMs.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@inproceedings{thakoor2021,
  title = {Bootstrapped {{Representation Learning}} on {{Graphs}}},
  author = {Thakoor, Shantanu and Tallec, Corentin and Azar, Mohammad Gheshlaghi and Munos, Remi and Veliƒçkoviƒá, Petar and Valko, Michal},
  date = {2021-03-08},
  url = {https://openreview.net/forum?id=QrzVRAA49Ud},
  urldate = {2023-07-06},
  abstract = {Current state-of-the-art self-supervised learning methods for graph neural networks are based on contrastive learning. As such, they heavily depend on the construction of augmentations and negative examples. Increasing the number of negative pairs improves performance, thereby requiring quadratic computation and memory cost to achieve peak performance. Inspired by BYOL, a recently introduced method for self-supervised learning that does not require negative pairs, we present Bootstrapped Graph Latents, BGRL, a self-supervised graph representation method that gets rid of this potentially quadratic bottleneck. BGRL outperforms or matches the previous unsupervised state-of-the-art results on several established benchmarks. Moreover, it enables the effective usage of graph attentional (GAT) encoders, allowing us to further improve the state of the art, in particular achieving 70.49\% Micro-F1 on the PPI dataset using the linear evaluation protocol.},
  eventtitle = {{{ICLR}} 2021 {{Workshop}} on {{Geometrical}} and {{Topological Representation Learning}}},
  langid = {english},
  file = {D:\Zotero\literature_data\storage\6YVP6X9A\Thakoor Á≠â - 2021 - Bootstrapped Representation Learning on Graphs.pdf}
}

@online{tian2020,
  title = {Contrastive {{Multiview Coding}}},
  author = {Tian, Yonglong and Krishnan, Dilip and Isola, Phillip},
  date = {2020-12-18},
  eprint = {1906.05849},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1906.05849},
  urldate = {2023-05-21},
  abstract = {Humans view the world through many sensory channels, e.g., the long-wavelength light channel, viewed by the left eye, or the high-frequency vibrations channel, heard by the right ear. Each view is noisy and incomplete, but important factors, such as physics, geometry, and semantics, tend to be shared between all views (e.g., a "dog" can be seen, heard, and felt). We investigate the classic hypothesis that a powerful representation is one that models view-invariant factors. We study this hypothesis under the framework of multiview contrastive learning, where we learn a representation that aims to maximize mutual information between different views of the same scene but is otherwise compact. Our approach scales to any number of views, and is view-agnostic. We analyze key properties of the approach that make it work, finding that the contrastive loss outperforms a popular alternative based on cross-view prediction, and that the more views we learn from, the better the resulting representation captures underlying scene semantics. Our approach achieves state-of-the-art results on image and video unsupervised learning benchmarks. Code is released at: http://github.com/HobbitLong/CMC/.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\I63FHXGA\\Tian Á≠â - 2020 - Contrastive Multiview Coding.pdf;D\:\\Zotero\\literature_data\\storage\\L82F7PR6\\1906.html}
}

@inproceedings{tian2020a,
  title = {Contrastive {{Multiview Coding}}},
  booktitle = {Computer {{Vision}} ‚Äì {{ECCV}} 2020},
  author = {Tian, Yonglong and Krishnan, Dilip and Isola, Phillip},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {776--794},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-58621-8_45},
  abstract = {Humans view the world through many sensory channels, e.g., the long-wavelength light channel, viewed by the left eye, or the high-frequency vibrations channel, heard by the right ear. Each view is noisy and incomplete, but important factors, such as physics, geometry, and semantics, tend to be shared between all views (e.g., a ‚Äúdog‚Äù can be seen, heard, and felt). We investigate the classic hypothesis that a powerful representation is one that models view-invariant factors. We study this hypothesis under the framework of multiview contrastive learning, where we learn a representation that aims to maximize mutual information between different views of the same scene but is otherwise compact. Our approach scales to any number of views, and is view-agnostic. We analyze key properties of the approach that make it work, finding that the contrastive loss outperforms a popular alternative based on cross-view prediction, and that the more views we learn from, the better the resulting representation captures underlying scene semantics. Code is available at: http://github.com/HobbitLong/CMC/.},
  isbn = {978-3-030-58621-8},
  langid = {english},
  file = {D:\Zotero\literature_data\storage\57237T7G\Tian Á≠â - 2020 - Contrastive Multiview Coding.pdf}
}

@online{touvron2023,
  title = {{{LLaMA}}: {{Open}} and {{Efficient Foundation Language Models}}},
  shorttitle = {{{LLaMA}}},
  author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth√©e and Rozi√®re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
  date = {2023-02-27},
  eprint = {2302.13971},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2302.13971},
  url = {http://arxiv.org/abs/2302.13971},
  urldate = {2023-06-03},
  abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\TI83NA6C\\Touvron Á≠â - 2023 - LLaMA Open and Efficient Foundation Language Mode.pdf;D\:\\Zotero\\literature_data\\storage\\5UTAF4H3\\2302.html}
}

@article{vasile,
  title = {{{PARTICULAR REGENERATIVE ASPECTS OF THE HPD}} 1001 {{GRAPEVINE VARIETY}}, {{CULTIVATED IN VITRO}}},
  author = {Vasile, Laslo and Maria, ZƒÉp√¢r≈£an and Simona, Vica≈ü},
  abstract = {The 1001 directly producing hybrid (HPD) grapevine variety was experimented upon, cultivated in vitro from apex, detached from shoots obtained after forcing the grapevine cords in growing chamber conditions. Two basic mediums were used: MS (after Murashige-Skoog) and G (after Gamborg). On these two environments, the apex showed regeneration and differentiation of 1-2 un-rooted plantlets. Two variants with the following compositions were experimented with afterwards: HPDV1 = G +0.5 mg/l TDZ (tidiazuron) +0.5 AIB (Œ≤ indolil acetic acid); and HPDV2 = G+0.5mg/l TDZ + 1.0mg/l AIB. 50 days after the culturing of the apex on these mediums, the tidiazuron proved its effect on the differentiation of the callus on grapevine variety 1001. In combination with a small dose of auxine ((HPDV1), a mass of callus is obtained, containing antocyans, friable, lacking plant regeneration ability. On the variant with a double dose of AIB (HPDV2), callus is formed on a high percentage of apexes (over 50\%), proves to be regenerative, differentiating neo-plantlets. We believe the mix of auxines in higher concentration with a smaller dose of tidiazuron (substance considered to be in the cytochinine class), favors the formation of embryoids on the callus and the differentiation of plantlets. We also believe that the inhibition of the differentiation of roots (even in the presence of an auxine) is due to the powerful cytochemical effect of the tidiazuron.},
  langid = {english},
  file = {D:\Zotero\literature_data\storage\SXU54GME\Vasile Á≠â - PARTICULAR REGENERATIVE ASPECTS OF THE HPD 1001 GR.pdf}
}

@online{vaswani2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2017-12-05},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2023-05-16},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\NNSXC63X\\Vaswani Á≠â - 2017 - Attention Is All You Need.pdf;D\:\\Zotero\\literature_data\\storage\\M7F6T44T\\1706.html}
}

@online{velickovic2018,
  title = {Graph {{Attention Networks}}},
  author = {Veliƒçkoviƒá, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li√≤, Pietro and Bengio, Yoshua},
  date = {2018-02-04},
  eprint = {1710.10903},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1710.10903},
  url = {http://arxiv.org/abs/1710.10903},
  urldate = {2023-04-26},
  abstract = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\2FVGHSCN\\Veliƒçkoviƒá Á≠â - 2018 - Graph Attention Networks.pdf;D\:\\Zotero\\literature_data\\storage\\VDQPCWS3\\1710.html}
}

@online{vu2023,
  title = {{{FreshLLMs}}: {{Refreshing Large Language Models}} with {{Search Engine Augmentation}}},
  shorttitle = {{{FreshLLMs}}},
  author = {Vu, Tu and Iyyer, Mohit and Wang, Xuezhi and Constant, Noah and Wei, Jerry and Wei, Jason and Tar, Chris and Sung, Yun-Hsuan and Zhou, Denny and Le, Quoc and Luong, Thang},
  date = {2023-11-22},
  eprint = {2310.03214},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2310.03214},
  urldate = {2023-12-06},
  abstract = {Most large language models (LLMs) are trained once and never updated; thus, they lack the ability to dynamically adapt to our ever-changing world. In this work, we perform a detailed study of the factuality of LLM-generated text in the context of answering questions that test current world knowledge. Specifically, we introduce FreshQA, a novel dynamic QA benchmark encompassing a diverse range of question and answer types, including questions that require fast-changing world knowledge as well as questions with false premises that need to be debunked. We benchmark a diverse array of both closed and open-source LLMs under a two-mode evaluation procedure that allows us to measure both correctness and hallucination. Through human evaluations involving more than 50K judgments, we shed light on limitations of these models and demonstrate significant room for improvement: for instance, all models (regardless of model size) struggle on questions that involve fast-changing knowledge and false premises. Motivated by these results, we present FreshPrompt, a simple few-shot prompting method that substantially boosts the performance of an LLM on FreshQA by incorporating relevant and up-to-date information retrieved from a search engine into the prompt. Our experiments show that FreshPrompt outperforms both competing search engine-augmented prompting methods such as Self-Ask (Press et al., 2022) as well as commercial systems such as Perplexity.AI. Further analysis of FreshPrompt reveals that both the number of retrieved evidences and their order play a key role in influencing the correctness of LLM-generated answers. Additionally, instructing the LLM to generate concise and direct answers helps reduce hallucination compared to encouraging more verbose answers. To facilitate future work, we release FreshQA at github.com/freshllms/freshqa and commit to updating it at regular intervals.},
  pubstate = {preprint},
  version = {2},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\SY3A3EVQ\\Vu Á≠â - 2023 - FreshLLMs Refreshing Large Language Models with S.pdf;D\:\\Zotero\\literature_data\\storage\\AFWFN6A7\\2310.html}
}

@online{vu2023a,
  title = {{{FreshLLMs}}: {{Refreshing Large Language Models}} with {{Search Engine Augmentation}}},
  shorttitle = {{{FreshLLMs}}},
  author = {Vu, Tu and Iyyer, Mohit and Wang, Xuezhi and Constant, Noah and Wei, Jerry and Wei, Jason and Tar, Chris and Sung, Yun-Hsuan and Zhou, Denny and Le, Quoc and Luong, Thang},
  date = {2023-11-22},
  eprint = {2310.03214},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2310.03214},
  urldate = {2023-12-01},
  abstract = {Most large language models (LLMs) are trained once and never updated; thus, they lack the ability to dynamically adapt to our ever-changing world. In this work, we perform a detailed study of the factuality of LLM-generated text in the context of answering questions that test current world knowledge. Specifically, we introduce FreshQA, a novel dynamic QA benchmark encompassing a diverse range of question and answer types, including questions that require fast-changing world knowledge as well as questions with false premises that need to be debunked. We benchmark a diverse array of both closed and open-source LLMs under a two-mode evaluation procedure that allows us to measure both correctness and hallucination. Through human evaluations involving more than 50K judgments, we shed light on limitations of these models and demonstrate significant room for improvement: for instance, all models (regardless of model size) struggle on questions that involve fast-changing knowledge and false premises. Motivated by these results, we present FreshPrompt, a simple few-shot prompting method that substantially boosts the performance of an LLM on FreshQA by incorporating relevant and up-to-date information retrieved from a search engine into the prompt. Our experiments show that FreshPrompt outperforms both competing search engine-augmented prompting methods such as Self-Ask (Press et al., 2022) as well as commercial systems such as Perplexity.AI. Further analysis of FreshPrompt reveals that both the number of retrieved evidences and their order play a key role in influencing the correctness of LLM-generated answers. Additionally, instructing the LLM to generate concise and direct answers helps reduce hallucination compared to encouraging more verbose answers. To facilitate future work, we release FreshQA at github.com/freshllms/freshqa and commit to updating it at regular intervals.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\CE2Z4SYC\\Vu Á≠â - 2023 - FreshLLMs Refreshing Large Language Models with S.pdf;D\:\\Zotero\\literature_data\\storage\\KCP97J24\\2310.html}
}

@online{wallat2024,
  title = {Temporal {{Blind Spots}} in {{Large Language Models}}},
  author = {Wallat, Jonas and Jatowt, Adam and Anand, Avishek},
  date = {2024-01-22},
  eprint = {2401.12078},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2401.12078},
  urldate = {2024-01-23},
  abstract = {Large language models (LLMs) have recently gained significant attention due to their unparalleled ability to perform various natural language processing tasks. These models, benefiting from their advanced natural language understanding capabilities, have demonstrated impressive zero-shot performance. However, the pre-training data utilized in LLMs is often confined to a specific corpus, resulting in inherent freshness and temporal scope limitations. Consequently, this raises concerns regarding the effectiveness of LLMs for tasks involving temporal intents. In this study, we aim to investigate the underlying limitations of general-purpose LLMs when deployed for tasks that require a temporal understanding. We pay particular attention to handling factual temporal knowledge through three popular temporal QA datasets. Specifically, we observe low performance on detailed questions about the past and, surprisingly, for rather new information. In manual and automatic testing, we find multiple temporal errors and characterize the conditions under which QA performance deteriorates. Our analysis contributes to understanding LLM limitations and offers valuable insights into developing future models that can better cater to the demands of temporally-oriented tasks. The code is available\textbackslash footnote\{https://github.com/jwallat/temporalblindspots\}.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language}
}

@article{wang,
  title = {Uncovering the {{Structural Fairness}} in {{Graph Contrastive Learning}}},
  author = {Wang, Ruijia and Wang, Xiao and Shi, Chuan and Song, Le},
  abstract = {Recent studies show that graph convolutional network (GCN) often performs worse for low-degree nodes, exhibiting the so-called structural unfairness for graphs with long-tailed degree distributions prevalent in the real world. Graph contrastive learning (GCL), which marries the power of GCN and contrastive learning, has emerged as a promising self-supervised approach for learning node representations. How does GCL behave in terms of structural fairness? Surprisingly, we find that representations obtained by GCL methods are already fairer to degree bias than those learned by GCN. We theoretically show that this fairness stems from intra-community concentration and inter-community scatter properties of GCL, resulting in a much clear community structure to drive low-degree nodes away from the community boundary. Based on our theoretical analysis, we further devise a novel graph augmentation method, called GRAph contrastive learning for DEgree bias (GRADE), which applies different strategies to low- and high-degree nodes. Extensive experiments on various benchmarks and evaluation protocols validate the effectiveness of the proposed method.},
  langid = {english},
  file = {D:\Zotero\literature_data\storage\Q9I9SBB3\Wang Á≠â - Uncovering the Structural Fairness in Graph Contra.pdf}
}

@online{wang2018,
  title = {{{GLUE}}: {{A Multi-Task Benchmark}} and {{Analysis Platform}} for {{Natural Language Understanding}}},
  shorttitle = {{{GLUE}}},
  author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
  date = {2018-04-20},
  url = {https://arxiv.org/abs/1804.07461v3},
  urldate = {2023-05-17},
  abstract = {For natural language understanding (NLU) technology to be maximally useful, both practically and as a scientific object of study, it must be general: it must be able to process language in a way that is not exclusively tailored to any one specific task or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation benchmark (GLUE), a tool for evaluating and analyzing the performance of models across a diverse range of existing NLU tasks. GLUE is model-agnostic, but it incentivizes sharing knowledge across tasks because certain tasks have very limited training data. We further provide a hand-crafted diagnostic test suite that enables detailed linguistic analysis of NLU models. We evaluate baselines based on current methods for multi-task and transfer learning and find that they do not immediately give substantial improvements over the aggregate performance of training a separate model per task, indicating room for improvement in developing general and robust NLU systems.},
  langid = {english},
  organization = {{arXiv.org}},
  file = {D:\Zotero\literature_data\storage\XCCH357A\Wang Á≠â - 2018 - GLUE A Multi-Task Benchmark and Analysis Platform.pdf}
}

@inproceedings{wang2021,
  title = {A {{Multi-dimensional Aesthetic Quality Assessment Model}} for {{Mobile Game Images}}},
  booktitle = {2021 {{International Conference}} on {{Visual Communications}} and {{Image Processing}} ({{VCIP}})},
  author = {Wang, Tao and Sun, Wei and Min, Xiongkuo and Lu, Wei and Zhang, Zicheng and Zhai, Guangtao},
  date = {2021-12-05},
  pages = {1--5},
  publisher = {{IEEE}},
  location = {{Munich, Germany}},
  doi = {10.1109/VCIP53242.2021.9675430},
  url = {https://ieeexplore.ieee.org/document/9675430/},
  urldate = {2023-08-02},
  abstract = {With the development of the game industry and the popularization of mobile devices, mobile games have played an important role in people‚Äôs entertainment life. The aesthetic quality of mobile game images determines the users‚Äô Quality of Experience (QoE) to a certain extent. In this paper, we propose a multi-task deep learning based method to evaluate the aesthetic quality of mobile game images in multiple dimensions (i.e. the fineness, color harmony, colorfulness, and overall quality). Specifically, we first extract the quality-aware feature representation through integrating the features from all intermediate layers of the convolution neural network (CNN) and then map these quality-aware features into the quality score space in each dimension via the quality regressor module, which consists of three fully connected (FC) layers. The proposed model is trained through a multi-task learning manner, where the quality-aware features are shared by different quality dimension prediction tasks, and the multi-dimensional quality scores of each image are regressed by multiple quality regression modules respectively. We further introduce an uncertainty principle to balance the loss of each task in the training stage. The experimental results show that our proposed model achieves the best performance on the Multi-dimensional Aesthetic assessment for Mobile Game image database (MAMG) among state-of-the-art image quality assessment (IQA) algorithms and aesthetic quality assessment (AQA) algorithms.},
  eventtitle = {2021 {{International Conference}} on {{Visual Communications}} and {{Image Processing}} ({{VCIP}})},
  isbn = {978-1-72818-551-4},
  langid = {english},
  keywords = {Ê∏∏ÊàèÂõæÁâáË¥®ÈáèËØÑ‰ª∑},
  file = {D:\Zotero\literature_data\storage\BX3797BM\Wang Á≠â - 2021 - A Multi-dimensional Aesthetic Quality Assessment M.pdf}
}

@article{wang2022,
  title = {Uncovering the {{Structural Fairness}} in {{Graph Contrastive Learning}}},
  author = {Wang, Ruijia and Wang, Xiao and Shi, Chuan and Song, Le},
  date = {2022-12-06},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {32465--32473},
  url = {https://proceedings.neurips.cc//paper_files/paper/2022/hash/d13565c82d1e44eda2da3bd00b35ca11-Abstract-Conference.html},
  urldate = {2023-05-06},
  langid = {english},
  keywords = {other GCL method},
  file = {D:\Zotero\literature_data\storage\UTA2CCB7\Wang Á≠â - 2022 - Uncovering the Structural Fairness in Graph Contra.pdf}
}

@online{wang2022a,
  title = {Uncovering the {{Structural Fairness}} in {{Graph Contrastive Learning}}},
  author = {Wang, Ruijia and Wang, Xiao and Shi, Chuan and Song, Le},
  date = {2022-10-06},
  eprint = {2210.03011},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2210.03011},
  urldate = {2023-05-26},
  abstract = {Recent studies show that graph convolutional network (GCN) often performs worse for low-degree nodes, exhibiting the so-called structural unfairness for graphs with long-tailed degree distributions prevalent in the real world. Graph contrastive learning (GCL), which marries the power of GCN and contrastive learning, has emerged as a promising self-supervised approach for learning node representations. How does GCL behave in terms of structural fairness? Surprisingly, we find that representations obtained by GCL methods are already fairer to degree bias than those learned by GCN. We theoretically show that this fairness stems from intra-community concentration and inter-community scatter properties of GCL, resulting in a much clear community structure to drive low-degree nodes away from the community boundary. Based on our theoretical analysis, we further devise a novel graph augmentation method, called GRAph contrastive learning for DEgree bias (GRADE), which applies different strategies to low- and high-degree nodes. Extensive experiments on various benchmarks and evaluation protocols validate the effectiveness of the proposed method.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks},
  file = {D\:\\Zotero\\literature_data\\storage\\73THJR65\\Wang Á≠â - 2022 - Uncovering the Structural Fairness in Graph Contra.pdf;D\:\\Zotero\\literature_data\\storage\\I82UPQCL\\2210.html}
}

@online{wang2022b,
  title = {Super-{{NaturalInstructions}}: {{Generalization}} via {{Declarative Instructions}} on 1600+ {{NLP Tasks}}},
  shorttitle = {Super-{{NaturalInstructions}}},
  author = {Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Arunkumar, Anjana and Ashok, Arjun and Dhanasekaran, Arut Selvan and Naik, Atharva and Stap, David and Pathak, Eshaan and Karamanolakis, Giannis and Lai, Haizhi Gary and Purohit, Ishan and Mondal, Ishani and Anderson, Jacob and Kuznia, Kirby and Doshi, Krima and Patel, Maitreya and Pal, Kuntal Kumar and Moradshahi, Mehrad and Parmar, Mihir and Purohit, Mirali and Varshney, Neeraj and Kaza, Phani Rohitha and Verma, Pulkit and Puri, Ravsehaj Singh and Karia, Rushang and Sampat, Shailaja Keyur and Doshi, Savan and Mishra, Siddhartha and Reddy, Sujan and Patro, Sumanta and Dixit, Tanay and Shen, Xudong and Baral, Chitta and Choi, Yejin and Smith, Noah A. and Hajishirzi, Hannaneh and Khashabi, Daniel},
  date = {2022-10-24},
  eprint = {2204.07705},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2204.07705},
  url = {http://arxiv.org/abs/2204.07705},
  urldate = {2023-06-03},
  abstract = {How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce Super-NaturalInstructions, a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classification, extraction, infilling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructions -- training models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones. Furthermore, we build Tk-Instruct, a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples). Our experiments show that Tk-Instruct outperforms existing instruction-following models such as InstructGPT by over 9\% on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\23GC3CLD\\Wang Á≠â - 2022 - Super-NaturalInstructions Generalization via Decl.pdf;D\:\\Zotero\\literature_data\\storage\\DBEIU8LG\\2204.html}
}

@online{wang2022c,
  title = {Incorporating {{Hierarchy}} into {{Text Encoder}}: A {{Contrastive Learning Approach}} for {{Hierarchical Text Classification}}},
  shorttitle = {Incorporating {{Hierarchy}} into {{Text Encoder}}},
  author = {Wang, Zihan and Wang, Peiyi and Huang, Lianzhe and Sun, Xin and Wang, Houfeng},
  date = {2022-03-23},
  eprint = {2203.03825},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2203.03825},
  urldate = {2023-08-24},
  abstract = {Hierarchical text classification is a challenging subtask of multi-label classification due to its complex label hierarchy. Existing methods encode text and label hierarchy separately and mix their representations for classification, where the hierarchy remains unchanged for all input text. Instead of modeling them separately, in this work, we propose Hierarchy-guided Contrastive Learning (HGCLR) to directly embed the hierarchy into a text encoder. During training, HGCLR constructs positive samples for input text under the guidance of the label hierarchy. By pulling together the input text and its positive sample, the text encoder can learn to generate the hierarchy-aware text representation independently. Therefore, after training, the HGCLR enhanced text encoder can dispense with the redundant hierarchy. Extensive experiments on three benchmark datasets verify the effectiveness of HGCLR.},
  pubstate = {preprint},
  keywords = {2,Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\XUCEMWD6\\Wang Á≠â - 2022 - Incorporating Hierarchy into Text Encoder a Contr.pdf;D\:\\Zotero\\literature_data\\storage\\8P5ZZXM8\\2203.html}
}

@online{wang2023,
  title = {Survey on {{Factuality}} in {{Large Language Models}}: {{Knowledge}}, {{Retrieval}} and {{Domain-Specificity}}},
  shorttitle = {Survey on {{Factuality}} in {{Large Language Models}}},
  author = {Wang, Cunxiang and Liu, Xiaoze and Yue, Yuanhao and Tang, Xiangru and Zhang, Tianhang and Jiayang, Cheng and Yao, Yunzhi and Gao, Wenyang and Hu, Xuming and Qi, Zehan and Wang, Yidong and Yang, Linyi and Wang, Jindong and Xie, Xing and Zhang, Zheng and Zhang, Yue},
  date = {2023-10-18},
  eprint = {2310.07521},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2310.07521},
  urldate = {2023-11-08},
  abstract = {This survey addresses the crucial issue of factuality in Large Language Models (LLMs). As LLMs find applications across diverse domains, the reliability and accuracy of their outputs become vital. We define the Factuality Issue as the probability of LLMs to produce content inconsistent with established facts. We first delve into the implications of these inaccuracies, highlighting the potential consequences and challenges posed by factual errors in LLM outputs. Subsequently, we analyze the mechanisms through which LLMs store and process facts, seeking the primary causes of factual errors. Our discussion then transitions to methodologies for evaluating LLM factuality, emphasizing key metrics, benchmarks, and studies. We further explore strategies for enhancing LLM factuality, including approaches tailored for specific domains. We focus two primary LLM configurations standalone LLMs and Retrieval-Augmented LLMs that utilizes external data, we detail their unique challenges and potential enhancements. Our survey offers a structured guide for researchers aiming to fortify the factual reliability of LLMs.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\YADUPCBV\\Wang Á≠â - 2023 - Survey on Factuality in Large Language Models Kno.pdf;D\:\\Zotero\\literature_data\\storage\\KIE38KEA\\2310.html}
}

@online{wang2023a,
  title = {Survey on {{Factuality}} in {{Large Language Models}}: {{Knowledge}}, {{Retrieval}} and {{Domain-Specificity}}},
  shorttitle = {Survey on {{Factuality}} in {{Large Language Models}}},
  author = {Wang, Cunxiang and Liu, Xiaoze and Yue, Yuanhao and Tang, Xiangru and Zhang, Tianhang and Jiayang, Cheng and Yao, Yunzhi and Gao, Wenyang and Hu, Xuming and Qi, Zehan and Wang, Yidong and Yang, Linyi and Wang, Jindong and Xie, Xing and Zhang, Zheng and Zhang, Yue},
  date = {2023-10-18},
  eprint = {2310.07521},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2310.07521},
  urldate = {2023-10-25},
  abstract = {This survey addresses the crucial issue of factuality in Large Language Models (LLMs). As LLMs find applications across diverse domains, the reliability and accuracy of their outputs become vital. We define the Factuality Issue as the probability of LLMs to produce content inconsistent with established facts. We first delve into the implications of these inaccuracies, highlighting the potential consequences and challenges posed by factual errors in LLM outputs. Subsequently, we analyze the mechanisms through which LLMs store and process facts, seeking the primary causes of factual errors. Our discussion then transitions to methodologies for evaluating LLM factuality, emphasizing key metrics, benchmarks, and studies. We further explore strategies for enhancing LLM factuality, including approaches tailored for specific domains. We focus two primary LLM configurations standalone LLMs and Retrieval-Augmented LLMs that utilizes external data, we detail their unique challenges and potential enhancements. Our survey offers a structured guide for researchers aiming to fortify the factual reliability of LLMs.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,‰∫ãÂÆûÊÄß},
  file = {D\:\\Zotero\\literature_data\\storage\\T7LUQJWT\\Wang Á≠â - 2023 - Survey on Factuality in Large Language Models Kno.pdf;D\:\\Zotero\\literature_data\\storage\\5CERW2F4\\2310.html}
}

@online{wang2023b,
  title = {Element-Aware {{Summarization}} with {{Large Language Models}}: {{Expert-aligned Evaluation}} and {{Chain-of-Thought Method}}},
  shorttitle = {Element-Aware {{Summarization}} with {{Large Language Models}}},
  author = {Wang, Yiming and Zhang, Zhuosheng and Wang, Rui},
  date = {2023-05-22},
  eprint = {2305.13412},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2305.13412},
  urldate = {2023-12-01},
  abstract = {Automatic summarization generates concise summaries that contain key ideas of source documents. As the most mainstream datasets for the news sub-domain, CNN/DailyMail and BBC XSum have been widely used for performance benchmarking. However, the reference summaries of those datasets turn out to be noisy, mainly in terms of factual hallucination and information redundancy. To address this challenge, we first annotate new expert-writing Element-aware test sets following the "Lasswell Communication Model" proposed by Lasswell (1948), allowing reference summaries to focus on more fine-grained news elements objectively and comprehensively. Utilizing the new test sets, we observe the surprising zero-shot summary ability of LLMs, which addresses the issue of the inconsistent results between human preference and automatic evaluation metrics of LLMs' zero-shot summaries in prior work. Further, we propose a Summary Chain-of-Thought (SumCoT) technique to elicit LLMs to generate summaries step by step, which helps them integrate more fine-grained details of source documents into the final summaries that correlate with the human writing mindset. Experimental results show our method outperforms state-of-the-art fine-tuned PLMs and zero-shot LLMs by +4.33/+4.77 in ROUGE-L on the two datasets, respectively. Dataset and code are publicly available at https://github.com/Alsace08/SumCoT.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Áéã‰∏ÄÈ∏£Â≠¶ÈïøÁöÑpaper},
  file = {D\:\\Zotero\\literature_data\\storage\\CCGPJ7KT\\Wang Á≠â - 2023 - Element-aware Summarization with Large Language Mo.pdf;D\:\\Zotero\\literature_data\\storage\\RBYWLVYS\\2305.html}
}

@online{wang2023c,
  title = {{{AIGCIQA2023}}: {{A Large-scale Image Quality Assessment Database}} for {{AI Generated Images}}: From the {{Perspectives}} of {{Quality}}, {{Authenticity}} and {{Correspondence}}},
  shorttitle = {{{AIGCIQA2023}}},
  author = {Wang, Jiarui and Duan, Huiyu and Liu, Jing and Chen, Shi and Min, Xiongkuo and Zhai, Guangtao},
  date = {2023-07-15},
  eprint = {2307.00211},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2307.00211},
  urldate = {2023-07-21},
  abstract = {In this paper, in order to get a better understanding of the human visual preferences for AIGIs, a large-scale IQA database for AIGC is established, which is named as AIGCIQA2023. We first generate over 2000 images based on 6 state-of-the-art text-to-image generation models using 100 prompts. Based on these images, a well-organized subjective experiment is conducted to assess the human visual preferences for each image from three perspectives including quality, authenticity and correspondence. Finally, based on this large-scale database, we conduct a benchmark experiment to evaluate the performance of several state-of-the-art IQA metrics on our constructed database.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {D:\Zotero\literature_data\storage\5QIZ362U\Wang Á≠â - 2023 - AIGCIQA2023 A Large-scale Image Quality Assessmen.pdf}
}

@online{wei2022,
  title = {Emergent {{Abilities}} of {{Large Language Models}}},
  author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
  date = {2022-10-26},
  eprint = {2206.07682},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2206.07682},
  urldate = {2023-12-25},
  abstract = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\RK4BE4K5\\Wei Á≠â - 2022 - Emergent Abilities of Large Language Models.pdf;D\:\\Zotero\\literature_data\\storage\\B6WTYTHX\\2206.html}
}

@online{weller2023,
  title = {"{{According}} to ..." {{Prompting Language Models Improves Quoting}} from {{Pre-Training Data}}},
  author = {Weller, Orion and Marone, Marc and Weir, Nathaniel and Lawrie, Dawn and Khashabi, Daniel and Van Durme, Benjamin},
  date = {2023-05-22},
  eprint = {2305.13252},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2305.13252},
  urldate = {2023-12-29},
  abstract = {Large Language Models (LLMs) may hallucinate and generate fake information, despite pre-training on factual data. Inspired by the journalistic device of "according to sources", we propose according-to prompting: directing LLMs to ground responses against previously observed text. To quantify this grounding, we propose a novel evaluation metric (QUIP-Score) that measures the extent to which model-produced answers are directly found in underlying text corpora. We illustrate with experiments on Wikipedia that these prompts improve grounding under our metrics, with the additional benefit of often improving end-task performance. Furthermore, prompts that ask the model to decrease grounding (or to ground to other corpora) decrease grounding, indicating the ability of language models to increase or decrease grounded generations on request.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\AULFUE99\\Weller Á≠â - 2023 - According to ... Prompting Language Models Impro.pdf;D\:\\Zotero\\literature_data\\storage\\PATY34MM\\2305.html}
}

@online{weller2023a,
  title = {"{{According}} to ..." {{Prompting Language Models Improves Quoting}} from {{Pre-Training Data}}},
  author = {Weller, Orion and Marone, Marc and Weir, Nathaniel and Lawrie, Dawn and Khashabi, Daniel and Van Durme, Benjamin},
  date = {2023-05-22},
  eprint = {2305.13252},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2305.13252},
  urldate = {2023-11-29},
  abstract = {Large Language Models (LLMs) may hallucinate and generate fake information, despite pre-training on factual data. Inspired by the journalistic device of "according to sources", we propose according-to prompting: directing LLMs to ground responses against previously observed text. To quantify this grounding, we propose a novel evaluation metric (QUIP-Score) that measures the extent to which model-produced answers are directly found in underlying text corpora. We illustrate with experiments on Wikipedia that these prompts improve grounding under our metrics, with the additional benefit of often improving end-task performance. Furthermore, prompts that ask the model to decrease grounding (or to ground to other corpora) decrease grounding, indicating the ability of language models to increase or decrease grounded generations on request.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\LUCWB8YZ\\Weller Á≠â - 2023 - According to ... Prompting Language Models Impro.pdf;D\:\\Zotero\\literature_data\\storage\\I3LSHXSP\\2305.html}
}

@online{williams2018,
  title = {A {{Broad-Coverage Challenge Corpus}} for {{Sentence Understanding}} through {{Inference}}},
  author = {Williams, Adina and Nangia, Nikita and Bowman, Samuel R.},
  date = {2018-02-19},
  eprint = {1704.05426},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1704.05426},
  url = {http://arxiv.org/abs/1704.05426},
  urldate = {2023-05-16},
  abstract = {This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. In addition to being one of the largest corpora available for the task of NLI, at 433k examples, this corpus improves upon available resources in its coverage: it offers data from ten distinct genres of written and spoken English--making it possible to evaluate systems on nearly the full complexity of the language--and it offers an explicit setting for the evaluation of cross-genre domain adaptation.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\FUV84WTU\\Williams Á≠â - 2018 - A Broad-Coverage Challenge Corpus for Sentence Und.pdf;D\:\\Zotero\\literature_data\\storage\\K22N5V7X\\1704.html}
}

@online{workshop2023,
  title = {{{BLOOM}}: {{A 176B-Parameter Open-Access Multilingual Language Model}}},
  shorttitle = {{{BLOOM}}},
  author = {Workshop, BigScience and Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Iliƒá, Suzana and Hesslow, Daniel and Castagn√©, Roman and Luccioni, Alexandra Sasha and Yvon, Fran√ßois and Gall√©, Matthias and Tow, Jonathan and Rush, Alexander M. and Biderman, Stella and Webson, Albert and Ammanamanchi, Pawan Sasanka and Wang, Thomas and Sagot, Beno√Æt and Muennighoff, Niklas and family=Moral, given=Albert Villanova, prefix=del, useprefix=true and Ruwase, Olatunji and Bawden, Rachel and Bekman, Stas and McMillan-Major, Angelina and Beltagy, Iz and Nguyen, Huu and Saulnier, Lucile and Tan, Samson and Suarez, Pedro Ortiz and Sanh, Victor and Lauren√ßon, Hugo and Jernite, Yacine and Launay, Julien and Mitchell, Margaret and Raffel, Colin and Gokaslan, Aaron and Simhi, Adi and Soroa, Aitor and Aji, Alham Fikri and Alfassy, Amit and Rogers, Anna and Nitzav, Ariel Kreisberg and Xu, Canwen and Mou, Chenghao and Emezue, Chris and Klamm, Christopher and Leong, Colin and family=Strien, given=Daniel, prefix=van, useprefix=true and Adelani, David Ifeoluwa and Radev, Dragomir and Ponferrada, Eduardo Gonz√°lez and Levkovizh, Efrat and Kim, Ethan and Natan, Eyal Bar and De Toni, Francesco and Dupont, G√©rard and Kruszewski, Germ√°n and Pistilli, Giada and Elsahar, Hady and Benyamina, Hamza and Tran, Hieu and Yu, Ian and Abdulmumin, Idris and Johnson, Isaac and Gonzalez-Dios, Itziar and family=Rosa, given=Javier, prefix=de la, useprefix=true and Chim, Jenny and Dodge, Jesse and Zhu, Jian and Chang, Jonathan and Frohberg, J√∂rg and Tobing, Joseph and Bhattacharjee, Joydeep and Almubarak, Khalid and Chen, Kimbo and Lo, Kyle and Von Werra, Leandro and Weber, Leon and Phan, Long and {allal}, Loubna Ben and Tanguy, Ludovic and Dey, Manan and Mu√±oz, Manuel Romero and Masoud, Maraim and Grandury, Mar√≠a and ≈†a≈°ko, Mario and Huang, Max and Coavoux, Maximin and Singh, Mayank and Jiang, Mike Tian-Jian and Vu, Minh Chien and Jauhar, Mohammad A. and Ghaleb, Mustafa and Subramani, Nishant and Kassner, Nora and Khamis, Nurulaqilla and Nguyen, Olivier and Espejel, Omar and family=Gibert, given=Ona, prefix=de, useprefix=true and Villegas, Paulo and Henderson, Peter and Colombo, Pierre and Amuok, Priscilla and Lhoest, Quentin and Harliman, Rheza and Bommasani, Rishi and L√≥pez, Roberto Luis and Ribeiro, Rui and Osei, Salomey and Pyysalo, Sampo and Nagel, Sebastian and Bose, Shamik and Muhammad, Shamsuddeen Hassan and Sharma, Shanya and Longpre, Shayne and Nikpoor, Somaieh and Silberberg, Stanislav and Pai, Suhas and Zink, Sydney and Torrent, Tiago Timponi and Schick, Timo and Thrush, Tristan and Danchev, Valentin and Nikoulina, Vassilina and Laippala, Veronika and Lepercq, Violette and Prabhu, Vrinda and Alyafeai, Zaid and Talat, Zeerak and Raja, Arun and Heinzerling, Benjamin and Si, Chenglei and Ta≈üar, Davut Emre and Salesky, Elizabeth and Mielke, Sabrina J. and Lee, Wilson Y. and Sharma, Abheesht and Santilli, Andrea and Chaffin, Antoine and Stiegler, Arnaud and Datta, Debajyoti and Szczechla, Eliza and Chhablani, Gunjan and Wang, Han and Pandey, Harshit and Strobelt, Hendrik and Fries, Jason Alan and Rozen, Jos and Gao, Leo and Sutawika, Lintang and Bari, M. Saiful and Al-shaibani, Maged S. and Manica, Matteo and Nayak, Nihal and Teehan, Ryan and Albanie, Samuel and Shen, Sheng and Ben-David, Srulik and Bach, Stephen H. and Kim, Taewoon and Bers, Tali and Fevry, Thibault and Neeraj, Trishala and Thakker, Urmish and Raunak, Vikas and Tang, Xiangru and Yong, Zheng-Xin and Sun, Zhiqing and Brody, Shaked and Uri, Yallow and Tojarieh, Hadar and Roberts, Adam and Chung, Hyung Won and Tae, Jaesung and Phang, Jason and Press, Ofir and Li, Conglong and Narayanan, Deepak and Bourfoune, Hatim and Casper, Jared and Rasley, Jeff and Ryabinin, Max and Mishra, Mayank and Zhang, Minjia and Shoeybi, Mohammad and Peyrounette, Myriam and Patry, Nicolas and Tazi, Nouamane and Sanseviero, Omar and family=Platen, given=Patrick, prefix=von, useprefix=true and Cornette, Pierre and Lavall√©e, Pierre Fran√ßois and Lacroix, R√©mi and Rajbhandari, Samyam and Gandhi, Sanchit and Smith, Shaden and Requena, St√©phane and Patil, Suraj and Dettmers, Tim and Baruwa, Ahmed and Singh, Amanpreet and Cheveleva, Anastasia and Ligozat, Anne-Laure and Subramonian, Arjun and N√©v√©ol, Aur√©lie and Lovering, Charles and Garrette, Dan and Tunuguntla, Deepak and Reiter, Ehud and Taktasheva, Ekaterina and Voloshina, Ekaterina and Bogdanov, Eli and Winata, Genta Indra and Schoelkopf, Hailey and Kalo, Jan-Christoph and Novikova, Jekaterina and Forde, Jessica Zosa and Clive, Jordan and Kasai, Jungo and Kawamura, Ken and Hazan, Liam and Carpuat, Marine and Clinciu, Miruna and Kim, Najoung and Cheng, Newton and Serikov, Oleg and Antverg, Omer and family=Wal, given=Oskar, prefix=van der, useprefix=true and Zhang, Rui and Zhang, Ruochen and Gehrmann, Sebastian and Mirkin, Shachar and Pais, Shani and Shavrina, Tatiana and Scialom, Thomas and Yun, Tian and Limisiewicz, Tomasz and Rieser, Verena and Protasov, Vitaly and Mikhailov, Vladislav and Pruksachatkun, Yada and Belinkov, Yonatan and Bamberger, Zachary and Kasner, Zdenƒõk and Rueda, Alice and Pestana, Amanda and Feizpour, Amir and Khan, Ammar and Faranak, Amy and Santos, Ana and Hevia, Anthony and Unldreaj, Antigona and Aghagol, Arash and Abdollahi, Arezoo and Tammour, Aycha and HajiHosseini, Azadeh and Behroozi, Bahareh and Ajibade, Benjamin and Saxena, Bharat and Ferrandis, Carlos Mu√±oz and Contractor, Danish and Lansky, David and David, Davis and Kiela, Douwe and Nguyen, Duong A. and Tan, Edward and Baylor, Emi and Ozoani, Ezinwanne and Mirza, Fatima and Ononiwu, Frankline and Rezanejad, Habib and Jones, Hessie and Bhattacharya, Indrani and Solaiman, Irene and Sedenko, Irina and Nejadgholi, Isar and Passmore, Jesse and Seltzer, Josh and Sanz, Julio Bonis and Dutra, Livia and Samagaio, Mairon and Elbadri, Maraim and Mieskes, Margot and Gerchick, Marissa and Akinlolu, Martha and McKenna, Michael and Qiu, Mike and Ghauri, Muhammed and Burynok, Mykola and Abrar, Nafis and Rajani, Nazneen and Elkott, Nour and Fahmy, Nour and Samuel, Olanrewaju and An, Ran and Kromann, Rasmus and Hao, Ryan and Alizadeh, Samira and Shubber, Sarmad and Wang, Silas and Roy, Sourav and Viguier, Sylvain and Le, Thanh and Oyebade, Tobi and Le, Trieu and Yang, Yoyo and Nguyen, Zach and Kashyap, Abhinav Ramesh and Palasciano, Alfredo and Callahan, Alison and Shukla, Anima and Miranda-Escalada, Antonio and Singh, Ayush and Beilharz, Benjamin and Wang, Bo and Brito, Caio and Zhou, Chenxi and Jain, Chirag and Xu, Chuxin and Fourrier, Cl√©mentine and Peri√±√°n, Daniel Le√≥n and Molano, Daniel and Yu, Dian and Manjavacas, Enrique and Barth, Fabio and Fuhrimann, Florian and Altay, Gabriel and Bayrak, Giyaseddin and Burns, Gully and Vrabec, Helena U. and Bello, Imane and Dash, Ishani and Kang, Jihyun and Giorgi, John and Golde, Jonas and Posada, Jose David and Sivaraman, Karthik Rangasai and Bulchandani, Lokesh and Liu, Lu and Shinzato, Luisa and family=Bykhovetz, given=Madeleine Hahn, prefix=de, useprefix=true and Takeuchi, Maiko and P√†mies, Marc and Castillo, Maria A. and Nezhurina, Marianna and S√§nger, Mario and Samwald, Matthias and Cullan, Michael and Weinberg, Michael and De Wolf, Michiel and Mihaljcic, Mina and Liu, Minna and Freidank, Moritz and Kang, Myungsun and Seelam, Natasha and Dahlberg, Nathan and Broad, Nicholas Michio and Muellner, Nikolaus and Fung, Pascale and Haller, Patrick and Chandrasekhar, Ramya and Eisenberg, Renata and Martin, Robert and Canalli, Rodrigo and Su, Rosaline and Su, Ruisi and Cahyawijaya, Samuel and Garda, Samuele and Deshmukh, Shlok S. and Mishra, Shubhanshu and Kiblawi, Sid and Ott, Simon and Sang-aroonsiri, Sinee and Kumar, Srishti and Schweter, Stefan and Bharati, Sushil and Laud, Tanmay and Gigant, Th√©o and Kainuma, Tomoya and Kusa, Wojciech and Labrak, Yanis and Bajaj, Yash Shailesh and Venkatraman, Yash and Xu, Yifan and Xu, Yingxin and Xu, Yu and Tan, Zhe and Xie, Zhongli and Ye, Zifan and Bras, Mathilde and Belkada, Younes and Wolf, Thomas},
  date = {2023-03-13},
  eprint = {2211.05100},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2211.05100},
  url = {http://arxiv.org/abs/2211.05100},
  urldate = {2023-06-03},
  abstract = {Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\2V3VZF39\\Workshop Á≠â - 2023 - BLOOM A 176B-Parameter Open-Access Multilingual L.pdf;D\:\\Zotero\\literature_data\\storage\\ULQCMFU4\\2211.html}
}

@article{worledge,
  title = {{{UNIFYING CORROBORATIVE AND CONTRIBUTIVE ATTRIBUTIONS IN LARGE LANGUAGE MODELS}}},
  author = {Worledge, Theodora and Shen, Judy Hanwen and Meister, Nicole and Winston, Caleb and Guestrin, Carlos},
  langid = {english},
  file = {D:\Zotero\literature_data\storage\YDV2LQEZ\Worledge Á≠â - UNIFYING CORROBORATIVE AND CONTRIBUTIVE ATTRIBUTIO.pdf}
}

@online{wu2018,
  title = {Unsupervised {{Feature Learning}} via {{Non-Parametric Instance-level Discrimination}}},
  author = {Wu, Zhirong and Xiong, Yuanjun and Yu, Stella and Lin, Dahua},
  date = {2018-05-04},
  eprint = {1805.01978},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1805.01978},
  urldate = {2023-07-24},
  abstract = {Neural net classifiers trained on data with annotated class labels can also capture apparent visual similarity among categories without being directed to do so. We study whether this observation can be extended beyond the conventional domain of supervised learning: Can we learn a good feature representation that captures apparent similarity among instances, instead of classes, by merely asking the feature to be discriminative of individual instances? We formulate this intuition as a non-parametric classification problem at the instance-level, and use noise-contrastive estimation to tackle the computational challenges imposed by the large number of instance classes. Our experimental results demonstrate that, under unsupervised learning settings, our method surpasses the state-of-the-art on ImageNet classification by a large margin. Our method is also remarkable for consistently improving test performance with more training data and better network architectures. By fine-tuning the learned feature, we further obtain competitive results for semi-supervised learning and object detection tasks. Our non-parametric model is highly compact: With 128 features per image, our method requires only 600MB storage for a million images, enabling fast nearest neighbour retrieval at the run time.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Memory Bank},
  file = {D\:\\Zotero\\literature_data\\storage\\ZW7V2YH6\\Wu Á≠â - 2018 - Unsupervised Feature Learning via Non-Parametric I.pdf;D\:\\Zotero\\literature_data\\storage\\F3M4WJNZ\\1805.html}
}

@online{wu2018a,
  title = {Unsupervised {{Feature Learning}} via {{Non-Parametric Instance-level Discrimination}}},
  author = {Wu, Zhirong and Xiong, Yuanjun and Yu, Stella and Lin, Dahua},
  date = {2018-05-04},
  eprint = {1805.01978},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1805.01978},
  urldate = {2023-07-22},
  abstract = {Neural net classifiers trained on data with annotated class labels can also capture apparent visual similarity among categories without being directed to do so. We study whether this observation can be extended beyond the conventional domain of supervised learning: Can we learn a good feature representation that captures apparent similarity among instances, instead of classes, by merely asking the feature to be discriminative of individual instances? We formulate this intuition as a non-parametric classification problem at the instance-level, and use noise-contrastive estimation to tackle the computational challenges imposed by the large number of instance classes. Our experimental results demonstrate that, under unsupervised learning settings, our method surpasses the state-of-the-art on ImageNet classification by a large margin. Our method is also remarkable for consistently improving test performance with more training data and better network architectures. By fine-tuning the learned feature, we further obtain competitive results for semi-supervised learning and object detection tasks. Our non-parametric model is highly compact: With 128 features per image, our method requires only 600MB storage for a million images, enabling fast nearest neighbour retrieval at the run time.},
  pubstate = {preprint},
  version = {1},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Memory Bank},
  file = {D\:\\Zotero\\literature_data\\storage\\LMZJ3HGS\\Wu Á≠â - 2018 - Unsupervised Feature Learning via Non-Parametric I.pdf;D\:\\Zotero\\literature_data\\storage\\A3S7DR7L\\1805.html}
}

@online{wu2019,
  title = {Simplifying {{Graph Convolutional Networks}}},
  author = {Wu, Felix and Zhang, Tianyi and family=Souza Jr., given=Amauri Holanda, prefix=de, useprefix=false and Fifty, Christopher and Yu, Tao and Weinberger, Kilian Q.},
  date = {2019-06-20},
  eprint = {1902.07153},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1902.07153},
  urldate = {2023-05-12},
  abstract = {Graph Convolutional Networks (GCNs) and their variants have experienced significant attention and have become the de facto methods for learning graph representations. GCNs derive inspiration primarily from recent deep learning approaches, and as a result, may inherit unnecessary complexity and redundant computation. In this paper, we reduce this excess complexity through successively removing nonlinearities and collapsing weight matrices between consecutive layers. We theoretically analyze the resulting linear model and show that it corresponds to a fixed low-pass filter followed by a linear classifier. Notably, our experimental evaluation demonstrates that these simplifications do not negatively impact accuracy in many downstream applications. Moreover, the resulting model scales to larger datasets, is naturally interpretable, and yields up to two orders of magnitude speedup over FastGCN.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,GCN,Statistics - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\9RN8MJFW\\Wu Á≠â - 2019 - Simplifying Graph Convolutional Networks.pdf;D\:\\Zotero\\literature_data\\storage\\UNLGSSRW\\1902.html}
}

@article{wu2021,
  title = {A {{Comprehensive Survey}} on {{Graph Neural Networks}}},
  author = {Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S.},
  date = {2021-01},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  shortjournal = {IEEE Trans. Neural Netw. Learning Syst.},
  volume = {32},
  number = {1},
  eprint = {1901.00596},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  pages = {4--24},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2020.2978386},
  url = {http://arxiv.org/abs/1901.00596},
  urldate = {2023-04-15},
  abstract = {Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this survey, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art graph neural networks into four categories, namely recurrent graph neural networks, convolutional graph neural networks, graph autoencoders, and spatial-temporal graph neural networks. We further discuss the applications of graph neural networks across various domains and summarize the open source codes, benchmark data sets, and model evaluation of graph neural networks. Finally, we propose potential research directions in this rapidly growing field.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\BZLGHD38\\Wu Á≠â - 2021 - A Comprehensive Survey on Graph Neural Networks.pdf;D\:\\Zotero\\literature_data\\storage\\QVQ3BCVY\\1901.html}
}

@online{wu2023,
  title = {{{SEGA}}: {{Structural Entropy Guided Anchor View}} for {{Graph Contrastive Learning}}},
  shorttitle = {{{SEGA}}},
  author = {Wu, Junran and Chen, Xueyuan and Shi, Bowen and Li, Shangzhe and Xu, Ke},
  date = {2023-05-08},
  eprint = {2305.04501},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.04501},
  url = {http://arxiv.org/abs/2305.04501},
  urldate = {2023-05-11},
  abstract = {In contrastive learning, the choice of ``view'' controls the information that the representation captures and influences the performance of the model. However, leading graph contrastive learning methods generally produce views via random corruption or learning, which could lead to the loss of essential information and alteration of semantic information. An anchor view that maintains the essential information of input graphs for contrastive learning has been hardly investigated. In this paper, based on the theory of graph information bottleneck, we deduce the definition of this anchor view; put differently, \textbackslash textit\{the anchor view with essential information of input graph is supposed to have the minimal structural uncertainty\}. Furthermore, guided by structural entropy, we implement the anchor view, termed \textbackslash textbf\{SEGA\}, for graph contrastive learning. We extensively validate the proposed anchor view on various benchmarks regarding graph classification under unsupervised, semi-supervised, and transfer learning and achieve significant performance boosts compared to the state-of-the-art methods.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Áõ∏ÂÖ≥ÂâçÊ≤ø},
  file = {D\:\\Zotero\\literature_data\\storage\\3CMIBQ56\\Wu Á≠â - 2023 - SEGA Structural Entropy Guided Anchor View for Gr.pdf;D\:\\Zotero\\literature_data\\storage\\DWBJQNZM\\2305.html}
}

@online{wu2023a,
  title = {Human {{Preference Score}} v2: {{A Solid Benchmark}} for {{Evaluating Human Preferences}} of {{Text-to-Image Synthesis}}},
  shorttitle = {Human {{Preference Score}} V2},
  author = {Wu, Xiaoshi and Hao, Yiming and Sun, Keqiang and Chen, Yixiong and Zhu, Feng and Zhao, Rui and Li, Hongsheng},
  date = {2023-09-25},
  eprint = {2306.09341},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2306.09341},
  urldate = {2023-10-16},
  abstract = {Recent text-to-image generative models can generate high-fidelity images from text inputs, but the quality of these generated images cannot be accurately evaluated by existing evaluation metrics. To address this issue, we introduce Human Preference Dataset v2 (HPD v2), a large-scale dataset that captures human preferences on images from a wide range of sources. HPD v2 comprises 798,090 human preference choices on 433,760 pairs of images, making it the largest dataset of its kind. The text prompts and images are deliberately collected to eliminate potential bias, which is a common issue in previous datasets. By fine-tuning CLIP on HPD v2, we obtain Human Preference Score v2 (HPS v2), a scoring model that can more accurately predict human preferences on generated images. Our experiments demonstrate that HPS v2 generalizes better than previous metrics across various image distributions and is responsive to algorithmic improvements of text-to-image generative models, making it a preferable evaluation metric for these models. We also investigate the design of the evaluation prompts for text-to-image generative models, to make the evaluation stable, fair and easy-to-use. Finally, we establish a benchmark for text-to-image generative models using HPS v2, which includes a set of recent text-to-image models from the academic, community and industry. The code and dataset is available at https://github.com/tgxs002/HPSv2 .},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Databases},
  file = {D\:\\Zotero\\literature_data\\storage\\F7E4VS6W\\Wu Á≠â - 2023 - Human Preference Score v2 A Solid Benchmark for E.pdf;D\:\\Zotero\\literature_data\\storage\\XTPF4M7B\\2306.html}
}

@online{wu2023b,
  title = {Better {{Aligning Text-to-Image Models}} with {{Human Preference}}},
  author = {Wu, Xiaoshi and Sun, Keqiang and Zhu, Feng and Zhao, Rui and Li, Hongsheng},
  date = {2023-03-25},
  eprint = {2303.14420},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2303.14420},
  urldate = {2023-07-20},
  abstract = {Recent years have witnessed a rapid growth of deep generative models, with text-to-image models gaining significant attention from the public. However, existing models often generate images that do not align well with human aesthetic preferences, such as awkward combinations of limbs and facial expressions. To address this issue, we collect a dataset of human choices on generated images from the Stable Foundation Discord channel. Our experiments demonstrate that current evaluation metrics for generative models do not correlate well with human choices. Thus, we train a human preference classifier with the collected dataset and derive a Human Preference Score (HPS) based on the classifier. Using the HPS, we propose a simple yet effective method to adapt Stable Diffusion to better align with human aesthetic preferences. Our experiments show that the HPS outperforms CLIP in predicting human choices and has good generalization capability towards images generated from other models. By tuning Stable Diffusion with the guidance of the HPS, the adapted model is able to generate images that are more preferred by human users.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Êï∞ÊçÆÈõÜ},
  file = {D\:\\Zotero\\literature_data\\storage\\IYZBV2BK\\Wu Á≠â - 2023 - Better Aligning Text-to-Image Models with Human Pr.pdf;D\:\\Zotero\\literature_data\\storage\\GUTZKEXX\\2303.html}
}

@article{xia2021,
  title = {Graph {{Learning}}: {{A Survey}}},
  shorttitle = {Graph {{Learning}}},
  author = {Xia, Feng and Sun, Ke and Yu, Shuo and Aziz, Abdul and Wan, Liangtian and Pan, Shirui and Liu, Huan},
  date = {2021-04},
  journaltitle = {IEEE Transactions on Artificial Intelligence},
  shortjournal = {IEEE Trans. Artif. Intell.},
  volume = {2},
  number = {2},
  eprint = {2105.00696},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {109--127},
  issn = {2691-4581},
  doi = {10.1109/TAI.2021.3076021},
  url = {http://arxiv.org/abs/2105.00696},
  urldate = {2023-04-12},
  abstract = {Graphs are widely used as a popular representation of the network structure of connected data. Graph data can be found in a broad spectrum of application domains such as social systems, ecosystems, biological networks, knowledge graphs, and information systems. With the continuous penetration of artificial intelligence technologies, graph learning (i.e., machine learning on graphs) is gaining attention from both researchers and practitioners. Graph learning proves effective for many tasks, such as classification, link prediction, and matching. Generally, graph learning methods extract relevant features of graphs by taking advantage of machine learning algorithms. In this survey, we present a comprehensive overview on the state-of-the-art of graph learning. Special attention is paid to four categories of existing graph learning methods, including graph signal processing, matrix factorization, random walk, and deep learning. Major models and algorithms under these categories are reviewed respectively. We examine graph learning applications in areas such as text, images, science, knowledge graphs, and combinatorial optimization. In addition, we discuss several promising research directions in this field.},
  keywords = {68T07,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Social and Information Networks,Graph LearningÁªºËø∞,I.2.6},
  file = {D\:\\Zotero\\literature_data\\storage\\Z9XL9WMP\\Xia Á≠â - 2021 - Graph Learning A Survey.pdf;D\:\\Zotero\\literature_data\\storage\\K2WIICSM\\2105.html}
}

@online{xiao2023,
  title = {Introduction to {{Transformers}}: An {{NLP Perspective}}},
  shorttitle = {Introduction to {{Transformers}}},
  author = {Xiao, Tong and Zhu, Jingbo},
  date = {2023-11-29},
  eprint = {2311.17633},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2311.17633},
  urldate = {2023-12-31},
  abstract = {Transformers have dominated empirical machine learning models of natural language processing. In this paper, we introduce basic concepts of Transformers and present key techniques that form the recent advances of these models. This includes a description of the standard Transformer architecture, a series of model refinements, and common applications. Given that Transformers and related deep learning techniques might be evolving in ways we have never seen, we cannot dive into all the model details or cover all the technical areas. Instead, we focus on just those concepts that are helpful for gaining a good understanding of Transformers and their variants. We also summarize the key ideas that impact this field, thereby yielding some insights into the strengths and limitations of these models.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {D:\Zotero\literature_data\storage\QI4YLA3Z\Xiao Âíå Zhu - 2023 - Introduction to Transformers an NLP Perspective.pdf}
}

@online{xiao2023a,
  title = {Introduction to {{Transformers}}: An {{NLP Perspective}}},
  shorttitle = {Introduction to {{Transformers}}},
  author = {Xiao, Tong and Zhu, Jingbo},
  date = {2023-11-29},
  eprint = {2311.17633},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2311.17633},
  urldate = {2023-12-30},
  abstract = {Transformers have dominated empirical machine learning models of natural language processing. In this paper, we introduce basic concepts of Transformers and present key techniques that form the recent advances of these models. This includes a description of the standard Transformer architecture, a series of model refinements, and common applications. Given that Transformers and related deep learning techniques might be evolving in ways we have never seen, we cannot dive into all the model details or cover all the technical areas. Instead, we focus on just those concepts that are helpful for gaining a good understanding of Transformers and their variants. We also summarize the key ideas that impact this field, thereby yielding some insights into the strengths and limitations of these models.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {D:\Zotero\literature_data\storage\DCAI4N4D\Xiao Âíå Zhu - 2023 - Introduction to Transformers an NLP Perspective.pdf}
}

@online{xie2023,
  title = {Enhancing {{Medical Text Evaluation}} with {{GPT-4}}},
  author = {Xie, Yiqing and Zhang, Sheng and Cheng, Hao and Gero, Zelalem and Wong, Cliff and Naumann, Tristan and Poon, Hoifung},
  date = {2023-11-16},
  eprint = {2311.09581},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2311.09581},
  urldate = {2023-11-29},
  abstract = {In the evaluation of medical text generation, it is essential to scrutinize each piece of information and ensure the utmost accuracy of the evaluation. Existing evaluation metrics either focus on coarse-level evaluation that assigns one score for the whole generated output or rely on evaluation models trained on general domain, resulting in inaccuracies when adapted to the medical domain. To address these issues, we propose a set of factuality-centric evaluation aspects and design corresponding GPT-4-based metrics for medical text generation. We systematically compare these metrics with existing ones on clinical note generation and medical report summarization tasks, revealing low inter-metric correlation. A comprehensive human evaluation confirms that the proposed GPT-4-based metrics exhibit substantially higher agreement with human judgments than existing evaluation metrics. Our study contributes to the understanding of medical text generation evaluation and offers a more reliable alternative to existing metrics.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\79LTUVKH\\Xie Á≠â - 2023 - Enhancing Medical Text Evaluation with GPT-4.pdf;D\:\\Zotero\\literature_data\\storage\\LUBDEFHM\\2311.html}
}

@online{xiong2020,
  title = {On {{Layer Normalization}} in the {{Transformer Architecture}}},
  author = {Xiong, Ruibin and Yang, Yunchang and He, Di and Zheng, Kai and Zheng, Shuxin and Xing, Chen and Zhang, Huishuai and Lan, Yanyan and Wang, Liwei and Liu, Tie-Yan},
  date = {2020-06-29},
  eprint = {2002.04745},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2002.04745},
  urldate = {2023-06-03},
  abstract = {The Transformer is widely used in natural language processing tasks. To train a Transformer however, one usually needs a carefully designed learning rate warm-up stage, which is shown to be crucial to the final performance but will slow down the optimization and bring more hyper-parameter tunings. In this paper, we first study theoretically why the learning rate warm-up stage is essential and show that the location of layer normalization matters. Specifically, we prove with mean field theory that at initialization, for the original-designed Post-LN Transformer, which places the layer normalization between the residual blocks, the expected gradients of the parameters near the output layer are large. Therefore, using a large learning rate on those gradients makes the training unstable. The warm-up stage is practically helpful for avoiding this problem. On the other hand, our theory also shows that if the layer normalization is put inside the residual blocks (recently proposed as Pre-LN Transformer), the gradients are well-behaved at initialization. This motivates us to remove the warm-up stage for the training of Pre-LN Transformers. We show in our experiments that Pre-LN Transformers without the warm-up stage can reach comparable results with baselines while requiring significantly less training time and hyper-parameter tuning on a wide range of applications.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\GT7DLI6Q\\Xiong Á≠â - 2020 - On Layer Normalization in the Transformer Architec.pdf;D\:\\Zotero\\literature_data\\storage\\FJILY8FU\\2002.html}
}

@online{xu2023,
  title = {{{ImageReward}}: {{Learning}} and {{Evaluating Human Preferences}} for {{Text-to-Image Generation}}},
  shorttitle = {{{ImageReward}}},
  author = {Xu, Jiazheng and Liu, Xiao and Wu, Yuchen and Tong, Yuxuan and Li, Qinkai and Ding, Ming and Tang, Jie and Dong, Yuxiao},
  date = {2023-06-06},
  eprint = {2304.05977},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2304.05977},
  urldate = {2023-07-20},
  abstract = {We present a comprehensive solution to learn and improve text-to-image models from human preference feedback. To begin with, we build ImageReward -- the first general-purpose text-to-image human preference reward model -- to effectively encode human preferences. Its training is based on our systematic annotation pipeline including rating and ranking, which collects 137k expert comparisons to date. In human evaluation, ImageReward outperforms existing scoring models and metrics, making it a promising automatic metric for evaluating text-to-image synthesis. On top of it, we propose Reward Feedback Learning (ReFL), a direct tuning algorithm to optimize diffusion models against a scorer. Both automatic and human evaluation support ReFL's advantages over compared methods. All code and datasets are provided at \textbackslash url\{https://github.com/THUDM/ImageReward\}.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Ê®°Âûã},
  file = {D\:\\Zotero\\literature_data\\storage\\TA7GMCY4\\Xu Á≠â - 2023 - ImageReward Learning and Evaluating Human Prefere.pdf;D\:\\Zotero\\literature_data\\storage\\KSRFUVT5\\2304.html}
}

@article{yang,
  title = {{{OPEN}}: {{Orthogonal Propagation}} with {{Ego-Network Modeling}}},
  author = {Yang, Liang and Kang, Lina and Zhang, Qiuliang and Li, Mengzhe and Niu, Bingxin and He, Dongxiao and Wang, Zhen and Wang, Chuan and Cao, Xiaochun and Guo, Yuanfang},
  abstract = {To alleviate the unfavorable effect of noisy topology in Graph Neural networks (GNNs), some efforts perform the local topology refinement through the pairwise propagation weight learning and the multi-channel extension. Unfortunately, most of them suffer a common and fatal drawback: irrelevant propagation to one node and in multi-channels. These two kinds of irrelevances make propagation weights in multi-channels free to be determined by the labeled data, and thus the GNNs are exposed to overfitting. To tackle this issue, a novel Orthogonal Propagation with Ego-Network modeling (OPEN) is proposed by modeling relevances between propagations. Specifically, the relevance between propagations to one node is modeled by whole ego-network modeling, while the relevance between propagations in multi-channels is modeled via diversity requirement. By interpreting the propagations to one node from the perspective of dimension reduction, propagation weights are inferred from principal components of the ego-network, which are orthogonal to each other. Theoretical analysis and experimental evaluations reveal four attractive characteristics of OPEN as modeling high-order relationships beyond pairwise one, preventing overfitting, robustness, and high efficiency.},
  langid = {english},
  file = {D:\Zotero\literature_data\storage\3G54A3LF\Yang Á≠â - OPEN Orthogonal Propagation with Ego-Network Mode.pdf}
}

@online{yang2022,
  title = {Tensor {{Programs V}}: {{Tuning Large Neural Networks}} via {{Zero-Shot Hyperparameter Transfer}}},
  shorttitle = {Tensor {{Programs V}}},
  author = {Yang, Greg and Hu, Edward J. and Babuschkin, Igor and Sidor, Szymon and Liu, Xiaodong and Farhi, David and Ryder, Nick and Pachocki, Jakub and Chen, Weizhu and Gao, Jianfeng},
  date = {2022-03-28},
  eprint = {2203.03466},
  eprinttype = {arxiv},
  eprintclass = {cond-mat},
  url = {http://arxiv.org/abs/2203.03466},
  urldate = {2023-06-04},
  abstract = {Hyperparameter (HP) tuning in deep learning is an expensive process, prohibitively so for neural networks (NNs) with billions of parameters. We show that, in the recently discovered Maximal Update Parametrization (muP), many optimal HPs remain stable even as model size changes. This leads to a new HP tuning paradigm we call muTransfer: parametrize the target model in muP, tune the HP indirectly on a smaller model, and zero-shot transfer them to the full-sized model, i.e., without directly tuning the latter at all. We verify muTransfer on Transformer and ResNet. For example, 1) by transferring pretraining HPs from a model of 13M parameters, we outperform published numbers of BERT-large (350M parameters), with a total tuning cost equivalent to pretraining BERT-large once; 2) by transferring from 40M parameters, we outperform published numbers of the 6.7B GPT-3 model, with tuning cost only 7\% of total pretraining cost. A Pytorch implementation of our technique can be found at github.com/microsoft/mup and installable via `pip install mup`.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Condensed Matter - Disordered Systems and Neural Networks},
  file = {D\:\\Zotero\\literature_data\\storage\\U3D3GFMQ\\Yang Á≠â - 2022 - Tensor Programs V Tuning Large Neural Networks vi.pdf;D\:\\Zotero\\literature_data\\storage\\8228PWMQ\\2203.html}
}

@article{yang2022a,
  title = {{{OPEN}}: {{Orthogonal Propagation}} with {{Ego-Network Modeling}}},
  shorttitle = {{{OPEN}}},
  author = {Yang, Liang and Kang, Lina and Zhang, Qiuliang and Li, Mengzhe and Niu, Bingxin and He, Dongxiao and Wang, Zhen and Wang, Chuan and Cao, Xiaochun and Guo, Yuanfang},
  date = {2022-12-06},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {9249--9261},
  url = {https://proceedings.neurips.cc//paper_files/paper/2022/hash/3c2b60a3f269c404e9329ee119f2d34a-Abstract-Conference.html},
  urldate = {2023-05-06},
  langid = {english},
  keywords = {ego-network},
  file = {D:\Zotero\literature_data\storage\CGXI57Z3\Yang Á≠â - 2022 - OPEN Orthogonal Propagation with Ego-Network Mode.pdf}
}

@online{yang2022b,
  title = {Feature {{Learning}} in {{Infinite-Width Neural Networks}}},
  author = {Yang, Greg and Hu, Edward J.},
  date = {2022-07-15},
  eprint = {2011.14522},
  eprinttype = {arxiv},
  eprintclass = {cond-mat},
  url = {http://arxiv.org/abs/2011.14522},
  urldate = {2023-06-04},
  abstract = {As its width tends to infinity, a deep neural network's behavior under gradient descent can become simplified and predictable (e.g. given by the Neural Tangent Kernel (NTK)), if it is parametrized appropriately (e.g. the NTK parametrization). However, we show that the standard and NTK parametrizations of a neural network do not admit infinite-width limits that can learn features, which is crucial for pretraining and transfer learning such as with BERT. We propose simple modifications to the standard parametrization to allow for feature learning in the limit. Using the *Tensor Programs* technique, we derive explicit formulas for such limits. On Word2Vec and few-shot learning on Omniglot via MAML, two canonical tasks that rely crucially on feature learning, we compute these limits exactly. We find that they outperform both NTK baselines and finite-width networks, with the latter approaching the infinite-width feature learning performance as width increases. More generally, we classify a natural space of neural network parametrizations that generalizes standard, NTK, and Mean Field parametrizations. We show 1) any parametrization in this space either admits feature learning or has an infinite-width training dynamics given by kernel gradient descent, but not both; 2) any such infinite-width limit can be computed using the Tensor Programs technique. Code for our experiments can be found at github.com/edwardjhu/TP4.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Condensed Matter - Disordered Systems and Neural Networks},
  file = {D\:\\Zotero\\literature_data\\storage\\9M5P5WRC\\Yang Âíå Hu - 2022 - Feature Learning in Infinite-Width Neural Networks.pdf;D\:\\Zotero\\literature_data\\storage\\ATCSH7L9\\2011.html}
}

@online{yang2023,
  title = {The {{Dawn}} of {{LMMs}}: {{Preliminary Explorations}} with {{GPT-4V}}(Ision)},
  shorttitle = {The {{Dawn}} of {{LMMs}}},
  author = {Yang, Zhengyuan and Li, Linjie and Lin, Kevin and Wang, Jianfeng and Lin, Chung-Ching and Liu, Zicheng and Wang, Lijuan},
  date = {2023-10-11},
  eprint = {2309.17421},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2309.17421},
  urldate = {2023-10-19},
  abstract = {Large multimodal models (LMMs) extend large language models (LLMs) with multi-sensory skills, such as visual understanding, to achieve stronger generic intelligence. In this paper, we analyze the latest model, GPT-4V(ision), to deepen the understanding of LMMs. The analysis focuses on the intriguing tasks that GPT-4V can perform, containing test samples to probe the quality and genericity of GPT-4V's capabilities, its supported inputs and working modes, and the effective ways to prompt the model. In our approach to exploring GPT-4V, we curate and organize a collection of carefully designed qualitative samples spanning a variety of domains and tasks. Observations from these samples demonstrate that GPT-4V's unprecedented ability in processing arbitrarily interleaved multimodal inputs and the genericity of its capabilities together make GPT-4V a powerful multimodal generalist system. Furthermore, GPT-4V's unique capability of understanding visual markers drawn on input images can give rise to new human-computer interaction methods such as visual referring prompting. We conclude the report with in-depth discussions on the emerging application scenarios and the future research directions for GPT-4V-based systems. We hope that this preliminary exploration will inspire future research on the next-generation multimodal task formulation, new ways to exploit and enhance LMMs to solve real-world problems, and gaining better understanding of multimodal foundation models. Finally, we acknowledge that the model under our study is solely the product of OpenAI's innovative work, and they should be fully credited for its development. Please see the GPT-4V contributions paper for the authorship and credit attribution: https://cdn.openai.com/contributions/gpt-4v.pdf},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,GPT-4V},
  file = {D:\Zotero\literature_data\storage\WMX4Y6QX\2309.html}
}

@article{yang2023a,
  title = {Multi-Level {{Transitional Contrast Learning}} for {{Personalized Image Aesthetics Assessment}}},
  author = {Yang, Zhichao and Li, Leida and Yang, Yuzhe and Li, Yaqian and Lin, Weisi},
  date = {2023},
  journaltitle = {IEEE Transactions on Multimedia},
  shortjournal = {IEEE Trans. Multimedia},
  pages = {1--13},
  issn = {1520-9210, 1941-0077},
  doi = {10.1109/TMM.2023.3290479},
  url = {https://ieeexplore.ieee.org/document/10168279/},
  urldate = {2023-08-02},
  abstract = {Personalized image aesthetics assessment (PIAA) is aimed at modeling the unique aesthetic preferences of individuals, based on which personalized aesthetic scores are predicted. People have different standards for image aesthetics, and accordingly, images rated at the same aesthetic level by different users explicitly reveal their aesthetic preferences. However, previous PIAA models treat each individual as an isolated optimization target, failing to take full advantage of the contrastive information among users. Further, although people‚Äôs aesthetic preferences are unique, they still share some commonalities, meaning that PIAA models could be built on the basis of generic aesthetics. Motivated by the above facts, this paper presents a Multi-level Transitional Contrast Learning (MTCL) framework for PIAA by transiting features from generic aesthetics to personalized aesthetics via contrastive learning. First, a generic image aesthetics assessment network is pre-trained to learn the common aesthetic features. Then, image sets rated to have the same aesthetic levels by different users are employed to learn the differentiated aesthetic features through multiple level-wise contrast learning based on the generic aesthetic features. Finally, a target user‚Äôs PIAA model is built by integrating generic and differentiated aesthetic features. Extensive experiments on four benchmark PIAA databases demonstrate that the proposed MTCL model outperforms the state-of-the-arts.},
  langid = {english},
  file = {D:\Zotero\literature_data\storage\UDK7FSZY\Yang Á≠â - 2023 - Multi-level Transitional Contrast Learning for Per.pdf}
}

@online{yao2023,
  title = {Research without {{Re-search}}: {{Maximal Update Parametrization Yields Accurate Loss Prediction}} across {{Scales}}},
  shorttitle = {Research without {{Re-search}}},
  author = {Yao, Yiqun and Wang, Yequan},
  date = {2023-04-28},
  eprint = {2304.06875},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.06875},
  url = {http://arxiv.org/abs/2304.06875},
  urldate = {2023-05-08},
  abstract = {As language models scale up, it becomes increasingly expensive to verify research ideas because conclusions on small models do not trivially transfer to large ones. A possible solution is to establish a generic system that directly predicts some metrics for large models solely based on the results and hyperparameters from small models. Existing methods based on scaling laws require hyperparameter search on the largest models, which is impractical with limited resources. We address this issue by presenting our discoveries indicating that Maximal Update parametrization (muP) enables accurate fitting of scaling laws for hyperparameters close to common loss basins, without any search. Thus, different models can be directly compared on large scales with loss prediction even before the training starts. We propose a new paradigm as a first step towards reliable academic research for any model scale without heavy computation. Code will be publicly available shortly.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Mu-scaling},
  file = {D\:\\Zotero\\literature_data\\storage\\KYXBW62B\\Yao Âíå Wang - 2023 - Research without Re-search Maximal Update Paramet.pdf;D\:\\Zotero\\literature_data\\storage\\2UKNCPXL\\2304.html}
}

@article{yi,
  title = {Towards {{Artistic Image Aesthetics Assessment}}: {{A Large-Scale Dataset}} and a {{New Method}}},
  author = {Yi, Ran and Tian, Haoyuan and Gu, Zhihao and Lai, Yu-Kun and Rosin, Paul L},
  langid = {english},
  file = {D:\Zotero\literature_data\storage\JL2D87FK\Yi Á≠â - Towards Artistic Image Aesthetics Assessment A La.pdf}
}

@online{yosinski2014,
  title = {How Transferable Are Features in Deep Neural Networks?},
  author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
  date = {2014-11-06},
  url = {https://arxiv.org/abs/1411.1792v1},
  urldate = {2023-05-17},
  abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.},
  langid = {english},
  organization = {{arXiv.org}},
  file = {D:\Zotero\literature_data\storage\3KW92AEL\Yosinski Á≠â - 2014 - How transferable are features in deep neural netwo.pdf}
}

@inproceedings{yosinski2014a,
  title = {How Transferable Are Features in Deep Neural Networks?},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
  date = {2014},
  volume = {27},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2014/hash/375c71349b295fbe2dcdca9206f20a06-Abstract.html},
  urldate = {2023-05-17},
  abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.},
  file = {D:\Zotero\literature_data\storage\8PWPAPN8\Yosinski Á≠â - 2014 - How transferable are features in deep neural netwo.pdf}
}

@online{you2021,
  title = {Graph {{Contrastive Learning}} with {{Augmentations}}},
  author = {You, Yuning and Chen, Tianlong and Sui, Yongduo and Chen, Ting and Wang, Zhangyang and Shen, Yang},
  date = {2021-04-03},
  eprint = {2010.13902},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2010.13902},
  urldate = {2023-04-23},
  abstract = {Generalizable, transferrable, and robust representation learning on graph-structured data remains a challenge for current graph neural networks (GNNs). Unlike what has been developed for convolutional neural networks (CNNs) for image data, self-supervised learning and pre-training are less explored for GNNs. In this paper, we propose a graph contrastive learning (GraphCL) framework for learning unsupervised representations of graph data. We first design four types of graph augmentations to incorporate various priors. We then systematically study the impact of various combinations of graph augmentations on multiple datasets, in four different settings: semi-supervised, unsupervised, and transfer learning as well as adversarial attacks. The results show that, even without tuning augmentation extents nor using sophisticated GNN architectures, our GraphCL framework can produce graph representations of similar or better generalizability, transferrability, and robustness compared to state-of-the-art methods. We also investigate the impact of parameterized graph augmentation extents and patterns, and observe further performance gains in preliminary experiments. Our codes are available at: https://github.com/Shen-Lab/GraphCL.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {D:\Zotero\literature_data\storage\ZLP9K5Z9\You Á≠â - 2021 - Graph Contrastive Learning with Augmentations.pdf}
}

@online{yue2023,
  title = {Automatic {{Evaluation}} of {{Attribution}} by {{Large Language Models}}},
  author = {Yue, Xiang and Wang, Boshi and Chen, Ziru and Zhang, Kai and Su, Yu and Sun, Huan},
  date = {2023-10-07},
  eprint = {2305.06311},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2305.06311},
  urldate = {2023-11-29},
  abstract = {A recent focus of large language model (LLM) development, as exemplified by generative search engines, is to incorporate external references to generate and support its claims. However, evaluating the attribution, i.e., verifying whether the generated statement is fully supported by the cited reference, remains an open problem. Although human evaluation is common practice, it is costly and time-consuming. In this paper, we investigate the automatic evaluation of attribution given by LLMs. We begin by defining different types of attribution errors, and then explore two approaches for automatic evaluation: prompting LLMs and fine-tuning smaller LMs. The fine-tuning data is repurposed from related tasks such as question answering, fact-checking, natural language inference, and summarization. We manually curate a set of test examples covering 12 domains from a generative search engine, New Bing. Our results on this curated test set and simulated examples from existing benchmarks highlight both promising signals and challenges. We hope our problem formulation, testbeds, and findings will help lay the foundation for future studies on this important problem.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\V9QMIJE9\\Yue Á≠â - 2023 - Automatic Evaluation of Attribution by Large Langu.pdf;D\:\\Zotero\\literature_data\\storage\\7T78BGAM\\2305.html}
}

@online{yue2023a,
  title = {Automatic {{Evaluation}} of {{Attribution}} by {{Large Language Models}}},
  author = {Yue, Xiang and Wang, Boshi and Chen, Ziru and Zhang, Kai and Su, Yu and Sun, Huan},
  date = {2023-10-07},
  eprint = {2305.06311},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2305.06311},
  urldate = {2023-10-25},
  abstract = {A recent focus of large language model (LLM) development, as exemplified by generative search engines, is to incorporate external references to generate and support its claims. However, evaluating the attribution, i.e., verifying whether the generated statement is fully supported by the cited reference, remains an open problem. Although human evaluation is common practice, it is costly and time-consuming. In this paper, we investigate the automatic evaluation of attribution given by LLMs. We begin by defining different types of attribution errors, and then explore two approaches for automatic evaluation: prompting LLMs and fine-tuning smaller LMs. The fine-tuning data is repurposed from related tasks such as question answering, fact-checking, natural language inference, and summarization. We manually curate a set of test examples covering 12 domains from a generative search engine, New Bing. Our results on this curated test set and simulated examples from existing benchmarks highlight both promising signals and challenges. We hope our problem formulation, testbeds, and findings will help lay the foundation for future studies on this important problem.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\QBIADP73\\Yue Á≠â - 2023 - Automatic Evaluation of Attribution by Large Langu.pdf;D\:\\Zotero\\literature_data\\storage\\YYXYESN8\\2305.html}
}

@online{yun2020,
  title = {Graph {{Transformer Networks}}},
  author = {Yun, Seongjun and Jeong, Minbyul and Kim, Raehyun and Kang, Jaewoo and Kim, Hyunwoo J.},
  date = {2020-02-04},
  eprint = {1911.06455},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1911.06455},
  urldate = {2023-07-01},
  abstract = {Graph neural networks (GNNs) have been widely used in representation learning on graphs and achieved state-of-the-art performance in tasks such as node classification and link prediction. However, most existing GNNs are designed to learn node representations on the fixed and homogeneous graphs. The limitations especially become problematic when learning representations on a misspecified graph or a heterogeneous graph that consists of various types of nodes and edges. In this paper, we propose Graph Transformer Networks (GTNs) that are capable of generating new graph structures, which involve identifying useful connections between unconnected nodes on the original graph, while learning effective node representation on the new graphs in an end-to-end fashion. Graph Transformer layer, a core layer of GTNs, learns a soft selection of edge types and composite relations for generating useful multi-hop connections so-called meta-paths. Our experiments show that GTNs learn new graph structures, based on data and tasks without domain knowledge, and yield powerful node representation via convolution on the new graphs. Without domain-specific graph preprocessing, GTNs achieved the best performance in all three benchmark node classification tasks against the state-of-the-art methods that require pre-defined meta-paths from domain knowledge.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks,Graph Transformer,Statistics - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\ZKN8Y6AA\\Yun Á≠â - 2020 - Graph Transformer Networks.pdf;D\:\\Zotero\\literature_data\\storage\\K5BNCSKV\\1911.html}
}

@online{zbontar2021,
  title = {Barlow {{Twins}}: {{Self-Supervised Learning}} via {{Redundancy Reduction}}},
  shorttitle = {Barlow {{Twins}}},
  author = {Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, St√©phane},
  date = {2021-06-14},
  eprint = {2103.03230},
  eprinttype = {arxiv},
  eprintclass = {cs, q-bio},
  doi = {10.48550/arXiv.2103.03230},
  url = {http://arxiv.org/abs/2103.03230},
  urldate = {2023-07-22},
  abstract = {Self-supervised learning (SSL) is rapidly closing the gap with supervised methods on large computer vision benchmarks. A successful approach to SSL is to learn embeddings which are invariant to distortions of the input sample. However, a recurring issue with this approach is the existence of trivial constant solutions. Most current methods avoid such solutions by careful implementation details. We propose an objective function that naturally avoids collapse by measuring the cross-correlation matrix between the outputs of two identical networks fed with distorted versions of a sample, and making it as close to the identity matrix as possible. This causes the embedding vectors of distorted versions of a sample to be similar, while minimizing the redundancy between the components of these vectors. The method is called Barlow Twins, owing to neuroscientist H. Barlow's redundancy-reduction principle applied to a pair of identical networks. Barlow Twins does not require large batches nor asymmetry between the network twins such as a predictor network, gradient stopping, or a moving average on the weight updates. Intriguingly it benefits from very high-dimensional output vectors. Barlow Twins outperforms previous methods on ImageNet for semi-supervised classification in the low-data regime, and is on par with current state of the art for ImageNet classification with a linear classifier head, and for transfer tasks of classification and object detection.},
  pubstate = {preprint},
  keywords = {Barlow Twins,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition},
  file = {D\:\\Zotero\\literature_data\\storage\\N9HIXBQQ\\Zbontar Á≠â - 2021 - Barlow Twins Self-Supervised Learning via Redunda.pdf;D\:\\Zotero\\literature_data\\storage\\JP47YGMB\\2103.html}
}

@online{zellers2018,
  title = {{{SWAG}}: {{A Large-Scale Adversarial Dataset}} for {{Grounded Commonsense Inference}}},
  shorttitle = {{{SWAG}}},
  author = {Zellers, Rowan and Bisk, Yonatan and Schwartz, Roy and Choi, Yejin},
  date = {2018-08-16},
  url = {https://arxiv.org/abs/1808.05326v1},
  urldate = {2023-05-17},
  abstract = {Given a partial description like "she opened the hood of the car," humans can reason about the situation and anticipate what might come next ("then, she examined the engine"). In this paper, we introduce the task of grounded commonsense inference, unifying natural language inference and commonsense reasoning. We present SWAG, a new dataset with 113k multiple choice questions about a rich spectrum of grounded situations. To address the recurring challenges of the annotation artifacts and human biases found in many existing datasets, we propose Adversarial Filtering (AF), a novel procedure that constructs a de-biased dataset by iteratively training an ensemble of stylistic classifiers, and using them to filter the data. To account for the aggressive adversarial filtering, we use state-of-the-art language models to massively oversample a diverse set of potential counterfactuals. Empirical results demonstrate that while humans can solve the resulting inference problems with high accuracy (88\%), various competitive models struggle on our task. We provide comprehensive analysis that indicates significant opportunities for future research.},
  langid = {english},
  organization = {{arXiv.org}},
  file = {D:\Zotero\literature_data\storage\73IWE3HV\Zellers Á≠â - 2018 - SWAG A Large-Scale Adversarial Dataset for Ground.pdf}
}

@article{zeng,
  title = {{{MAPLE}}: {{Micro Analysis}} of {{Pairwise Language Evolution}} for {{Few-Shot Claim Verification}}},
  author = {Zeng, Xia and Zubiaga, Arkaitz},
  abstract = {Claim verification is an essential step in the automated fact-checking pipeline which assesses the veracity of a claim against a piece of evidence. In this work, we explore the potential of few-shot claim verification, where only very limited data is available for supervision. We propose MAPLE (Micro Analysis of Pairwise Language Evolution), a pioneering approach that explores the alignment between a claim and its evidence with a small seq2seq model and a novel semantic measure. Its innovative utilization of micro language evolution path leverages unlabelled pairwise data to facilitate claim verification while imposing low demand on data annotations and computing resources. MAPLE demonstrates significant performance improvements over SOTA baselines SEED, PET and LLaMA 2 across three factchecking datasets: FEVER, Climate FEVER, and SciFact. Data and code are available here.},
  langid = {english},
  file = {D:\Zotero\literature_data\storage\UEUWWHIK\Zeng Âíå Zubiaga - MAPLE Micro Analysis of Pairwise Language Evoluti.pdf}
}

@online{zeng2021,
  title = {{{PanGu-}}\$\textbackslash alpha\$: {{Large-scale Autoregressive Pretrained Chinese Language Models}} with {{Auto-parallel Computation}}},
  shorttitle = {{{PanGu-}}\$\textbackslash alpha\$},
  author = {Zeng, Wei and Ren, Xiaozhe and Su, Teng and Wang, Hui and Liao, Yi and Wang, Zhiwei and Jiang, Xin and Yang, ZhenZhang and Wang, Kaisheng and Zhang, Xiaoda and Li, Chen and Gong, Ziyan and Yao, Yifan and Huang, Xinjing and Wang, Jun and Yu, Jianfeng and Guo, Qi and Yu, Yue and Zhang, Yan and Wang, Jin and Tao, Hengtao and Yan, Dasen and Yi, Zexuan and Peng, Fang and Jiang, Fangqing and Zhang, Han and Deng, Lingfeng and Zhang, Yehong and Lin, Zhe and Zhang, Chao and Zhang, Shaojie and Guo, Mingyue and Gu, Shanzhi and Fan, Gaojun and Wang, Yaowei and Jin, Xuefeng and Liu, Qun and Tian, Yonghong},
  date = {2021-04-26},
  eprint = {2104.12369},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2104.12369},
  url = {http://arxiv.org/abs/2104.12369},
  urldate = {2023-06-03},
  abstract = {Large-scale Pretrained Language Models (PLMs) have become the new paradigm for Natural Language Processing (NLP). PLMs with hundreds of billions parameters such as GPT-3 have demonstrated strong performances on natural language understanding and generation with \textbackslash textit\{few-shot in-context\} learning. In this work, we present our practice on training large-scale autoregressive language models named PanGu-\$\textbackslash alpha\$, with up to 200 billion parameters. PanGu-\$\textbackslash alpha\$ is developed under the MindSpore and trained on a cluster of 2048 Ascend 910 AI processors. The training parallelism strategy is implemented based on MindSpore Auto-parallel, which composes five parallelism dimensions to scale the training task to 2048 processors efficiently, including data parallelism, op-level model parallelism, pipeline model parallelism, optimizer model parallelism and rematerialization. To enhance the generalization ability of PanGu-\$\textbackslash alpha\$, we collect 1.1TB high-quality Chinese data from a wide range of domains to pretrain the model. We empirically test the generation ability of PanGu-\$\textbackslash alpha\$ in various scenarios including text summarization, question answering, dialogue generation, etc. Moreover, we investigate the effect of model scales on the few-shot performances across a broad range of Chinese NLP tasks. The experimental results demonstrate the superior capabilities of PanGu-\$\textbackslash alpha\$ in performing various tasks under few-shot or zero-shot settings.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\H2TRRPXV\\Zeng Á≠â - 2021 - PanGu-$alpha$ Large-scale Autoregressive Pretrai.pdf;D\:\\Zotero\\literature_data\\storage\\H6HMNT4M\\2104.html}
}

@online{zeng2022,
  title = {{{GLM-130B}}: {{An Open Bilingual Pre-trained Model}}},
  shorttitle = {{{GLM-130B}}},
  author = {Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and Tam, Weng Lam and Ma, Zixuan and Xue, Yufei and Zhai, Jidong and Chen, Wenguang and Zhang, Peng and Dong, Yuxiao and Tang, Jie},
  date = {2022-10-05},
  eprint = {2210.02414},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2210.02414},
  url = {http://arxiv.org/abs/2210.02414},
  urldate = {2023-06-03},
  abstract = {We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model at least as good as GPT-3 and unveil how models of such a scale can be successfully pre-trained. Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and disconvergence. In this paper, we introduce the training process of GLM-130B including its design choices, training strategies for both efficiency and stability, and engineering efforts. The resultant GLM-130B model offers significant outperformance over GPT-3 175B on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0 260B -- the largest Chinese language model -- across related benchmarks. Finally, we leverage a unique scaling property of GLM-130B to reach INT4 quantization, without quantization aware training and with almost no performance loss, making it the first among 100B-scale models. More importantly, the property allows its effective inference on 4\$\textbackslash times\$RTX 3090 (24G) or 8\$\textbackslash times\$RTX 2080 Ti (11G) GPUs, the most ever affordable GPUs required for using 100B-scale models. The GLM-130B model weights are publicly accessible and its code, training logs, related toolkit, and lessons learned are open-sourced at https://github.com/THUDM/GLM-130B .},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\B98K4SC5\\Zeng Á≠â - 2022 - GLM-130B An Open Bilingual Pre-trained Model.pdf;D\:\\Zotero\\literature_data\\storage\\DU9L4KRS\\2210.html}
}

@article{zhai2020,
  title = {Perceptual Image Quality Assessment: A Survey},
  shorttitle = {Perceptual Image Quality Assessment},
  author = {Zhai, Guangtao and Min, Xiongkuo},
  date = {2020-11},
  journaltitle = {Science China Information Sciences},
  shortjournal = {Sci. China Inf. Sci.},
  volume = {63},
  number = {11},
  pages = {211301},
  issn = {1674-733X, 1869-1919},
  doi = {10.1007/s11432-019-2757-1},
  url = {https://link.springer.com/10.1007/s11432-019-2757-1},
  urldate = {2023-07-25},
  abstract = {Perceptual quality assessment plays a vital role in the visual communication systems owing to the existence of quality degradations introduced in various stages of visual signal acquisition, compression, transmission and display. Quality assessment for visual signals can be performed subjectively and objectively, and objective quality assessment is usually preferred owing to its high efficiency and easy deployment. A large number of subjective and objective visual quality assessment studies have been conducted during recent years. In this survey, we give an up-to-date and comprehensive review of these studies. Specifically, the frequently used subjective image quality assessment databases are first reviewed, as they serve as the validation set for the objective measures. Second, the objective image quality assessment measures are classified and reviewed according to the applications and the methodologies utilized in the quality measures. Third, the performances of the state-of-the-art quality measures for visual signals are compared with an introduction of the evaluation protocols. This survey provides a general overview of classical algorithms and recent progresses in the field of perceptual image quality assessment.},
  langid = {english},
  file = {D:\Zotero\literature_data\storage\CW2H2BEK\Zhai Âíå Min - 2020 - Perceptual image quality assessment a survey.pdf}
}

@article{zhang,
  title = {{{MD-VQA}}: {{Multi-Dimensional Quality Assessment}} for {{UGC Live Videos}}},
  author = {Zhang, Zicheng and Wu, Wei and Sun, Wei and Tu, Danyang and Lu, Wei and Min, Xiongkuo and Chen, Ying and Zhai, Guangtao},
  abstract = {User-generated content (UGC) live videos are often bothered by various distortions during capture procedures and thus exhibit diverse visual qualities. Such source videos are further compressed and transcoded by media server providers before being distributed to end-users. Because of the flourishing of UGC live videos, effective video quality assessment (VQA) tools are needed to monitor and perceptually optimize live streaming videos in the distributing process. In this paper, we address UGC Live VQA problems by constructing a first-of-a-kind subjective UGC Live VQA database and developing an effective evaluation tool. Concretely, 418 source UGC videos are collected in real live streaming scenarios and 3,762 compressed ones at different bit rates are generated for the subsequent subjective VQA experiments. Based on the built database, we develop a Multi-Dimensional VQA (MD-VQA) evaluator to measure the visual quality of UGC live videos from semantic, distortion, and motion aspects respectively. Extensive experimental results show that MD-VQA achieves state-ofthe-art performance on both our UGC Live VQA database and existing compressed UGC VQA databases.},
  langid = {english},
  file = {D:\Zotero\literature_data\storage\RDUF8HEU\Zhang Á≠â - MD-VQA Multi-Dimensional Quality Assessment for U.pdf}
}

@online{zhang2021,
  title = {From {{Canonical Correlation Analysis}} to {{Self-supervised Graph Neural Networks}}},
  author = {Zhang, Hengrui and Wu, Qitian and Yan, Junchi and Wipf, David and Yu, Philip S.},
  date = {2021-10-27},
  eprint = {2106.12484},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2106.12484},
  urldate = {2023-05-26},
  abstract = {We introduce a conceptually simple yet effective model for self-supervised representation learning with graph data. It follows the previous methods that generate two views of an input graph through data augmentation. However, unlike contrastive methods that focus on instance-level discrimination, we optimize an innovative feature-level objective inspired by classical Canonical Correlation Analysis. Compared with other works, our approach requires none of the parameterized mutual information estimator, additional projector, asymmetric structures, and most importantly, negative samples which can be costly. We show that the new objective essentially 1) aims at discarding augmentation-variant information by learning invariant representations, and 2) can prevent degenerated solutions by decorrelating features in different dimensions. Our theoretical analysis further provides an understanding for the new objective which can be equivalently seen as an instantiation of the Information Bottleneck Principle under the self-supervised setting. Despite its simplicity, our method performs competitively on seven public graph datasets. The code is available at: https://github.com/hengruizhang98/CCA-SSG.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\87MK9EJN\\Zhang Á≠â - 2021 - From Canonical Correlation Analysis to Self-superv.pdf;D\:\\Zotero\\literature_data\\storage\\ASCFCYY9\\2106.html}
}

@inproceedings{zhang2021a,
  title = {From {{Canonical Correlation Analysis}} to {{Self-supervised Graph Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhang, Hengrui and Wu, Qitian and Yan, Junchi and Wipf, David and Yu, Philip S},
  date = {2021},
  volume = {34},
  pages = {76--89},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2021/hash/00ac8ed3b4327bdd4ebbebcb2ba10a00-Abstract.html},
  urldate = {2023-07-30},
  abstract = {We introduce a conceptually simple yet effective model for self-supervised representation learning with graph data. It follows the previous methods that generate two views of an input graph through data augmentation. However, unlike contrastive methods that focus on instance-level discrimination, we optimize an innovative feature-level objective inspired by classical Canonical Correlation Analysis. Compared with other works, our approach requires none of the parameterized mutual information estimator, additional projector, asymmetric structures, and most importantly, negative samples which can be costly. We show that the new objective essentially 1) aims at discarding augmentation-variant information by learning invariant representations, and 2) can prevent degenerated solutions by decorrelating features in different dimensions. Our theoretical analysis further provides an understanding for the new objective which can be equivalently seen as an instantiation of the Information Bottleneck Principle under the self-supervised setting. Despite its simplicity, our method performs competitively on seven public graph datasets.},
  file = {D:\Zotero\literature_data\storage\2CUV9T5F\Zhang Á≠â - 2021 - From Canonical Correlation Analysis to Self-superv.pdf}
}

@inproceedings{zhang2021b,
  title = {From {{Canonical Correlation Analysis}} to {{Self-supervised Graph Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhang, Hengrui and Wu, Qitian and Yan, Junchi and Wipf, David and Yu, Philip S},
  date = {2021},
  volume = {34},
  pages = {76--89},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2021/hash/00ac8ed3b4327bdd4ebbebcb2ba10a00-Abstract.html},
  urldate = {2023-05-06},
  abstract = {We introduce a conceptually simple yet effective model for self-supervised representation learning with graph data. It follows the previous methods that generate two views of an input graph through data augmentation. However, unlike contrastive methods that focus on instance-level discrimination, we optimize an innovative feature-level objective inspired by classical Canonical Correlation Analysis. Compared with other works, our approach requires none of the parameterized mutual information estimator, additional projector, asymmetric structures, and most importantly, negative samples which can be costly. We show that the new objective essentially 1) aims at discarding augmentation-variant information by learning invariant representations, and 2) can prevent degenerated solutions by decorrelating features in different dimensions. Our theoretical analysis further provides an understanding for the new objective which can be equivalently seen as an instantiation of the Information Bottleneck Principle under the self-supervised setting. Despite its simplicity, our method performs competitively on seven public graph datasets.},
  keywords = {other GCL method},
  file = {D:\Zotero\literature_data\storage\V2VVDUMK\Zhang Á≠â - 2021 - From Canonical Correlation Analysis to Self-superv.pdf}
}

@inproceedings{zhang2021c,
  title = {From {{Canonical Correlation Analysis}} to {{Self-supervised Graph Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhang, Hengrui and Wu, Qitian and Yan, Junchi and Wipf, David and Yu, Philip S},
  date = {2021},
  volume = {34},
  pages = {76--89},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/00ac8ed3b4327bdd4ebbebcb2ba10a00-Abstract.html},
  urldate = {2023-05-06},
  abstract = {We introduce a conceptually simple yet effective model for self-supervised representation learning with graph data. It follows the previous methods that generate two views of an input graph through data augmentation. However, unlike contrastive methods that focus on instance-level discrimination, we optimize an innovative feature-level objective inspired by classical Canonical Correlation Analysis. Compared with other works, our approach requires none of the parameterized mutual information estimator, additional projector, asymmetric structures, and most importantly, negative samples which can be costly. We show that the new objective essentially 1) aims at discarding augmentation-variant information by learning invariant representations, and 2) can prevent degenerated solutions by decorrelating features in different dimensions. Our theoretical analysis further provides an understanding for the new objective which can be equivalently seen as an instantiation of the Information Bottleneck Principle under the self-supervised setting. Despite its simplicity, our method performs competitively on seven public graph datasets.},
  file = {D:\Zotero\literature_data\storage\5HMM8T7L\Zhang Á≠â - 2021 - From Canonical Correlation Analysis to Self-superv.pdf}
}

@online{zhang2022,
  title = {Hierarchical {{Graph Transformer}} with {{Adaptive Node Sampling}}},
  author = {Zhang, Zaixi and Liu, Qi and Hu, Qingyong and Lee, Chee-Kong},
  date = {2022-10-08},
  eprint = {2210.03930},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2210.03930},
  urldate = {2023-06-07},
  abstract = {The Transformer architecture has achieved remarkable success in a number of domains including natural language processing and computer vision. However, when it comes to graph-structured data, transformers have not achieved competitive performance, especially on large graphs. In this paper, we identify the main deficiencies of current graph transformers:(1) Existing node sampling strategies in Graph Transformers are agnostic to the graph characteristics and the training process. (2) Most sampling strategies only focus on local neighbors and neglect the long-range dependencies in the graph. We conduct experimental investigations on synthetic datasets to show that existing sampling strategies are sub-optimal. To tackle the aforementioned problems, we formulate the optimization strategies of node sampling in Graph Transformer as an adversary bandit problem, where the rewards are related to the attention weights and can vary in the training procedure. Meanwhile, we propose a hierarchical attention scheme with graph coarsening to capture the long-range interactions while reducing computational complexity. Finally, we conduct extensive experiments on real-world datasets to demonstrate the superiority of our method over existing graph transformers and popular GNNs.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,ego-netÈááÊ†∑},
  file = {D\:\\Zotero\\literature_data\\storage\\YM7WQGZS\\Zhang Á≠â - 2022 - Hierarchical Graph Transformer with Adaptive Node .pdf;D\:\\Zotero\\literature_data\\storage\\NYWTDH27\\2210.html}
}

@online{zhang2023,
  title = {Siren's {{Song}} in the {{AI Ocean}}: {{A Survey}} on {{Hallucination}} in {{Large Language Models}}},
  shorttitle = {Siren's {{Song}} in the {{AI Ocean}}},
  author = {Zhang, Yue and Li, Yafu and Cui, Leyang and Cai, Deng and Liu, Lemao and Fu, Tingchen and Huang, Xinting and Zhao, Enbo and Zhang, Yu and Chen, Yulong and Wang, Longyue and Luu, Anh Tuan and Bi, Wei and Shi, Freda and Shi, Shuming},
  date = {2023-09-24},
  eprint = {2309.01219},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2309.01219},
  url = {http://arxiv.org/abs/2309.01219},
  urldate = {2023-10-19},
  abstract = {While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning,ÂπªËßâ},
  file = {D\:\\Zotero\\literature_data\\storage\\SBS5HUZ3\\Zhang Á≠â - 2023 - Siren's Song in the AI Ocean A Survey on Hallucin.pdf;D\:\\Zotero\\literature_data\\storage\\RRYU769L\\2309.html}
}

@online{zhang2023a,
  title = {Instruction {{Tuning}} for {{Large Language Models}}: {{A Survey}}},
  shorttitle = {Instruction {{Tuning}} for {{Large Language Models}}},
  author = {Zhang, Shengyu and Dong, Linfeng and Li, Xiaoya and Zhang, Sen and Sun, Xiaofei and Wang, Shuhe and Li, Jiwei and Hu, Runyi and Zhang, Tianwei and Wu, Fei and Wang, Guoyin},
  date = {2023-10-09},
  eprint = {2308.10792},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2308.10792},
  url = {http://arxiv.org/abs/2308.10792},
  urldate = {2023-10-19},
  abstract = {This paper surveys research works in the quickly advancing field of instruction tuning (IT), a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of \textbackslash textsc\{(instruction, output)\} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies of existing strategies and suggest some avenues for fruitful research. Project page: github.com/xiaoya-li/Instruction-Tuning-Survey},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Êåá‰ª§ÂæÆË∞É},
  file = {D\:\\Zotero\\literature_data\\storage\\T9R4LSVA\\Zhang Á≠â - 2023 - Instruction Tuning for Large Language Models A Su.pdf;D\:\\Zotero\\literature_data\\storage\\YENEC8UB\\2308.html}
}

@online{zhang2023b,
  title = {A {{Perceptual Quality Assessment Exploration}} for {{AIGC Images}}},
  author = {Zhang, Zicheng and Li, Chunyi and Sun, Wei and Liu, Xiaohong and Min, Xiongkuo and Zhai, Guangtao},
  date = {2023-03-22},
  eprint = {2303.12618},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2303.12618},
  urldate = {2023-07-25},
  abstract = {\textbackslash underline\{AI\} \textbackslash underline\{G\}enerated \textbackslash underline\{C\}ontent (\textbackslash textbf\{AIGC\}) has gained widespread attention with the increasing efficiency of deep learning in content creation. AIGC, created with the assistance of artificial intelligence technology, includes various forms of content, among which the AI-generated images (AGIs) have brought significant impact to society and have been applied to various fields such as entertainment, education, social media, etc. However, due to hardware limitations and technical proficiency, the quality of AIGC images (AGIs) varies, necessitating refinement and filtering before practical use. Consequently, there is an urgent need for developing objective models to assess the quality of AGIs. Unfortunately, no research has been carried out to investigate the perceptual quality assessment for AGIs specifically. Therefore, in this paper, we first discuss the major evaluation aspects such as technical issues, AI artifacts, unnaturalness, discrepancy, and aesthetics for AGI quality assessment. Then we present the first perceptual AGI quality assessment database, AGIQA-1K, which consists of 1,080 AGIs generated from diffusion models. A well-organized subjective experiment is followed to collect the quality labels of the AGIs. Finally, we conduct a benchmark experiment to evaluate the performance of current image quality assessment (IQA) models.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {D\:\\Zotero\\literature_data\\storage\\BGZL8MGI\\Zhang Á≠â - 2023 - A Perceptual Quality Assessment Exploration for AI.pdf;D\:\\Zotero\\literature_data\\storage\\MCFBPWCY\\2303.html}
}

@inproceedings{zhao2021,
  title = {Self-{{Supervised Visual Representations Learning}} by {{Contrastive Mask Prediction}}},
  author = {Zhao, Yucheng and Wang, Guangting and Luo, Chong and Zeng, Wenjun and Zha, Zheng-Jun},
  date = {2021},
  pages = {10160--10169},
  url = {https://openaccess.thecvf.com/content/ICCV2021/html/Zhao_Self-Supervised_Visual_Representations_Learning_by_Contrastive_Mask_Prediction_ICCV_2021_paper.html},
  urldate = {2023-07-24},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  langid = {english},
  file = {D:\Zotero\literature_data\storage\R234RGDI\Zhao Á≠â - 2021 - Self-Supervised Visual Representations Learning by.pdf}
}

@online{zhao2021a,
  title = {Gophormer: {{Ego-Graph Transformer}} for {{Node Classification}}},
  shorttitle = {Gophormer},
  author = {Zhao, Jianan and Li, Chaozhuo and Wen, Qianlong and Wang, Yiqi and Liu, Yuming and Sun, Hao and Xie, Xing and Ye, Yanfang},
  date = {2021-10-25},
  eprint = {2110.13094},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2110.13094},
  urldate = {2023-05-06},
  abstract = {Transformers have achieved remarkable performance in a myriad of fields including natural language processing and computer vision. However, when it comes to the graph mining area, where graph neural network (GNN) has been the dominant paradigm, transformers haven't achieved competitive performance, especially on the node classification task. Existing graph transformer models typically adopt fully-connected attention mechanism on the whole input graph and thus suffer from severe scalability issues and are intractable to train in data insufficient cases. To alleviate these issues, we propose a novel Gophormer model which applies transformers on ego-graphs instead of full-graphs. Specifically, Node2Seq module is proposed to sample ego-graphs as the input of transformers, which alleviates the challenge of scalability and serves as an effective data augmentation technique to boost model performance. Moreover, different from the feature-based attention strategy in vanilla transformers, we propose a proximity-enhanced attention mechanism to capture the fine-grained structural bias. In order to handle the uncertainty introduced by the ego-graph sampling, we further propose a consistency regularization and a multi-sample inference strategy for stabilized training and testing, respectively. Extensive experiments on six benchmark datasets are conducted to demonstrate the superiority of Gophormer over existing graph transformers and popular GNNs, revealing the promising future of graph transformers.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,ego-network,ÊúÄÂÖàÁúã},
  file = {D\:\\Zotero\\literature_data\\storage\\LKCE2G7I\\Zhao Á≠â - 2021 - Gophormer Ego-Graph Transformer for Node Classifi.pdf;D\:\\Zotero\\literature_data\\storage\\54UT2YGH\\2110.html}
}

@online{zhao2022,
  title = {From {{Stars}} to {{Subgraphs}}: {{Uplifting Any GNN}} with {{Local Structure Awareness}}},
  shorttitle = {From {{Stars}} to {{Subgraphs}}},
  author = {Zhao, Lingxiao and Jin, Wei and Akoglu, Leman and Shah, Neil},
  date = {2022-04-20},
  eprint = {2110.03753},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2110.03753},
  urldate = {2023-08-27},
  abstract = {Message Passing Neural Networks (MPNNs) are a common type of Graph Neural Network (GNN), in which each node's representation is computed recursively by aggregating representations (messages) from its immediate neighbors akin to a star-shaped pattern. MPNNs are appealing for being efficient and scalable, how-ever their expressiveness is upper-bounded by the 1st-order Weisfeiler-Lehman isomorphism test (1-WL). In response, prior works propose highly expressive models at the cost of scalability and sometimes generalization performance. Our work stands between these two regimes: we introduce a general framework to uplift any MPNN to be more expressive, with limited scalability overhead and greatly improved practical performance. We achieve this by extending local aggregation in MPNNs from star patterns to general subgraph patterns (e.g.,k-egonets):in our framework, each node representation is computed as the encoding of a surrounding induced subgraph rather than encoding of immediate neighbors only (i.e. a star). We choose the subgraph encoder to be a GNN (mainly MPNNs, considering scalability) to design a general framework that serves as a wrapper to up-lift any GNN. We call our proposed method GNN-AK(GNN As Kernel), as the framework resembles a convolutional neural network by replacing the kernel with GNNs. Theoretically, we show that our framework is strictly more powerful than 1\&2-WL, and is not less powerful than 3-WL. We also design subgraph sampling strategies which greatly reduce memory footprint and improve speed while maintaining performance. Our method sets new state-of-the-art performance by large margins for several well-known graph ML tasks; specifically, 0.08 MAE on ZINC,74.79\% and 86.887\% accuracy on CIFAR10 and PATTERN respectively.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Rooted Random Walk Subgraph Sampling,Statistics - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\R6EZHUCJ\\Zhao Á≠â - 2022 - From Stars to Subgraphs Uplifting Any GNN with Lo.pdf;D\:\\Zotero\\literature_data\\storage\\AKD2DUIQ\\2110.html}
}

@online{zhao2022a,
  title = {From {{Stars}} to {{Subgraphs}}: {{Uplifting Any GNN}} with {{Local Structure Awareness}}},
  shorttitle = {From {{Stars}} to {{Subgraphs}}},
  author = {Zhao, Lingxiao and Jin, Wei and Akoglu, Leman and Shah, Neil},
  date = {2022-04-20},
  eprint = {2110.03753},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2110.03753},
  url = {http://arxiv.org/abs/2110.03753},
  urldate = {2023-05-19},
  abstract = {Message Passing Neural Networks (MPNNs) are a common type of Graph Neural Network (GNN), in which each node's representation is computed recursively by aggregating representations (messages) from its immediate neighbors akin to a star-shaped pattern. MPNNs are appealing for being efficient and scalable, how-ever their expressiveness is upper-bounded by the 1st-order Weisfeiler-Lehman isomorphism test (1-WL). In response, prior works propose highly expressive models at the cost of scalability and sometimes generalization performance. Our work stands between these two regimes: we introduce a general framework to uplift any MPNN to be more expressive, with limited scalability overhead and greatly improved practical performance. We achieve this by extending local aggregation in MPNNs from star patterns to general subgraph patterns (e.g.,k-egonets):in our framework, each node representation is computed as the encoding of a surrounding induced subgraph rather than encoding of immediate neighbors only (i.e. a star). We choose the subgraph encoder to be a GNN (mainly MPNNs, considering scalability) to design a general framework that serves as a wrapper to up-lift any GNN. We call our proposed method GNN-AK(GNN As Kernel), as the framework resembles a convolutional neural network by replacing the kernel with GNNs. Theoretically, we show that our framework is strictly more powerful than 1\&2-WL, and is not less powerful than 3-WL. We also design subgraph sampling strategies which greatly reduce memory footprint and improve speed while maintaining performance. Our method sets new state-of-the-art performance by large margins for several well-known graph ML tasks; specifically, 0.08 MAE on ZINC,74.79\% and 86.887\% accuracy on CIFAR10 and PATTERN respectively.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\ICLX8JKI\\Zhao Á≠â - 2022 - From Stars to Subgraphs Uplifting Any GNN with Lo.pdf;D\:\\Zotero\\literature_data\\storage\\GU6QEBF3\\2110.html}
}

@online{zhao2023,
  title = {Explainability for {{Large Language Models}}: {{A Survey}}},
  shorttitle = {Explainability for {{Large Language Models}}},
  author = {Zhao, Haiyan and Chen, Hanjie and Yang, Fan and Liu, Ninghao and Deng, Huiqi and Cai, Hengyi and Wang, Shuaiqiang and Yin, Dawei and Du, Mengnan},
  date = {2023-09-16},
  eprint = {2309.01029},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2309.01029},
  urldate = {2023-10-19},
  abstract = {Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications. Therefore, understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts. In this paper, we introduce a taxonomy of explainability techniques and provide a structured overview of methods for explaining Transformer-based language models. We categorize techniques based on the training paradigms of LLMs: traditional fine-tuning-based paradigm and prompting-based paradigm. For each paradigm, we summarize the goals and dominant approaches for generating local explanations of individual predictions and global explanations of overall model knowledge. We also discuss metrics for evaluating generated explanations, and discuss how explanations can be leveraged to debug models and improve performance. Lastly, we examine key challenges and emerging opportunities for explanation techniques in the era of LLMs in comparison to conventional machine learning models.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,ÂèØËß£ÈáäÊÄß},
  file = {D\:\\Zotero\\literature_data\\storage\\3M5DUH23\\Zhao Á≠â - 2023 - Explainability for Large Language Models A Survey.pdf;D\:\\Zotero\\literature_data\\storage\\PIAXU4F6\\2309.html}
}

@online{zhao2023a,
  title = {A {{Survey}} of {{Large Language Models}}},
  author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
  date = {2023-11-24},
  eprint = {2303.18223},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.18223},
  url = {http://arxiv.org/abs/2303.18223},
  urldate = {2023-11-29},
  abstract = {Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\8THBGQ6Q\\Zhao Á≠â - 2023 - A Survey of Large Language Models.pdf;D\:\\Zotero\\literature_data\\storage\\H5MC7NX7\\2303.html}
}

@online{zhao2023b,
  title = {A {{Survey}} of {{Large Language Models}}},
  author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
  date = {2023-05-07},
  eprint = {2303.18223},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2303.18223},
  urldate = {2023-06-01},
  abstract = {Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\GGHTFKSK\\Zhao Á≠â - 2023 - A Survey of Large Language Models.pdf;D\:\\Zotero\\literature_data\\storage\\BZQHR55M\\2303.html}
}

@online{zhao2023c,
  title = {A {{Survey}} of {{Large Language Models}}},
  author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
  date = {2023-11-24},
  eprint = {2303.18223},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2303.18223},
  urldate = {2024-02-04},
  abstract = {Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\K9AL3BUM\\Zhao Á≠â - 2023 - A Survey of Large Language Models.pdf;D\:\\Zotero\\literature_data\\storage\\WBKJPD69\\2303.html}
}

@online{zhong2023,
  title = {Poisoning {{Retrieval Corpora}} by {{Injecting Adversarial Passages}}},
  author = {Zhong, Zexuan and Huang, Ziqing and Wettig, Alexander and Chen, Danqi},
  date = {2023-10-29},
  eprint = {2310.19156},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2310.19156},
  url = {http://arxiv.org/abs/2310.19156},
  urldate = {2023-12-02},
  abstract = {Dense retrievers have achieved state-of-the-art performance in various information retrieval tasks, but to what extent can they be safely deployed in real-world applications? In this work, we propose a novel attack for dense retrieval systems in which a malicious user generates a small number of adversarial passages by perturbing discrete tokens to maximize similarity with a provided set of training queries. When these adversarial passages are inserted into a large retrieval corpus, we show that this attack is highly effective in fooling these systems to retrieve them for queries that were not seen by the attacker. More surprisingly, these adversarial passages can directly generalize to out-of-domain queries and corpora with a high success attack rate -- for instance, we find that 50 generated passages optimized on Natural Questions can mislead {$>$}94\% of questions posed in financial documents or online forums. We also benchmark and compare a range of state-of-the-art dense retrievers, both unsupervised and supervised. Although different systems exhibit varying levels of vulnerability, we show they can all be successfully attacked by injecting up to 500 passages, a small fraction compared to a retrieval corpus of millions of passages.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {D\:\\Zotero\\literature_data\\storage\\EZB5KRXN\\Zhong Á≠â - 2023 - Poisoning Retrieval Corpora by Injecting Adversari.pdf;D\:\\Zotero\\literature_data\\storage\\TZH4DI5A\\2310.html}
}

@online{zhong2023a,
  title = {{{MQuAKE}}: {{Assessing Knowledge Editing}} in {{Language Models}} via {{Multi-Hop Questions}}},
  shorttitle = {{{MQuAKE}}},
  author = {Zhong, Zexuan and Wu, Zhengxuan and Manning, Christopher D. and Potts, Christopher and Chen, Danqi},
  date = {2023-10-29},
  eprint = {2305.14795},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2305.14795},
  urldate = {2023-12-02},
  abstract = {The information stored in large language models (LLMs) falls out of date quickly, and retraining from scratch is often not an option. This has recently given rise to a range of techniques for injecting new facts through updating model weights. Current evaluation paradigms are extremely limited, mainly validating the recall of edited facts, but changing one fact should cause rippling changes to the model's related beliefs. If we edit the UK Prime Minister to now be Rishi Sunak, then we should get a different answer to Who is married to the British Prime Minister? In this work, we present a benchmark, MQuAKE (Multi-hop Question Answering for Knowledge Editing), comprising multi-hop questions that assess whether edited models correctly answer questions where the answer should change as an entailed consequence of edited facts. While we find that current knowledge-editing approaches can recall edited facts accurately, they fail catastrophically on the constructed multi-hop questions. We thus propose a simple memory-based approach, MeLLo, which stores all edited facts externally while prompting the language model iteratively to generate answers that are consistent with the edited facts. While MQuAKE remains challenging, we show that MeLLo scales well with LLMs (up to 175B) and outperforms previous model editors by a large margin.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {D\:\\Zotero\\literature_data\\storage\\6ADWPCPK\\Zhong Á≠â - 2023 - MQuAKE Assessing Knowledge Editing in Language Mo.pdf;D\:\\Zotero\\literature_data\\storage\\48CI32SI\\2305.html}
}

@online{zhou2022,
  title = {Data {{Augmentation}} on {{Graphs}}: {{A Technical Survey}}},
  shorttitle = {Data {{Augmentation}} on {{Graphs}}},
  author = {Zhou, Jiajun and Xie, Chenxuan and Wen, Zhenyu and Zhao, Xiangyu and Xuan, Qi},
  date = {2022-12-19},
  eprint = {2212.09970},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2212.09970},
  urldate = {2023-08-23},
  abstract = {In recent years, graph representation learning has achieved remarkable success while suffering from low-quality data problems. As a mature technology to improve data quality in computer vision, data augmentation has also attracted increasing attention in graph domain. For promoting the development of this emerging research direction, in this survey, we comprehensively review and summarize the existing graph data augmentation (GDAug) techniques. Specifically, we first summarize a variety of feasible taxonomies, and then classify existing GDAug studies based on fine-grained graph elements. Furthermore, for each type of GDAug technique, we formalize the general definition, discuss the technical details, and give schematic illustration. In addition, we also summarize common performance metrics and specific design metrics for constructing a GDAug evaluation system. Finally, we summarize the applications of GDAug from both data and model levels, as well as future directions. Latest advances in GDAug are summarized in a GitHub repository: https://github.com/jjzhou012/GDAug-Survey.},
  pubstate = {preprint},
  version = {1},
  keywords = {Computer Science - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\X453YB2V\\Zhou Á≠â - 2022 - Data Augmentation on Graphs A Technical Survey.pdf;D\:\\Zotero\\literature_data\\storage\\6JXZGVU6\\2212.html}
}

@online{zhu2020,
  title = {Deep {{Graph Contrastive Representation Learning}}},
  author = {Zhu, Yanqiao and Xu, Yichen and Yu, Feng and Liu, Qiang and Wu, Shu and Wang, Liang},
  date = {2020-07-13},
  eprint = {2006.04131},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.04131},
  urldate = {2023-04-23},
  abstract = {Graph representation learning nowadays becomes fundamental in analyzing graphstructured data. Inspired by recent success of contrastive methods, in this paper, we propose a novel framework for unsupervised graph representation learning by leveraging a contrastive objective at the node level. Specifically, we generate two graph views by corruption and learn node representations by maximizing the agreement of node representations in these two views. To provide diverse node contexts for the contrastive objective, we propose a hybrid scheme for generating graph views on both structure and attribute levels. Besides, we provide theoretical justification behind our motivation from two perspectives, mutual information and the classical triplet loss. We perform empirical experiments on both transductive and inductive learning tasks using a variety of real-world datasets. Experimental experiments demonstrate that despite its simplicity, our proposed method consistently outperforms existing state-of-the-art methods by large margins. Moreover, our unsupervised method even surpasses its supervised counterparts on transductive tasks, demonstrating its great potential in real-world applications.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,GRACE,Statistics - Machine Learning},
  file = {D:\Zotero\literature_data\storage\AKQY3G98\Zhu Á≠â - 2020 - Deep Graph Contrastive Representation Learning.pdf}
}

@online{zhu2021,
  title = {Transfer {{Learning}} of {{Graph Neural Networks}} with {{Ego-graph Information Maximization}}},
  author = {Zhu, Qi and Yang, Carl and Xu, Yidan and Wang, Haonan and Zhang, Chao and Han, Jiawei},
  date = {2021-10-26},
  eprint = {2009.05204},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2009.05204},
  url = {http://arxiv.org/abs/2009.05204},
  urldate = {2023-05-11},
  abstract = {Graph neural networks (GNNs) have achieved superior performance in various applications, but training dedicated GNNs can be costly for large-scale graphs. Some recent work started to study the pre-training of GNNs. However, none of them provide theoretical insights into the design of their frameworks, or clear requirements and guarantees towards their transferability. In this work, we establish a theoretically grounded and practically useful framework for the transfer learning of GNNs. Firstly, we propose a novel view towards the essential graph information and advocate the capturing of it as the goal of transferable GNN training, which motivates the design of EGI (Ego-Graph Information maximization) to analytically achieve this goal. Secondly, when node features are structure-relevant, we conduct an analysis of EGI transferability regarding the difference between the local graph Laplacians of the source and target graphs. We conduct controlled synthetic experiments to directly justify our theoretical conclusions. Comprehensive experiments on two real-world network datasets show consistent results in the analyzed setting of direct-transfering, while those on large-scale knowledge graphs show promising results in the more practical setting of transfering with fine-tuning.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,ego-network,Statistics - Machine Learning},
  file = {D\:\\Zotero\\literature_data\\storage\\GSY62NTV\\Zhu Á≠â - 2021 - Transfer Learning of Graph Neural Networks with Eg.pdf;D\:\\Zotero\\literature_data\\storage\\RKV5VVRI\\2009.html}
}

@inproceedings{zhu2021a,
  title = {Transfer {{Learning}} of {{Graph Neural Networks}} with {{Ego-graph Information Maximization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhu, Qi and Yang, Carl and Xu, Yidan and Wang, Haonan and Zhang, Chao and Han, Jiawei},
  date = {2021},
  volume = {34},
  pages = {1766--1779},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2021/hash/0dd6049f5fa537d41753be6d37859430-Abstract.html},
  urldate = {2023-05-06},
  abstract = {Graph neural networks (GNNs) have achieved superior performance in various applications, but training dedicated GNNs can be costly for large-scale graphs. Some recent work started to study the pre-training of GNNs. However, none of them provide theoretical insights into the design of their frameworks, or clear requirements and guarantees towards their transferability. In this work, we establish a theoretically grounded and practically useful framework for the transfer learning of GNNs. Firstly, we propose a novel view towards the essential graph information and advocate the capturing of it as the goal of transferable GNN training, which motivates the design of EGI (Ego-Graph Information maximization) to analytically achieve this goal. Secondly,when node features are structure-relevant, we conduct an analysis of EGI transferability regarding the difference between the local graph Laplacians of the source and target graphs. We conduct controlled synthetic experiments to directly justify our theoretical conclusions. Comprehensive experiments on two real-world network datasets show consistent results in the analyzed setting of direct-transfering, while those on large-scale knowledge graphs show promising results in the more practical setting of transfering with fine-tuning.},
  keywords = {ego-network},
  file = {D:\Zotero\literature_data\storage\KKWGCZL7\Zhu Á≠â - 2021 - Transfer Learning of Graph Neural Networks with Eg.pdf}
}

@inproceedings{zhu2021b,
  title = {Graph {{Contrastive Learning}} with {{Adaptive Augmentation}}},
  booktitle = {Proceedings of the {{Web Conference}} 2021},
  author = {Zhu, Yanqiao and Xu, Yichen and Yu, Feng and Liu, Qiang and Wu, Shu and Wang, Liang},
  date = {2021-04-19},
  eprint = {2010.14945},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {2069--2080},
  doi = {10.1145/3442381.3449802},
  url = {http://arxiv.org/abs/2010.14945},
  urldate = {2023-07-26},
  abstract = {Recently, contrastive learning (CL) has emerged as a successful method for unsupervised graph representation learning. Most graph CL methods first perform stochastic augmentation on the input graph to obtain two graph views and maximize the agreement of representations in the two views. Despite the prosperous development of graph CL methods, the design of graph augmentation schemes -- a crucial component in CL -- remains rarely explored. We argue that the data augmentation schemes should preserve intrinsic structures and attributes of graphs, which will force the model to learn representations that are insensitive to perturbation on unimportant nodes and edges. However, most existing methods adopt uniform data augmentation schemes, like uniformly dropping edges and uniformly shuffling features, leading to suboptimal performance. In this paper, we propose a novel graph contrastive representation learning method with adaptive augmentation that incorporates various priors for topological and semantic aspects of the graph. Specifically, on the topology level, we design augmentation schemes based on node centrality measures to highlight important connective structures. On the node attribute level, we corrupt node features by adding more noise to unimportant node features, to enforce the model to recognize underlying semantic information. We perform extensive experiments of node classification on a variety of real-world datasets. Experimental results demonstrate that our proposed method consistently outperforms existing state-of-the-art baselines and even surpasses some supervised counterparts, which validates the effectiveness of the proposed contrastive framework with adaptive augmentation.},
  keywords = {Computer Science - Machine Learning,GCA},
  file = {D\:\\Zotero\\literature_data\\storage\\4AGSXHNA\\Zhu Á≠â - 2021 - Graph Contrastive Learning with Adaptive Augmentat.pdf;D\:\\Zotero\\literature_data\\storage\\SDP2LB7P\\2010.html}
}

@article{zhu2022,
  title = {Multi-{{Modal Knowledge Graph Construction}} and {{Application}}: {{A Survey}}},
  shorttitle = {Multi-{{Modal Knowledge Graph Construction}} and {{Application}}},
  author = {Zhu, Xiangru and Li, Zhixu and Wang, Xiaodan and Jiang, Xueyao and Sun, Penglei and Wang, Xuwu and Xiao, Yanghua and Yuan, Nicholas Jing},
  date = {2022},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  eprint = {2202.05786},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {1--20},
  issn = {1041-4347, 1558-2191, 2326-3865},
  doi = {10.1109/TKDE.2022.3224228},
  url = {http://arxiv.org/abs/2202.05786},
  urldate = {2023-03-02},
  abstract = {Recent years have witnessed the resurgence of knowledge engineering which is featured by the fast growth of knowledge graphs. However, most of existing knowledge graphs are represented with pure symbols, which hurts the machine‚Äôs capability to understand the real world. The multi-modalization of knowledge graphs is an inevitable key step towards the realization of human-level machine intelligence. The results of this endeavor are Multi-modal Knowledge Graphs (MMKGs). In this survey on MMKGs constructed by texts and images, we first give definitions of MMKGs, followed with the preliminaries on multi-modal tasks and techniques. We then systematically review the challenges, progresses and opportunities on the construction and application of MMKGs respectively, with detailed analyses of the strength and weakness of different solutions. We finalize this survey with open research problems relevant to MMKGs.},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,E.0,E.2,I.2.4},
  file = {D:\Zotero\literature_data\storage\XE49EZ6E\Zhu Á≠â - 2022 - Multi-Modal Knowledge Graph Construction and Appli.pdf}
}

@online{zotero-1940,
  title = {(1 Â∞ÅÁßÅ‰ø°) Â¶Ç‰ΩïÁúãÂæÖ {{OpenAI}} ËøëÊúüÂÖ¨ÂºÄÁöÑ {{WebGPT}} Ê®°ÂûãÔºü - Áü•‰πé},
  url = {https://www.zhihu.com/question/506813687},
  urldate = {2023-11-29},
  file = {D:\Zotero\literature_data\storage\M8ANEJPM\506813687.html}
}

@online{zotero-2170,
  url = {https://www.aminer.cn/ranks/conf?domain_ids=all&type=ccf&search=&metric=google_h5_index&order=descend&ccf_level=A,B,C&category=Conference},
  urldate = {2023-12-14},
  file = {D:\Zotero\literature_data\storage\R7ETSWSG\conf.html}
}

@online{zotero-2172,
  title = {Â≠¶ÊúØ‰ºöËÆÆÊéíÂêç-ËÆ°ÁÆóÊú∫È°∂Á∫ß‰ºöËÆÆÊéíÂêç-ËÆ°ÁÆóÊú∫ÂõΩÈôÖÂõΩÂÜÖ‰ºöËÆÆÊéíÂêç - {{AMiner}}},
  url = {https://www.aminer.cn/ranks/conf?domain_ids=all&type=ccf&search=&metric=google_h5_index&order=descend&ccf_level=A,B,C&category=Conference},
  urldate = {2023-12-14},
  file = {D:\Zotero\literature_data\storage\XL9QMWUU\conf.html}
}

@online{zotero-262,
  title = {(11Êù°Ê∂àÊÅØ) Êú∫Âô®Â≠¶‰π†Ôºà4Ôºâ‚Äî‚ÄîÂº±ÁõëÁù£Â≠¶‰π†\_{{Fo}}*({{Bi}})ÁöÑÂçöÂÆ¢-{{CSDNÂçöÂÆ¢}}},
  url = {https://blog.csdn.net/weixin_48615832/article/details/120381855},
  urldate = {2023-04-22}
}

@online{zotero-2718,
  title = {ReadPaper ‰ΩøÁî®Â∞èÊåáÂçó},
  url = {https://docs.qq.com},
  urldate = {2024-01-16},
  abstract = {ËÖæËÆØÊñáÊ°£-Âú®Á∫øÊñáÊ°£},
  langid = {zh\_CN},
  file = {D:\Zotero\literature_data\storage\AT9BWH39\DR1ZtZ3JTZWJDa2x0.html}
}

@online{zotero-2773,
  title = {{{LLMSurvey}}/Assets/{{LLM}}\_{{Survey}}\_{{Chinese}}.Pdf at Main ¬∑ {{RUCAIBox}}/{{LLMSurvey}}},
  url = {https://github.com/RUCAIBox/LLMSurvey/blob/main/assets/LLM_Survey_Chinese.pdf},
  urldate = {2024-02-04},
  abstract = {The official GitHub page for the survey paper "A Survey of Large Language Models". - RUCAIBox/LLMSurvey},
  langid = {english},
  organization = {{GitHub}},
  file = {D:\Zotero\literature_data\storage\UF6TTJZ4\LLM_Survey_Chinese.html}
}

@online{zotero-372,
  title = {InjectiveÊòØ‰ªÄ‰πàÊÑèÊÄù\_ÁôæÂ∫¶ÊêúÁ¥¢},
  url = {https://www.baidu.com/s?wd=injective%E6%98%AF%E4%BB%80%E4%B9%88%E6%84%8F%E6%80%9D},
  urldate = {2023-05-03},
  file = {D:\Zotero\literature_data\storage\4E9YRV8B\s.html}
}

@online{zotero-4,
  title = {On the {{Performance}} of {{GoogLeNet}} and {{AlexNet Applied}} to {{Sketches}} | {{Proceedings}} of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/10171},
  urldate = {2023-04-12},
  file = {D:\Zotero\literature_data\storage\6HBMIZ5R\10171.html}
}

@online{zotero-490,
  title = {(14Êù°Ê∂àÊÅØ) {{Training}} Language Models to Follow Instructions with Human feedbackÁøªËØë\_{{nopSledÁöÑÂçöÂÆ¢-CSDNÂçöÂÆ¢}}},
  url = {https://blog.csdn.net/qq_28385535/article/details/128182340},
  urldate = {2023-05-07},
  file = {D:\Zotero\literature_data\storage\5ZNRGKYY\128182340.html}
}

@online{zotero-500,
  title = {Â§ßÊ®°ÂûãÁßëÁ†îÊñ∞ËåÉÂºèÔºöÊó†ÈúÄËÆ≠ÁªÉÂ§ßÊ®°ÂûãÂç≥ÂèØÈ¢ÑÁü•Â§ßÊ®°ÂûãË°®Áé∞ - Áü•‰πé},
  url = {https://zhuanlan.zhihu.com/p/625356129},
  urldate = {2023-05-08},
  keywords = {Ê®°Âûãloss-ËßÑÊ®°È¢ÑÊµã},
  file = {D:\Zotero\literature_data\storage\MCKC6HWE\625356129.html}
}

@online{zotero-502,
  title = {GPT-4 ÊäÄÊúØÊä•Âëä‰∏ÄËßà},
  url = {https://zhuanlan.zhihu.com/p/620547381},
  urldate = {2023-05-08},
  abstract = {GPT-4 ÊòØ OpenAI ÂèëÂ∏ÉÁöÑÊñ∞ÁâàÂ§ßËßÑÊ®°„ÄÅÂ§öÊ®°ÊÄÅÁöÑÊ®°ÂûãÔºåÁõ∏ÊØî ChatGPTÔºàGPT-3.5 ÔºâÁâàÊú¨ÔºåÊñ∞ÊîØÊåÅ‰∫ÜÂõæÁâáËæìÂÖ•„ÄÇ GPT-4 Âü∫Á°ÄÊ®°ÂûãËøòÊòØÂü∫‰∫é Transformer ÁöÑÂ§ßËßÑÊ®°È¢ÑËÆ≠ÁªÉÊ®°ÂûãÔºåËØ•Ê®°Âûã‰∏ªË¶ÅÂäüËÉΩÊòØÈ¢ÑÊµãÊñáÊ°£‰∏≠ÁöÑ‰∏ã‰∏Ä‰∏™ËØç„ÄÇÂêéËÆ≠ÁªÉÁöÑ‚Ä¶},
  langid = {chinese},
  organization = {{Áü•‰πé‰∏ìÊ†è}},
  keywords = {Âõ¢ÈòüÂàÜÂ∑•},
  file = {D:\Zotero\literature_data\storage\J3W3ZC82\620547381.html}
}

@online{zotero-587,
  title = {Torch\_geometric.Nn.Conv.{{APPNP}} ‚Äî Pytorch\_geometric Documentation},
  url = {https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.APPNP.html#torch_geometric.nn.conv.APPNP},
  urldate = {2023-05-12},
  keywords = {APPNP},
  file = {D:\Zotero\literature_data\storage\CZNHMN2D\torch_geometric.nn.conv.APPNP.html}
}

@online{zotero-589,
  title = {Torch\_geometric.Nn.Conv.{{SGConv}} ‚Äî Pytorch\_geometric Documentation},
  url = {https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.SGConv.html#torch_geometric.nn.conv.SGConv},
  urldate = {2023-05-12},
  keywords = {GCN},
  file = {D:\Zotero\literature_data\storage\R43DGCUL\torch_geometric.nn.conv.SGConv.html}
}

@online{zotero-648,
  title = {Context2vec: {{Learning Generic Context Embedding}} with {{Bidirectional LSTM}} - {{ACL Anthology}}},
  url = {https://aclanthology.org/K16-1006/},
  urldate = {2023-05-17}
}

@online{zotero-649,
  title = {Context2vec: {{Learning Generic Context Embedding}} with {{Bidirectional LSTM}} - {{ACL Anthology}}},
  url = {https://aclanthology.org/K16-1006/},
  urldate = {2023-05-17}
}

@online{zotero-650,
  title = {Papers with {{Code}} - {{CoNLL-2003 Dataset}}},
  url = {https://paperswithcode.com/dataset/conll-2003},
  urldate = {2023-05-17},
  abstract = {CoNLL-2003 is a named entity recognition dataset released as a part of CoNLL-2003 shared task: language-independent named entity recognition. The data consists of eight files covering two languages: English and German. For each of the languages there is a training file, a development file, a test file and a large file with unannotated data. The English data was taken from the Reuters Corpus. This corpus consists of Reuters news stories between August 1996 and August 1997. For the training and development set, ten days worth of data were taken from the files representing the end of August 1996. For the test set, the texts were from December 1996. The preprocessed raw data covers the month of September 1996. The text for the German data was taken from the ECI Multilingual Text Corpus. This corpus consists of texts in many languages. The portion of data that was used for this task, was extracted from the German newspaper Frankfurter Rundshau. All three of the training, development and test sets were taken from articles written in one week at the end of August 1992. The raw data were taken from the months of September to December 1992. | English      data | Articles | Sentences | Tokens  | LOC  | MISC | ORG  | PER  | |-------------------|----------|-----------|---------|------|------|------|------| | Training     set  | 946      | 14,987    | 203,621 | 7140 | 3438 | 6321 | 6600 | | Development  set  | 216      | 3,466     | 51,362  | 1837 | 922  | 1341 | 1842 | | Test         set  | 231      | 3,684     | 46,435  | 1668 | 702  | 1661 | 1617 | Number of articles, sentences, tokens and entities (locations, miscellaneous, organizations, and persons) in English data files. | German       data | Articles | Sentences | Tokens  | LOC  | MISC | ORG  | PER  | |-------------------|----------|-----------|---------|------|------|------|------| | Training     set  | 553      | 12,705    | 206,931 | 4363 | 2288 | 2427 | 2773 | | Development  set  | 201      | 3,068     | 51,444  | 1181 | 1010 | 1241 | 1401 | | Test         set  | 155      | 3,160     | 51,943  | 1035 | 670  | 773  | 1195 | Number of articles, sentences, tokens and entities (locations, miscellaneous, organizations, and persons) in German data files.},
  langid = {english},
  file = {D:\Zotero\literature_data\storage\CSTS3I6R\conll-2003.html}
}

@online{zotero-658,
  title = {Supervised {{Learning}} of {{Universal Sentence Representations}} from {{Natural Language Inference Data}} - {{ACL Anthology}}},
  url = {https://aclanthology.org/D17-1070/},
  urldate = {2023-05-17}
}

@online{zotero-661,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/document/5206848},
  urldate = {2023-05-17}
}

@online{zotero-662,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/document/5206848},
  urldate = {2023-05-17},
  file = {D:\Zotero\literature_data\storage\4YE42654\5206848.html}
}

@online{zotero-663,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/document/5206848},
  urldate = {2023-05-17},
  file = {D:\Zotero\literature_data\storage\Y9ILNKAJ\5206848.html}
}

@online{zotero-729,
  title = {Â¶Ç‰ΩïÁî®ÁÆÄÂçïÊòìÊáÇÁöÑ‰æãÂ≠êËß£ÈáäÊù°‰ª∂ÈöèÊú∫Âú∫Ôºà{{CRF}}ÔºâÊ®°ÂûãÔºü{{ÂÆÉÂíåHMMÊúâ‰ªÄ‰πàÂå∫Âà´}}Ôºü - {{PeaceÁöÑÂõûÁ≠î}} - Áü•‰πé {{https://www.zhihu.com/question/35866596/answer/236886066}}}
}

@online{zotero-837,
  title = {An Empirical Analysis of Compute-Optimal Large Language Model Training},
  url = {https://www.deepmind.com/publications/an-empirical-analysis-of-compute-optimal-large-language-model-training},
  urldate = {2023-06-04},
  abstract = {We investigate the optimal model and dataset size for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training \textbackslash nummodels language models ranging from 70 million to 10 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the training dataset size should be scaled equally: for every doubling of model size the training dataset size should also be doubled. We test this hypothesis by training a more compute-optimal model, \textbackslash Chinchilla, using the same compute budget as \textbackslash gopher but with 70B parameters and 4\$\textbackslash times\$ more data. \textbackslash chinchilla uniformly and significantly outperforms \textbackslash Gopher, GPT-3, Jurassic-1, and \textbackslash mtnlg on a large range of downstream evaluation tasks. As a highlight, \textbackslash chinchilla reaches an average accuracy of 67.5\textbackslash\% on the MMLU benchmark, over a 7\textbackslash\% improvement over \textbackslash gopher.},
  langid = {english},
  file = {D:\Zotero\literature_data\storage\S3I9JIXY\an-empirical-analysis-of-compute-optimal-large-language-model-training.html}
}

@online{zotero-969,
  title = {Bootstrapped {{Representation Learning}} on {{Graphs}} | {{OpenReview}}},
  url = {https://openreview.net/forum?id=QrzVRAA49Ud},
  urldate = {2023-07-06},
  file = {D:\Zotero\literature_data\storage\UTBGN5AD\forum.html}
}

@online{zuccon2023,
  title = {{{ChatGPT Hallucinates}} When {{Attributing Answers}}},
  author = {Zuccon, Guido and Koopman, Bevan and Shaik, Razia},
  date = {2023-09-17},
  eprint = {2309.09401},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2309.09401},
  urldate = {2024-01-03},
  abstract = {Can ChatGPT provide evidence to support its answers? Does the evidence it suggests actually exist and does it really support its answer? We investigate these questions using a collection of domain-specific knowledge-based questions, specifically prompting ChatGPT to provide both an answer and supporting evidence in the form of references to external sources. We also investigate how different prompts impact answers and evidence. We find that ChatGPT provides correct or partially correct answers in about half of the cases (50.6\% of the times), but its suggested references only exist 14\% of the times. We further provide insights on the generated references that reveal common traits among the references that ChatGPT generates, and show how even if a reference provided by the model does exist, this reference often does not support the claims ChatGPT attributes to it. Our findings are important because (1) they are the first systematic analysis of the references created by ChatGPT in its answers; (2) they suggest that the model may leverage good quality information in producing correct answers, but is unable to attribute real evidence to support its answers. Prompts, raw result files and manual analysis are made publicly available.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Digital Libraries,Computer Science - Information Retrieval,Ë¥°ÁåÆÊÄßÂΩíÂõ†},
  file = {D\:\\Zotero\\literature_data\\storage\\6XL9DR56\\Zuccon Á≠â - 2023 - ChatGPT Hallucinates when Attributing Answers.pdf;D\:\\Zotero\\literature_data\\storage\\FXRPZANF\\2309.html}
}
